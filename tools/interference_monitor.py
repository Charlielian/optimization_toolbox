import io
import logging
import os
from datetime import date, datetime, timedelta

import pandas as pd
import streamlit as st

# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç™¾å®ç®±å·¥å…·é›† - å¹²æ‰°ç›‘æ§å·¥å…·
æ•´åˆåŸæœ‰çš„app_streamlit.pyåŠŸèƒ½
"""


class InterferenceMonitor:
    """å¹²æ‰°ç›‘æ§å·¥å…·"""

    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)

        # å·¥å…·é…ç½®
        self.required_columns = [
            'celname', 'cgi', 'grid_id', 'grid_name', 'grid_pp',
            'pinduan', 'tt_mark', 'zhishi', 'if_cell', 'if_flag'
        ]

    def render(self):
        """æ¸²æŸ“å¹²æ‰°åˆ†æå¼•æ“ç•Œé¢"""
        st.title("ğŸ“¡ å¹²æ‰°åˆ†æå¼•æ“")
        st.caption("ç»Ÿä¸€å¹²æ‰°æ•°æ®ç®¡ç†ä¸æ™ºèƒ½åˆ†æå¹³å°")

        # åŠŸèƒ½å¯¼èˆª
        tab1, tab2, tab3 = st.tabs(["ğŸ” æ•°æ®æŸ¥è¯¢", "ğŸ“Š åŒ—å‘åˆ†æ", "ğŸ“ˆ æŠ¥å‘Šå¯¼å‡º"])

        with tab1:
            self._render_query_page()

        with tab2:
            self._render_north_analysis_page()

        with tab3:
            self._render_export_page()

    def _render_import_page(self):
        """æ¸²æŸ“æ•°æ®å¯¼å…¥é¡µé¢"""
        st.subheader("ğŸ“¥ æ•°æ®å¯¼å…¥")
        st.info("æ”¯æŒæ‹–æ‹½æˆ–ç‚¹å‡»é€‰æ‹©æ–‡ä»¶ã€‚å¹²æ‰°æ–‡ä»¶åéœ€åŒ…å« '_nr_cel' (5G) æˆ– '_lte_cel' (4G)ã€‚")

        # å¯¼å…¥å°åŒºæ˜ å°„è¡¨
        st.markdown("#### å¯¼å…¥å°åŒºæ˜ å°„è¡¨")
        mapping_file = st.file_uploader(
            "é€‰æ‹©æ˜ å°„è¡¨ï¼ˆExcel/CSVï¼Œæœ€å¤§600MBï¼‰",
            type=['xlsx', 'xls', 'csv'],
            key="mapping"
        )

        if st.button(
            "å¯¼å…¥æ˜ å°„è¡¨",
            type="primary",
            use_container_width=True,
            disabled=(
                mapping_file is None)):
            try:
                df = self._read_excel_or_csv(mapping_file)
                success_count = self._import_cell_mapping(df)
                st.success(f"æ˜ å°„è¡¨å¯¼å…¥æˆåŠŸï¼Œå…±å¯¼å…¥ {success_count} æ¡ã€‚")
            except Exception as e:
                st.error(f"æ˜ å°„è¡¨å¯¼å…¥å¤±è´¥: {e}")

        st.divider()

        # æ‰¹é‡å¯¼å…¥å¹²æ‰°æ–‡ä»¶
        st.markdown("#### æ‰¹é‡å¯¼å…¥å¹²æ‰°æ–‡ä»¶")
        files = st.file_uploader(
            "é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªå¹²æ‰°æ–‡ä»¶ï¼ˆExcel/CSVï¼Œæœ€å¤§600MBï¼‰",
            type=['xlsx', 'xls', 'csv'],
            accept_multiple_files=True,
            key="rip_files"
        )

        if st.button(
            "å¼€å§‹æ‰¹é‡å¯¼å…¥å¹²æ‰°æ–‡ä»¶",
            type="primary",
            use_container_width=True,
            disabled=(
                not files)):
            self._batch_import_interference_files(files)

        st.divider()

        # æ˜¾ç¤ºå¯¼å…¥ç»Ÿè®¡
        self._show_import_stats()

    def _render_query_page(self):
        """æ¸²æŸ“æ•°æ®æŸ¥è¯¢é¡µé¢"""
        st.subheader("ğŸ” æŸ¥è¯¢å¹²æ‰°å°åŒº")

        col1, col2, col3 = st.columns(3)
        with col1:
            start_d = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=date.today() -
                timedelta(
                    days=7))
        with col2:
            end_d = st.date_input("ç»“æŸæ—¥æœŸ", value=date.today())
        with col3:
            only_above = st.checkbox("ä»…çœ‹å¹²æ‰°å€¼ > -107", value=False)

        c1, c2, c3 = st.columns(3)
        with c1:
            kw_cgi = st.text_input("CGI åŒ…å«", value="")
        with c2:
            kw_cel = st.text_input("å°åŒºååŒ…å«", value="")
        with c3:
            run = st.button("æŸ¥è¯¢", type="primary", use_container_width=True)

        if run:
            self._execute_query(start_d, end_d, kw_cgi, kw_cel, only_above)

    def _render_north_analysis_page(self):
        """æ¸²æŸ“åŒ—å‘åˆ†æé¡µé¢"""
        st.subheader("ğŸ“Š åŒ—å‘å¹²æ‰°æ–‡ä»¶åˆ†æ")
        st.info("æ”¯æŒåˆ†æã€åŒ—å‘å½“æ—¥å¹²æ‰°ã€‘æ–‡ä»¶å¤¹å†…çš„4G/5Gå¹²æ‰°æ–‡ä»¶ï¼Œè‡ªåŠ¨è¯†åˆ«å¹²æ‰°ç±»å‹å¹¶æ±‡èšå¹²æ‰°å€¼ã€‚")

        # æ–‡ä»¶ä¸Šä¼ 
        st.markdown("#### ä¸Šä¼ åŒ—å‘å¹²æ‰°æ–‡ä»¶")
        st.warning(
            "âš ï¸ é‡è¦æç¤ºï¼šè™½ç„¶ç•Œé¢å¯èƒ½æ˜¾ç¤º200MBé™åˆ¶ï¼Œä½†ç³»ç»Ÿå·²é…ç½®æ”¯æŒæœ€å¤§600MBçš„æ–‡ä»¶ä¸Šä¼ ã€‚å¦‚æœæ‚¨çš„æ–‡ä»¶è¶…è¿‡200MBï¼Œè¯·ç›´æ¥å°è¯•ä¸Šä¼ ã€‚")

        uploaded_files = st.file_uploader(
            "é€‰æ‹©åŒ—å‘å¹²æ‰°æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼Œæœ€å¤§600MBï¼‰",
            type=['csv'],
            accept_multiple_files=True,
            key="north_files",
            help="æ”¯æŒæœ€å¤§600MBçš„CSVæ–‡ä»¶ä¸Šä¼ "
        )

        if uploaded_files:
            st.info(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")

            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            with st.expander("æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨"):
                for i, file in enumerate(uploaded_files, 1):
                    st.write(f"{i}. {file.name}")

            # å¤„ç†æŒ‰é’®
            if st.button(
                "å¼€å§‹åˆ†æåŒ—å‘å¹²æ‰°æ–‡ä»¶",
                type="primary",
                    use_container_width=True):
                self._process_north_interference_files(uploaded_files)

    def _render_export_page(self):
        """æ¸²æŸ“æŠ¥å‘Šå¯¼å‡ºé¡µé¢"""
        st.subheader("ğŸ“ˆ å¯¼å‡ºæŠ¥å‘Šï¼ˆExcelï¼‰")

        col1, col2 = st.columns(2)
        with col1:
            start_d = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=date.today() -
                timedelta(
                    days=7),
                key="e_start")
        with col2:
            end_d = st.date_input("ç»“æŸæ—¥æœŸ", value=date.today(), key="e_end")

        if st.button("ç”Ÿæˆå¹¶ä¸‹è½½ Excel", type="primary", use_container_width=True):
            self._generate_excel_report(start_d, end_d)

    def _read_excel_or_csv(self, uploaded_file):
        """è¯»å–Excelæˆ–CSVæ–‡ä»¶"""
        name = uploaded_file.name.lower()
        if name.endswith(('.xlsx', '.xls')):
            return pd.read_excel(uploaded_file)
        elif name.endswith('.csv'):
            uploaded_file.seek(0)
            content = uploaded_file.read()
            for enc in ['utf-8', 'gbk', 'gb2312', 'utf-16']:
                try:
                    return pd.read_csv(io.BytesIO(content), encoding=enc)
                except UnicodeDecodeError:
                    continue
            raise Exception("æ— æ³•è§£æCSVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç¼–ç ")
        else:
            raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {name}")

    def _import_cell_mapping(self, df: pd.DataFrame) -> int:
        """å¯¼å…¥å°åŒºæ˜ å°„è¡¨"""
        missing = [c for c in self.required_columns if c not in df.columns]
        if missing:
            raise Exception(f"å¯¼å…¥å¤±è´¥ï¼šæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {', '.join(missing)}")

        df = df.fillna('')

        # å‡†å¤‡æ•°æ®
        records = []
        for _, r in df.iterrows():
            records.append(
                (r['celname'],
                 r['cgi'],
                    r['grid_id'],
                    r['grid_name'],
                    r['grid_pp'],
                    r['pinduan'],
                    r['tt_mark'],
                    r['zhishi'],
                    r['if_cell'],
                    r['if_flag']))

        # æ¸…ç©ºç°æœ‰æ•°æ®å¹¶æ’å…¥æ–°æ•°æ®
        self.db_manager.execute_update("DELETE FROM cell_mapping")

        insert_sql = """
        INSERT INTO cell_mapping (
            celname, cgi, grid_id, grid_name, grid_pp,
            pinduan, tt_mark, zhishi, if_cell, if_flag
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """

        success = self.db_manager.execute_many(insert_sql, records)
        if not success:
            raise Exception("æ•°æ®åº“æ’å…¥å¤±è´¥")

        # è®°å½•å¯¼å…¥æ—¥å¿—
        self.db_manager.log_import(
            "interference_monitor", "cell_mapping", "mapping",
            len(records), len(records), 0, "success"
        )

        return len(records)

    def _batch_import_interference_files(self, files):
        """æ‰¹é‡å¯¼å…¥å¹²æ‰°æ–‡ä»¶"""
        total_ok, total_err, total_files = 0, 0, 0

        for f in files:
            total_files += 1
            try:
                df = self._read_excel_or_csv(f)
                ok_rows, err_rows = self._import_interference_data(df, f.name)
                total_ok += ok_rows
                total_err += err_rows
                st.success(f"{f.name} å¯¼å…¥æˆåŠŸï¼š{ok_rows} æ¡ï¼›è·³è¿‡é”™è¯¯è¡Œï¼š{err_rows} æ¡")
            except Exception as e:
                st.error(f"{f.name} å¯¼å…¥å¤±è´¥ï¼š{e}")

        st.info(f"æœ¬æ¬¡å¯¼å…¥å®Œæˆã€‚æ–‡ä»¶æ•°ï¼š{total_files}ï¼ŒæˆåŠŸè®°å½•ï¼š{total_ok}ï¼Œé”™è¯¯è¡Œï¼š{total_err}")

    def _import_interference_data(
            self,
            df: pd.DataFrame,
            file_name: str) -> tuple:
        """å¯¼å…¥å¹²æ‰°æ•°æ®"""
        is_5g = '_nr_cel' in file_name.lower()
        is_4g = '_lte_cel' in file_name.lower()

        if not (is_5g or is_4g):
            raise Exception("æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼Œæ–‡ä»¶ååº”åŒ…å« '_nr_cel'(5G) æˆ– '_lte_cel'(4G)")

        required = [
            'æ•°æ®æ—¶é—´',
            'CGI',
            'å°åŒºå',
            'å…¨é¢‘æ®µå‡å€¼'] if is_5g else [
            'æ•°æ®æ—¶é—´',
            'CGI',
            'å°åŒºå',
            'å¹³å‡å¹²æ‰°ç”µå¹³']
        missing = [c for c in required if c not in df.columns]
        if missing:
            raise Exception(f"æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—: {', '.join(missing)}")

        data_list, error_rows = [], 0

        for idx, row in df.iterrows():
            try:
                # å¤„ç†æ—¶é—´å­—æ®µ - æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼
                dstr = str(row['æ•°æ®æ—¶é—´']).replace('\t', '').strip()
                # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                date_formats = [
                    "%Y-%m-%d %H:%M:%S",
                    "%Y-%m-%d",
                    "%Y/%m/%d %H:%M:%S",
                    "%Y/%m/%d",
                    "%Y%m%d",
                    "%Y-%m-%d %H:%M",
                    "%Y/%m/%d %H:%M"
                ]
                d = None
                for fmt in date_formats:
                    try:
                        d = datetime.strptime(dstr, fmt)
                        break
                    except ValueError:
                        continue
                
                if d is None:
                    self.logger.warning(f"æ–‡ä»¶ {file_name} ç¬¬ {idx+1} è¡Œ: æ— æ³•è§£ææ—¥æœŸæ ¼å¼ '{dstr}'ï¼Œè·³è¿‡æ­¤è¡Œ")
                    error_rows += 1
                    continue

                date_str = d.strftime("%Y%m%d")
                cgi = str(row['CGI']).strip()
                celname = str(row['å°åŒºå']).strip()

                if is_5g:
                    zhishi = '5g'
                    rip_str = str(row['å…¨é¢‘æ®µå‡å€¼']).strip()
                    pinduan = '700M' if 'CBN' in celname else '2.6G'
                else:
                    zhishi = '4g'
                    rip_str = str(row['å¹³å‡å¹²æ‰°ç”µå¹³']).strip()
                    pinduan = 'lte'

                try:
                    if_rip = '1' if float(rip_str) > -107 else '0'
                except (ValueError, TypeError):
                    if_rip = 'n/a'

                data_list.append(
                    (date_str, cgi, celname, zhishi, pinduan, rip_str, if_rip))
            except Exception as e:
                error_rows += 1
                self.logger.warning(f"æ–‡ä»¶ {file_name} ç¬¬ {idx+1} è¡Œå¤„ç†å¤±è´¥: {str(e)}")
                if idx < 5:  # åªè®°å½•å‰5è¡Œçš„è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                    import traceback
                    self.logger.debug(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

        if not data_list:
            return 0, error_rows

        insert_sql = """
        INSERT OR REPLACE INTO interference_data (date_str, cgi, celname, zhishi, pinduan, rip_str, if_rip)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        """

        success = self.db_manager.execute_many(insert_sql, data_list)
        if not success:
            raise Exception("æ‰¹é‡æ’å…¥æ•°æ®åº“å¤±è´¥")

        # è®°å½•å¯¼å…¥æ—¥å¿—
        self.db_manager.log_import(
            "interference_monitor", file_name, "interference",
            len(data_list), len(data_list), error_rows, "success"
        )

        return len(data_list), error_rows

    def _execute_query(self, start_d, end_d, kw_cgi, kw_cel, only_above):
        """æ‰§è¡ŒæŸ¥è¯¢"""
        try:
            s = self._to_yyyymmdd(start_d)
            e = self._to_yyyymmdd(end_d)

            if s > e:
                st.error("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
                return

            df = self._query_interference_range(
                s, e, kw_cgi, kw_cel, only_above)

            if df.empty:
                st.warning("æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®")
                return

            out = self._summarize_interference(df, s, e)

            if out.empty:
                st.warning("æ±‡æ€»åæ— æ•°æ®")
                return

            st.success(f"æŸ¥è¯¢åˆ° {len(df)} æ¡åŸå§‹è®°å½•ï¼›æ±‡æ€»è¡Œæ•°ï¼š{len(out)}")

            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.dataframe(out, use_container_width=True)

            # ä¸‹è½½åŠŸèƒ½
            csv = out.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                "ä¸‹è½½æ±‡æ€»CSV",
                data=csv,
                file_name=f"å¹²æ‰°å°åŒºæ±‡æ€»_{s}_{e}.csv",
                mime="text/csv"
            )

        except Exception as ex:
            st.error(f"æŸ¥è¯¢å¤±è´¥ï¼š{ex}")

    def _query_interference_range(
            self,
            start_yyyymmdd: str,
            end_yyyymmdd: str,
            cgi_kw: str = "",
            cel_kw: str = "",
            only_above_threshold=False):
        """æŸ¥è¯¢å¹²æ‰°æ•°æ®èŒƒå›´"""
        base_sql = """
        SELECT r.date_str, r.cgi, r.celname, r.zhishi, r.pinduan, r.rip_str, r.if_rip
        FROM interference_data r
        WHERE r.date_str BETWEEN ? AND ?
        """
        params = [start_yyyymmdd, end_yyyymmdd]

        if cgi_kw:
            base_sql += " AND r.cgi LIKE ?"
            params.append(f"%{cgi_kw}%")
        if cel_kw:
            base_sql += " AND r.celname LIKE ?"
            params.append(f"%{cel_kw}%")
        if only_above_threshold:
            base_sql += " AND CAST(r.rip_str AS REAL) > -107"

        base_sql += " ORDER BY r.cgi, r.date_str"

        return self.db_manager.get_dataframe(base_sql, tuple(params))

    def _summarize_interference(
            self,
            df_rip: pd.DataFrame,
            start_yyyymmdd: str,
            end_yyyymmdd: str):
        """æ±‡æ€»å¹²æ‰°æ•°æ® - ä»¥æ˜ å°„è¡¨ä¸ºä¸»ï¼Œä¿ç•™æ‰€æœ‰æ˜ å°„è¡Œ"""
        if df_rip.empty:
            # å³ä½¿æ²¡æœ‰å¹²æ‰°æ•°æ®ï¼Œä¹Ÿè¦è¿”å›æ˜ å°„è¡¨æ•°æ®
            return self._get_empty_result_with_mapping(start_yyyymmdd, end_yyyymmdd)

        # ç”Ÿæˆå®Œæ•´çš„æ—¥æœŸèŒƒå›´
        start_date = datetime.strptime(start_yyyymmdd, '%Y%m%d')
        end_date = datetime.strptime(end_yyyymmdd, '%Y%m%d')
        date_range = []
        current = start_date
        while current <= end_date:
            date_range.append(current.strftime('%Y%m%d'))
            current += timedelta(days=1)

        df = df_rip.copy()

        def gt_minus107(x):
            try:
                return float(x) > -107
            except Exception:
                return False

        # ç»Ÿè®¡å¹²æ‰°å€¼å¤§äº-107çš„å¤©æ•°ï¼ˆæŒ‰CGIç»Ÿè®¡ï¼‰
        df['gt'] = df['rip_str'].apply(gt_minus107)
        count_df = df.groupby('cgi')['gt'].sum().reset_index().rename(
            columns={'gt': 'å¹²æ‰°å€¼> -107å¤©æ•°'})

        # é‡è¦ï¼šå…ˆè·å–æ‰€æœ‰æ˜ å°„è¡¨æ•°æ®ï¼ˆå¯¹äºæœ‰å¹²æ‰°æ•°æ®çš„CGIï¼‰
        cgi_list = df['cgi'].unique().tolist()
        cells = self._get_cells_by_cgi_list(cgi_list)
        
        if cells.empty:
            # å¦‚æœæ²¡æœ‰æ˜ å°„è¡¨æ•°æ®ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘ï¼Œä½†ä»éœ€è¦åŒ…å«ç½‘æ ¼ç›¸å…³åˆ—
            pivot = df.pivot_table(
                index=['cgi', 'celname', 'zhishi', 'pinduan'],
                columns='date_str',
                values='rip_str',
                aggfunc='first',
                fill_value='n/a(æ— æ•°æ®)'
            )
            pivot = pivot.reset_index()
            
            # ç¡®ä¿æ‰€æœ‰æ—¥æœŸåˆ—éƒ½å­˜åœ¨
            for date_str in date_range:
                if date_str not in pivot.columns:
                    pivot[date_str] = 'n/a(æ— æ•°æ®)'
            
            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            out = pd.merge(pivot, count_df, on='cgi', how='left')
            
            # æ·»åŠ ç½‘æ ¼ç›¸å…³åˆ—ï¼ˆå³ä½¿æ²¡æœ‰æ˜ å°„è¡¨æ•°æ®ï¼Œä¹Ÿè¦åŒ…å«è¿™äº›åˆ—ï¼Œå€¼ä¸ºç©ºï¼‰
            out['grid_id'] = ''
            out['grid_name'] = ''
            out['grid_pp'] = ''
            out['tt_mark'] = ''
            out['if_cell'] = ''
            out['if_flag'] = ''
            
            # æŒ‰æ—¥æœŸæ’åºåˆ—
            date_cols = sorted([c for c in out.columns if c.isdigit() and len(c) == 8])
            base_cols = ['cgi', 'celname', 'zhishi', 'pinduan', 'grid_id', 'grid_name', 'grid_pp', 
                         'tt_mark', 'if_cell', 'if_flag', 'å¹²æ‰°å€¼> -107å¤©æ•°']
            out = out[base_cols + date_cols]
            
            # åº”ç”¨ä¸­æ–‡åˆ—åæ˜ å°„
            out = self._apply_chinese_column_mapping(out)
            return out

        # ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼Œé¿å…å¾ªç¯
        # å…ˆåˆ›å»ºå¹²æ‰°æ•°æ®çš„pivot tableï¼ˆåªæŒ‰CGIï¼Œä¸æŒ‰å…¶ä»–å­—æ®µåˆ†ç»„ï¼‰
        pivot = df.pivot_table(
            index='cgi',
            columns='date_str',
            values='rip_str',
            aggfunc='first',
            fill_value='n/a(æ— æ•°æ®)'
        )
        pivot = pivot.reset_index()
        
        # ç¡®ä¿æ‰€æœ‰æ—¥æœŸåˆ—éƒ½å­˜åœ¨
        for date_str in date_range:
            if date_str not in pivot.columns:
                pivot[date_str] = 'n/a(æ— æ•°æ®)'
        
        # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯åˆ°pivot
        pivot_with_count = pd.merge(pivot, count_df, on='cgi', how='left')
        pivot_with_count['å¹²æ‰°å€¼> -107å¤©æ•°'] = pivot_with_count['å¹²æ‰°å€¼> -107å¤©æ•°'].fillna(0)
        
        # å…³é”®ï¼šä»¥æ˜ å°„è¡¨ä¸ºä¸»ï¼Œä½¿ç”¨left mergeä¿ç•™æ‰€æœ‰æ˜ å°„è¡Œ
        # è¿™æ ·åŒä¸€ä¸ªCGIçš„å¤šè¡Œæ˜ å°„éƒ½ä¼šä¿ç•™
        result_df = pd.merge(
            cells,
            pivot_with_count,
            on='cgi',
            how='left',
            suffixes=('', '_dup')
        )
        
        # å¯¹äºæ²¡æœ‰å¹²æ‰°æ•°æ®çš„æ—¥æœŸåˆ—ï¼Œå¡«å……ä¸º 'n/a(æ— æ•°æ®)'
        date_cols = [col for col in result_df.columns if col.isdigit() and len(col) == 8]
        for col in date_cols:
            result_df[col] = result_df[col].fillna('n/a(æ— æ•°æ®)')
        
        # å¦‚æœå¹²æ‰°å€¼> -107å¤©æ•°åˆ—ä¸ºç©ºï¼Œå¡«å……ä¸º0
        if 'å¹²æ‰°å€¼> -107å¤©æ•°' in result_df.columns:
            result_df['å¹²æ‰°å€¼> -107å¤©æ•°'] = result_df['å¹²æ‰°å€¼> -107å¤©æ•°'].fillna(0)
        
        # æŒ‰æ—¥æœŸæ’åºåˆ—
        date_cols = sorted([c for c in result_df.columns if c.isdigit() and len(c) == 8])
        base_cols = ['cgi', 'celname', 'zhishi', 'pinduan', 'grid_id', 'grid_name', 'grid_pp', 
                     'tt_mark', 'if_cell', 'if_flag', 'å¹²æ‰°å€¼> -107å¤©æ•°']
        # åªä¿ç•™å­˜åœ¨çš„åˆ—
        existing_base_cols = [col for col in base_cols if col in result_df.columns]
        result_df = result_df[existing_base_cols + date_cols]
        
        # åº”ç”¨ä¸­æ–‡åˆ—åæ˜ å°„
        result_df = self._apply_chinese_column_mapping(result_df)
        return result_df
    
    def _get_empty_result_with_mapping(self, start_yyyymmdd: str, end_yyyymmdd: str):
        """å½“æ²¡æœ‰å¹²æ‰°æ•°æ®æ—¶ï¼Œè¿”å›æ˜ å°„è¡¨æ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰"""
        # ç”Ÿæˆå®Œæ•´çš„æ—¥æœŸèŒƒå›´
        start_date = datetime.strptime(start_yyyymmdd, '%Y%m%d')
        end_date = datetime.strptime(end_yyyymmdd, '%Y%m%d')
        date_range = []
        current = start_date
        while current <= end_date:
            date_range.append(current.strftime('%Y%m%d'))
            current += timedelta(days=1)
        
        # è·å–æ‰€æœ‰æ˜ å°„è¡¨æ•°æ®ï¼ˆè¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æŸ¥è¯¢æ¡ä»¶ï¼‰
        cells = self._get_cells()
        
        if cells.empty:
            return pd.DataFrame()
        
        # ä¸ºæ¯ä¸ªæ˜ å°„è¡Œåˆ›å»ºä¸€è¡Œï¼Œæ‰€æœ‰æ—¥æœŸåˆ—å¡«å……ä¸º 'n/a(æ— æ•°æ®)'
        result_rows = []
        for idx, cell_row in cells.iterrows():
            row = {
                'cgi': cell_row.get('cgi', ''),
                'celname': cell_row.get('celname', ''),
                'zhishi': cell_row.get('zhishi', ''),
                'pinduan': cell_row.get('pinduan', ''),
                'grid_id': cell_row.get('grid_id', ''),
                'grid_name': cell_row.get('grid_name', ''),
                'grid_pp': cell_row.get('grid_pp', ''),
                'tt_mark': cell_row.get('tt_mark', ''),
                'if_cell': cell_row.get('if_cell', ''),
                'if_flag': cell_row.get('if_flag', ''),
                'å¹²æ‰°å€¼> -107å¤©æ•°': 0
            }
            
            for date_str in date_range:
                row[date_str] = 'n/a(æ— æ•°æ®)'
            
            result_rows.append(row)
        
        result_df = pd.DataFrame(result_rows)
        
        # æŒ‰æ—¥æœŸæ’åºåˆ—
        date_cols = sorted([c for c in result_df.columns if c.isdigit() and len(c) == 8])
        base_cols = ['cgi', 'celname', 'zhishi', 'pinduan', 'grid_id', 'grid_name', 'grid_pp', 
                     'tt_mark', 'if_cell', 'if_flag', 'å¹²æ‰°å€¼> -107å¤©æ•°']
        existing_base_cols = [col for col in base_cols if col in result_df.columns]
        result_df = result_df[existing_base_cols + date_cols]
        
        result_df = self._apply_chinese_column_mapping(result_df)
        return result_df

    def _get_cells(self):
        """è·å–å°åŒºæ˜ å°„æ•°æ®"""
        return self.db_manager.get_dataframe("""
            SELECT DISTINCT grid_id, grid_name, grid_pp, cgi, celname, zhishi, pinduan, tt_mark, if_cell, if_flag
            FROM cell_mapping
        """)
    
    def _get_cells_by_cgi_list(self, cgi_list):
        """æ ¹æ®CGIåˆ—è¡¨è·å–å°åŒºæ˜ å°„æ•°æ®"""
        if not cgi_list:
            return pd.DataFrame()
        
        placeholders = ','.join(['?'] * len(cgi_list))
        # ç§»é™¤DISTINCTï¼Œç¡®ä¿åŒä¸€ä¸ªCGIçš„å¤šè¡Œæ˜ å°„éƒ½èƒ½è¿”å›
        # å› ä¸ºcell_mappingè¡¨å…è®¸åŒä¸€ä¸ªCGIæœ‰å¤šä¸ªä¸åŒçš„ç½‘æ ¼æ˜ å°„ï¼ˆé€šè¿‡UNIQUE (cgi, grid_id)çº¦æŸï¼‰
        sql = f"""
            SELECT grid_id, grid_name, grid_pp, cgi, celname, zhishi, pinduan, tt_mark, if_cell, if_flag
            FROM cell_mapping
            WHERE cgi IN ({placeholders})
            ORDER BY cgi, grid_id
        """
        return self.db_manager.get_dataframe(sql, tuple(cgi_list))

    def _apply_chinese_column_mapping(self, df):
        """åº”ç”¨ä¸­æ–‡åˆ—åæ˜ å°„"""
        column_mapping = {
            'cgi': 'CGI',
            'celname': 'å°åŒºå',
            'zhishi': 'åˆ¶å¼',
            'pinduan': 'é¢‘æ®µ',
            'grid_id': 'ç½‘æ ¼ID',
            'grid_name': 'ç½‘æ ¼ä¸­æ–‡',
            'grid_pp': 'ç½‘æ ¼æ ‡ç­¾',
            'tt_mark': 'å¤‡æ³¨',
            'if_cell': 'æ˜¯å¦æ˜ å°„å°åŒº',
            'if_flag': 'æ˜¯å¦ç¼“å†²åŒº',
            'å¹²æ‰°å€¼> -107å¤©æ•°': 'å¹²æ‰°å€¼> -107å¤©æ•°'
        }

        # é‡å‘½ååˆ—
        df = df.rename(columns=column_mapping)

        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼Œå°†ä¸­æ–‡åˆ—åæ”¾åœ¨å‰é¢
        chinese_cols = [
            'CGI',
            'å°åŒºå',
            'åˆ¶å¼',
            'é¢‘æ®µ',
            'ç½‘æ ¼ID',
            'ç½‘æ ¼ä¸­æ–‡',
            'ç½‘æ ¼æ ‡ç­¾',
            'å¤‡æ³¨',
            'æ˜¯å¦æ˜ å°„å°åŒº',
            'æ˜¯å¦ç¼“å†²åŒº',
            'å¹²æ‰°å€¼> -107å¤©æ•°']
        date_cols = [col for col in df.columns if col.isdigit()
                     and len(col) == 8]

        # åªä¿ç•™å­˜åœ¨çš„åˆ—
        existing_chinese_cols = [
            col for col in chinese_cols if col in df.columns]
        final_cols = existing_chinese_cols + date_cols

        return df[final_cols]

    def _to_yyyymmdd(self, d: date):
        """è½¬æ¢æ—¥æœŸæ ¼å¼"""
        return d.strftime('%Y%m%d')

    def _convert_to_numeric(self, value):
        """å°†å¹²æ‰°å€¼è½¬æ¢ä¸ºæ•°å­—"""
        if pd.isna(value) or value == 'n/a(æ— æ•°æ®)' or value == '':
            return None

        try:
            # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            return float(value)
        except (ValueError, TypeError):
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›None
            return None

    def _show_import_stats(self):
        """æ˜¾ç¤ºå¯¼å…¥ç»Ÿè®¡"""
        try:
            stats = self.db_manager.execute_query("""
                SELECT MIN(date_str) AS min_d, MAX(date_str) AS max_d, COUNT(*) AS cnt
                FROM interference_data
            """)

            if stats and stats[0]['cnt'] > 0:
                st.success(
                    f"å·²å¯¼å…¥å¹²æ‰°æ•°æ®ï¼š{
                        stats[0]['min_d']} è‡³ {
                        stats[0]['max_d']}ï¼Œå…± {
                        stats[0]['cnt']} æ¡")
            else:
                st.warning("æ•°æ®åº“ä¸­å°šæ— å¹²æ‰°æ•°æ®")
        except Exception as e:
            st.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

    def _process_north_interference_files(self, uploaded_files):
        """å¤„ç†åŒ—å‘å¹²æ‰°æ–‡ä»¶"""
        st.markdown("#### åŒ—å‘å¹²æ‰°æ–‡ä»¶å¤„ç†")
        st.info("ğŸ“ è¯·ä¸Šä¼ åŒ—å‘å¹²æ‰°æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æå¤„ç†")

        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "é€‰æ‹©åŒ—å‘å¹²æ‰°æ–‡ä»¶",
            type=['xlsx', 'xls', 'csv'],
            accept_multiple_files=True,
            help="æ”¯æŒExcelå’ŒCSVæ ¼å¼çš„åŒ—å‘å¹²æ‰°æ•°æ®æ–‡ä»¶"
        )

        if uploaded_files:
            st.success(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")

            # å¤„ç†é€‰é¡¹
            col1, col2 = st.columns(2)
            with col1:
                process_option = st.selectbox(
                    "å¤„ç†æ–¹å¼",
                    ["æ•°æ®é¢„è§ˆ", "æ•°æ®å¯¼å…¥", "æ•°æ®åˆ†æ"],
                    help="é€‰æ‹©å¯¹ä¸Šä¼ æ–‡ä»¶çš„æ“ä½œæ–¹å¼"
                )

            with col2:
                if st.button("å¼€å§‹å¤„ç†", type="primary"):
                    self._handle_north_interference_processing(
                        uploaded_files, process_option)

    def _handle_north_interference_processing(self, files, process_option):
        """å¤„ç†åŒ—å‘å¹²æ‰°æ–‡ä»¶"""
        try:
            if process_option == "æ•°æ®é¢„è§ˆ":
                self._preview_north_interference_data(files)
            elif process_option == "æ•°æ®å¯¼å…¥":
                self._import_north_interference_data(files)
            elif process_option == "æ•°æ®åˆ†æ":
                self._analyze_north_interference_data(files)
        except Exception as e:
            st.error(f"å¤„ç†å¤±è´¥: {e}")

    def _preview_north_interference_data(self, files):
        """é¢„è§ˆåŒ—å‘å¹²æ‰°æ•°æ®"""
        st.markdown("##### ğŸ“Š æ•°æ®é¢„è§ˆ")

        for i, file in enumerate(files):
            st.markdown(f"**æ–‡ä»¶ {i + 1}: {file.name}**")

            try:
                if file.name.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(file)
                else:
                    df = pd.read_csv(file)

                st.write(f"è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")
                st.write("åˆ—å:", list(df.columns))
                st.dataframe(df.head(10), use_container_width=True)

            except Exception as e:
                st.error(f"æ–‡ä»¶ {file.name} è¯»å–å¤±è´¥: {e}")

    def _import_north_interference_data(self, files):
        """å¯¼å…¥åŒ—å‘å¹²æ‰°æ•°æ®"""
        st.markdown("##### ğŸ“¥ æ•°æ®å¯¼å…¥")

        total_imported = 0
        for file in files:
            try:
                if file.name.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(file)
                else:
                    df = pd.read_csv(file)

                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„åŒ—å‘å¹²æ‰°æ•°æ®æ ¼å¼è¿›è¡Œå­—æ®µæ˜ å°„
                # æš‚æ—¶æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
                st.success(f"æ–‡ä»¶ {file.name}: {len(df)} æ¡è®°å½•")
                total_imported += len(df)

            except Exception as e:
                st.error(f"æ–‡ä»¶ {file.name} å¯¼å…¥å¤±è´¥: {e}")

        st.success(f"æ€»è®¡å¯¼å…¥ {total_imported} æ¡åŒ—å‘å¹²æ‰°æ•°æ®")

    def _analyze_north_interference_data(self, files):
        """åˆ†æåŒ—å‘å¹²æ‰°æ•°æ®"""
        st.markdown("##### ğŸ“ˆ æ•°æ®åˆ†æ")

        all_data = []
        for file in files:
            try:
                if file.name.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(file)
                else:
                    df = pd.read_csv(file)

                all_data.append(df)

            except Exception as e:
                st.error(f"æ–‡ä»¶ {file.name} è¯»å–å¤±è´¥: {e}")

        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            st.write(f"åˆå¹¶æ•°æ®: {len(combined_df)} æ¡è®°å½•")

            # åŸºæœ¬ç»Ÿè®¡
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»è®°å½•æ•°", len(combined_df))
            with col2:
                st.metric("æ–‡ä»¶æ•°é‡", len(files))
            with col3:
                st.metric("å¹³å‡æ¯æ–‡ä»¶", f"{len(combined_df) // len(files)} æ¡")

            # æ•°æ®æ¦‚è§ˆ
            st.dataframe(combined_df.describe(), use_container_width=True)

    def _generate_excel_report(self, start_d, end_d):
        """ç”ŸæˆExcelæŠ¥å‘Š"""
        try:
            s = self._to_yyyymmdd(start_d)
            e = self._to_yyyymmdd(end_d)

            if s > e:
                st.error("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
                return

            with st.spinner('æ­£åœ¨ç”Ÿæˆ Excel æ–‡ä»¶...'):
                df = self._query_interference_range(s, e)
                if df.empty:
                    st.warning("æ‰€é€‰æ—¥æœŸèŒƒå›´å†…æ— æ•°æ®")
                else:
                    st.info(f"æŸ¥è¯¢åˆ° {len(df)} æ¡åŸå§‹è®°å½•")
                    out = self._summarize_interference(df, s, e)
                    if out.empty:
                        st.warning("æ±‡æ€»åæ— æ•°æ®")
                    else:
                        st.info(f"æ±‡æ€»å {len(out)} è¡Œæ•°æ®ï¼Œ{len(out.columns)} åˆ—")

                        # ç”ŸæˆExcelæ–‡ä»¶
                        buffer = io.BytesIO()
                        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                            # å¤„ç†æ•°æ®ï¼šå°†å¹²æ‰°å€¼è½¬æ¢ä¸ºæ•°å­—
                            out_processed = out.copy()
                            date_cols = [
                                c for c in out.columns if c.isdigit() and len(c) == 8]

                            # è½¬æ¢æ—¥æœŸåˆ—çš„å¹²æ‰°å€¼ä¸ºæ•°å­—
                            for col in date_cols:
                                out_processed[col] = out_processed[col].apply(
                                    lambda x: self._convert_to_numeric(x))

                            out_processed.to_excel(
                                writer, index=False, sheet_name='å¹²æ‰°ç›‘æ§æ•°æ®')

                            # åº”ç”¨æ¡ä»¶æ ¼å¼
                            ws = writer.sheets['å¹²æ‰°ç›‘æ§æ•°æ®']
                            date_col_indices = [
                                i for i, c in enumerate(
                                    out_processed.columns, start=1) if c.isdigit()]

                            if date_col_indices:
                                # åˆ›å»ºæ ¼å¼ï¼šé»„è‰²èƒŒæ™¯ï¼Œçº¢è‰²å­—ä½“
                                fmt = writer.book.add_format({
                                    'bg_color': '#FFFF00',  # é»„è‰²èƒŒæ™¯
                                    'font_color': '#FF0000'  # çº¢è‰²å­—ä½“
                                })

                                # å¯¹æ¯ä¸ªæ—¥æœŸåˆ—åº”ç”¨æ¡ä»¶æ ¼å¼
                                for col_idx in date_col_indices:
                                    # åº”ç”¨æ¡ä»¶æ ¼å¼ï¼šå¹²æ‰°å€¼å¤§äº-107æ—¶æ˜¾ç¤ºé»„è‰²èƒŒæ™¯å’Œçº¢è‰²å­—ä½“
                                    ws.conditional_format(1,
                                                          col_idx - 1,
                                                          len(out_processed),
                                                          col_idx - 1,
                                                          {'type': 'cell',
                                                              'criteria': 'greater than',
                                                              'value': -107,
                                                              'format': fmt})

                        buffer.seek(0)
                        st.download_button(
                            "ä¸‹è½½ Excel",
                            data=buffer.getvalue(),
                            file_name=f"å¹²æ‰°ç›‘æ§_{s}_{e}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True)
        except Exception as ex:
            st.error(f"å¯¼å‡ºå¤±è´¥ï¼š{ex}")
