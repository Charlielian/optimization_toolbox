import json
import logging
from datetime import datetime, timedelta

import pandas as pd
import streamlit as st

# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç™¾å®ç®±å·¥å…·é›† - ç³»ç»Ÿç®¡ç†å·¥å…·
æä¾›ç³»ç»Ÿé…ç½®ã€æ•°æ®ç®¡ç†ã€æ—¥å¿—æŸ¥çœ‹ç­‰åŠŸèƒ½
"""


class SystemManager:
    """ç³»ç»Ÿç®¡ç†å·¥å…·"""

    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)

    def render(self):
        """æ¸²æŸ“ç³»ç»Ÿç®¡ç†ç•Œé¢"""
        st.title("âš™ï¸ ç³»ç»Ÿç®¡ç†")
        st.caption("ç³»ç»Ÿé…ç½®ã€æ•°æ®ç®¡ç†å’Œç»´æŠ¤å·¥å…·")

        # åŠŸèƒ½å¯¼èˆª
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“Š ç³»ç»ŸçŠ¶æ€", "âš™ï¸ ç³»ç»Ÿé…ç½®", "ğŸ’¾ æ•°æ®ç®¡ç†", "ğŸ“‹ æ—¥å¿—æŸ¥çœ‹", "ğŸ”§ ç³»ç»Ÿç»´æŠ¤"
        ])

        with tab1:
            self._render_system_status()

        with tab2:
            self._render_system_config()

        with tab3:
            self._render_data_management()

        with tab4:
            self._render_log_viewer()

        with tab5:
            self._render_system_maintenance()

    def _render_system_status(self):
        """æ¸²æŸ“ç³»ç»ŸçŠ¶æ€é¡µé¢"""
        st.subheader("ğŸ“Š ç³»ç»ŸçŠ¶æ€")

        try:
            # è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
            stats = self.db_manager.get_database_stats()

            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric(
                    "æ•°æ®åº“å¤§å°",
                    f"{stats.get('db_size_mb', 0):.1f} MB",
                    help="æ•°æ®åº“æ–‡ä»¶å¤§å°"
                )

            with col2:
                st.metric(
                    "å°åŒºæ˜ å°„",
                    f"{stats.get('cell_mapping_count', 0):,}",
                    help="å°åŒºæ˜ å°„è®°å½•æ•°"
                )

            with col3:
                st.metric(
                    "å¹²æ‰°æ•°æ®",
                    f"{stats.get('interference_data_count', 0):,}",
                    help="å¹²æ‰°ç›‘æ§æ•°æ®è®°å½•æ•°"
                )

            with col4:
                st.metric("å®¹é‡æ•°æ®", f"{self.db_manager.execute_query(
                    'SELECT COUNT(*) as count FROM performance_data WHERE data_type = \'capacity\'')[0]['count']:,}", help="å®¹é‡ç›‘æ§æ•°æ®è®°å½•æ•°")

            st.divider()

            # è¯¦ç»†ç»Ÿè®¡è¡¨æ ¼
            st.markdown("#### è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯")

            table_stats = {
                'è¡¨å': [
                    'å°åŒºæ˜ å°„è¡¨', 'å·¥å‚è¡¨', 'å¹²æ‰°æ•°æ®è¡¨', 'å®¹é‡æ•°æ®è¡¨',
                    'æµé‡æ•°æ®è¡¨', 'å·¥å…·è¡¨', 'é…ç½®è¡¨', 'æ—¥å¿—è¡¨'
                ],
                'è®°å½•æ•°': [
                    stats.get('cell_mapping_count', 0),
                    stats.get('engineering_params_count', 0),
                    stats.get('interference_data_count', 0),
                    self.db_manager.execute_query("SELECT COUNT(*) as count FROM performance_data WHERE data_type = 'capacity'")[0]['count'],
                    self.db_manager.execute_query("SELECT COUNT(*) as count FROM performance_data WHERE data_type = 'traffic'")[0]['count'],
                    stats.get('tools_count', 0),
                    stats.get('system_config_count', 0),
                    stats.get('import_logs_count', 0)
                ]
            }

            df_stats = pd.DataFrame(table_stats)
            st.dataframe(df_stats, use_container_width=True, hide_index=True)

            # å·¥å…·çŠ¶æ€
            st.markdown("#### å·¥å…·çŠ¶æ€")
            tools_list = self.db_manager.get_tools_list()

            if tools_list:
                tools_df = pd.DataFrame(tools_list)
                st.dataframe(
                    tools_df[['tool_name', 'tool_type', 'version', 'status', 'updated_at']],
                    use_container_width=True,
                    hide_index=True
                )
            else:
                st.info("æš‚æ— å·²æ³¨å†Œçš„å·¥å…·")

        except Exception as e:
            st.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")

    def _render_system_config(self):
        """æ¸²æŸ“ç³»ç»Ÿé…ç½®é¡µé¢"""
        st.subheader("âš™ï¸ ç³»ç»Ÿé…ç½®")

        # é…ç½®é¡¹ç®¡ç†
        st.markdown("#### ç³»ç»Ÿé…ç½®é¡¹")

        # è·å–å½“å‰é…ç½®
        configs = [
            ("interference_threshold", "å¹²æ‰°é˜ˆå€¼", "-107", "å¹²æ‰°å€¼é˜ˆå€¼è®¾ç½®"),
            ("traffic_threshold", "æµé‡é˜ˆå€¼", "0.1", "æµé‡é˜ˆå€¼è®¾ç½®"),
            ("auto_backup", "è‡ªåŠ¨å¤‡ä»½", "false", "æ˜¯å¦å¯ç”¨è‡ªåŠ¨å¤‡ä»½"),
            ("log_retention", "æ—¥å¿—ä¿ç•™å¤©æ•°", "30", "æ—¥å¿—æ–‡ä»¶ä¿ç•™å¤©æ•°"),
            ("max_upload_size", "æœ€å¤§ä¸Šä¼ å¤§å°(MB)", "600", "æ–‡ä»¶ä¸Šä¼ å¤§å°é™åˆ¶"),
            ("backup_interval", "å¤‡ä»½é—´éš”(å°æ—¶)", "24", "è‡ªåŠ¨å¤‡ä»½é—´éš”æ—¶é—´")
        ]

        for config_key, config_name, default_value, description in configs:
            current_value = self.db_manager.get_system_config(
                config_key) or default_value

            col1, col2 = st.columns([2, 1])

            with col1:
                new_value = st.text_input(
                    f"{config_name} ({config_key})",
                    value=current_value,
                    help=description,
                    key=f"config_{config_key}"
                )

            with col2:
                if st.button(f"æ›´æ–°", key=f"update_{config_key}"):
                    if self.db_manager.set_system_config(
                            config_key, new_value, description=description):
                        st.success(f"{config_name} æ›´æ–°æˆåŠŸ")
                        st.rerun()
                    else:
                        st.error(f"{config_name} æ›´æ–°å¤±è´¥")

        st.divider()

        # æ‰¹é‡é…ç½®æ“ä½œ
        st.markdown("#### æ‰¹é‡é…ç½®æ“ä½œ")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸ”„ é‡ç½®ä¸ºé»˜è®¤å€¼", use_container_width=True):
                self._reset_to_default_config()

        with col2:
            if st.button("ğŸ’¾ å¯¼å‡ºé…ç½®", use_container_width=True):
                self._export_config()

        with col3:
            if st.button("ğŸ“¥ å¯¼å…¥é…ç½®", use_container_width=True):
                self._import_config()

    def _render_data_management(self):
        """æ¸²æŸ“æ•°æ®ç®¡ç†é¡µé¢"""
        st.subheader("ğŸ’¾ æ•°æ®ç®¡ç†")

        # æ•°æ®å¤‡ä»½
        st.markdown("#### æ•°æ®å¤‡ä»½")

        col1, col2 = st.columns(2)

        with col1:
            backup_name = st.text_input(
                "å¤‡ä»½åç§°", value=f"backup_{
                    datetime.now().strftime('%Y%m%d_%H%M%S')}")

        with col2:
            if st.button("ğŸ’¾ åˆ›å»ºå¤‡ä»½", use_container_width=True):
                backup_path = f"{backup_name}.db"
                if self.db_manager.backup_database(backup_path):
                    st.success(f"æ•°æ®åº“å¤‡ä»½æˆåŠŸ: {backup_path}")
                else:
                    st.error("æ•°æ®åº“å¤‡ä»½å¤±è´¥")

        st.divider()

        # æ•°æ®æ¸…ç†
        st.markdown("#### æ•°æ®æ¸…ç†")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸ—‘ï¸ æ¸…ç†æ—§æ—¥å¿—", use_container_width=True):
                self._cleanup_old_logs()

        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ•°æ®", use_container_width=True):
                self._cleanup_temp_data()

        with col3:
            if st.button("ğŸ—‘ï¸ æ¸…ç†é‡å¤æ•°æ®", use_container_width=True):
                self._cleanup_duplicate_data()
        st.divider()

        # æŒ‰æ—¥æœŸåˆ é™¤æ•°æ®
        st.markdown("#### æŒ‰æ—¥æœŸåˆ é™¤æ•°æ®ï¼ˆå¹²æ‰°æ•°æ®ä¸æ€§èƒ½æ•°æ®ï¼‰")
        st.info("æ ¹æ®æ—¥æœŸèŒƒå›´æ‰¹é‡åˆ é™¤å¹²æ‰°æ•°æ®å’Œæ€§èƒ½æ•°æ®ï¼Œæ“ä½œä¸å¯æ¢å¤ï¼Œè¯·è°¨æ…ä½¿ç”¨ã€‚")

        col_type, col_mode = st.columns(2)
        with col_type:
            delete_data_type = st.selectbox(
                "é€‰æ‹©æ•°æ®ç±»å‹",
                ["å¹²æ‰°æ•°æ®", "æ€§èƒ½æ•°æ®ï¼ˆå®¹é‡ï¼‰", "å¹²æ‰°+æ€§èƒ½ï¼ˆå®¹é‡ï¼‰"],
                key="delete_data_type"
            )
        with col_mode:
            delete_mode = st.radio(
                "åˆ é™¤èŒƒå›´",
                ["å•æ—¥", "æ—¥æœŸåŒºé—´"],
                horizontal=True,
                key="delete_date_mode"
            )

        if delete_mode == "å•æ—¥":
            target_date = st.date_input("é€‰æ‹©æ—¥æœŸ", key="delete_single_date")
            start_date = end_date = target_date
        else:
            col_sd, col_ed = st.columns(2)
            with col_sd:
                start_date = st.date_input("å¼€å§‹æ—¥æœŸ", key="delete_range_start")
            with col_ed:
                end_date = st.date_input("ç»“æŸæ—¥æœŸ", key="delete_range_end")

        st.warning(
            "âš ï¸ å°†åˆ é™¤æ‰€é€‰æ—¥æœŸèŒƒå›´å†…çš„å…¨éƒ¨ç›¸å…³è®°å½•ï¼Œåˆ é™¤åä¸å¯æ¢å¤ï¼Œå»ºè®®å…ˆå¤‡ä»½æ•°æ®åº“ã€‚"
        )
        confirm_text = st.text_input(
            "è¯·è¾“å…¥ DELETE ä»¥ç¡®è®¤åˆ é™¤ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰",
            key="delete_data_confirm"
        )

        if st.button(
            "ğŸ—‘ï¸ æŒ‰æ—¥æœŸåˆ é™¤æ•°æ®",
            type="secondary",
            use_container_width=True,
            disabled=(confirm_text != "DELETE")
        ):
            self._delete_data_by_date_range(delete_data_type, start_date, end_date)

        # æ•°æ®ç»Ÿè®¡
        st.markdown("#### æ•°æ®ç»Ÿè®¡")

        if st.button("ğŸ“Š ç”Ÿæˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Š", use_container_width=True):
            self._generate_data_statistics()

    def _render_log_viewer(self):
        """æ¸²æŸ“æ—¥å¿—æŸ¥çœ‹é¡µé¢"""
        st.subheader("ğŸ“‹ æ—¥å¿—æŸ¥çœ‹")

        # æ—¥å¿—ç­›é€‰
        col1, col2, col3 = st.columns(3)

        with col1:
            tool_filter = st.selectbox(
                "å·¥å…·ç­›é€‰",
                options=["å…¨éƒ¨"] + [tool['tool_name'] for tool in self.db_manager.get_tools_list()],
                key="log_tool_filter"
            )

        with col2:
            status_filter = st.selectbox(
                "çŠ¶æ€ç­›é€‰",
                options=["å…¨éƒ¨", "success", "error", "warning"],
                key="log_status_filter"
            )

        with col3:
            limit = st.number_input(
                "æ˜¾ç¤ºæ¡æ•°",
                value=100,
                min_value=10,
                max_value=1000,
                step=10)

        # æŸ¥è¯¢æ—¥å¿—
        if st.button("ğŸ” æŸ¥è¯¢æ—¥å¿—", type="primary"):
            self._display_logs(tool_filter, status_filter, limit)

        st.divider()

        # æ—¥å¿—æ“ä½œ
        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸ“¥ å¯¼å‡ºæ—¥å¿—", use_container_width=True):
                self._export_logs()

        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…ç†æ—¥å¿—", use_container_width=True):
                self._cleanup_logs()

        with col3:
            if st.button("ğŸ”„ åˆ·æ–°æ—¥å¿—", use_container_width=True):
                st.rerun()

    def _render_system_maintenance(self):
        """æ¸²æŸ“ç³»ç»Ÿç»´æŠ¤é¡µé¢"""
        st.subheader("ğŸ”§ ç³»ç»Ÿç»´æŠ¤")

        # æ•°æ®åº“ç»´æŠ¤
        st.markdown("#### æ•°æ®åº“ç»´æŠ¤")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            if st.button("ğŸ”§ ä¼˜åŒ–æ•°æ®åº“", use_container_width=True):
                self._optimize_database()

        with col2:
            if st.button("ğŸ” æ£€æŸ¥å®Œæ•´æ€§", use_container_width=True):
                self._check_database_integrity()

        with col3:
            if st.button("ğŸ“Š é‡å»ºç´¢å¼•", use_container_width=True):
                self._rebuild_indexes()
        
        with col4:
            if st.button("ğŸ” æ£€æŸ¥ç½‘æ ¼åç§°æ˜ å°„", use_container_width=True):
                self._check_grid_name_mapping()

        st.divider()
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        st.markdown("#### æ•°æ®è´¨é‡æ£€æŸ¥")
        
        if st.button("ğŸ” æ£€æŸ¥ç½‘æ ¼åç§°é”™è¯¯æ˜ å°„ï¼ˆåœ°å¸‚åç§°ï¼‰", type="primary"):
            self._check_grid_name_city_mapping()
        
        st.divider()

        # ç³»ç»Ÿä¿¡æ¯
        st.markdown("#### ç³»ç»Ÿä¿¡æ¯")

        try:
            # è·å–ç³»ç»Ÿä¿¡æ¯
            system_info = self._get_system_info()

            col1, col2 = st.columns(2)

            with col1:
                st.json(system_info)

            with col2:
                if st.button("ğŸ“‹ å¯¼å‡ºç³»ç»Ÿä¿¡æ¯", use_container_width=True):
                    self._export_system_info(system_info)

        except Exception as e:
            st.error(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")

        st.divider()

        # å±é™©æ“ä½œ
        st.markdown("#### âš ï¸ å±é™©æ“ä½œ")

        with st.expander("é‡ç½®ç³»ç»Ÿ", expanded=False):
            st.warning("âš ï¸ æ­¤æ“ä½œå°†æ¸…ç©ºæ‰€æœ‰æ•°æ®ï¼Œè¯·è°¨æ…æ“ä½œï¼")

            confirm_text = st.text_input("è¯·è¾“å…¥ 'RESET' ç¡®è®¤é‡ç½®")

            if st.button(
                "ğŸ—‘ï¸ é‡ç½®ç³»ç»Ÿ", type="secondary", disabled=(
                    confirm_text != "RESET")):
                self._reset_system()

    def _reset_to_default_config(self):
        """é‡ç½®ä¸ºé»˜è®¤é…ç½®"""
        default_configs = {
            "interference_threshold": "-107",
            "traffic_threshold": "0.1",
            "auto_backup": "false",
            "log_retention": "30",
            "max_upload_size": "600",
            "backup_interval": "24"
        }

        success_count = 0
        for key, value in default_configs.items():
            if self.db_manager.set_system_config(key, value):
                success_count += 1

        if success_count == len(default_configs):
            st.success("é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
        else:
            st.error(f"éƒ¨åˆ†é…ç½®é‡ç½®å¤±è´¥ï¼ŒæˆåŠŸ {success_count}/{len(default_configs)}")

    def _export_config(self):
        """å¯¼å‡ºé…ç½®"""
        try:
            configs = self.db_manager.execute_query(
                "SELECT * FROM system_config")
            config_df = pd.DataFrame(configs)

            csv = config_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                "ä¸‹è½½é…ç½®æ–‡ä»¶",
                data=csv,
                file_name=f"system_config_{
                    datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv")
        except Exception as e:
            st.error(f"å¯¼å‡ºé…ç½®å¤±è´¥: {e}")

    def _import_config(self):
        """å¯¼å…¥é…ç½®"""
        uploaded_file = st.file_uploader(
            "é€‰æ‹©é…ç½®æ–‡ä»¶", type=['csv'], key="config_import")

        if uploaded_file and st.button("å¯¼å…¥é…ç½®"):
            try:
                df = pd.read_csv(uploaded_file)

                success_count = 0
                for _, row in df.iterrows():
                    if self.db_manager.set_system_config(
                        row['config_key'],
                        row['config_value'],
                        row.get('config_type', 'string'),
                        row.get('description', '')
                    ):
                        success_count += 1

                st.success(f"é…ç½®å¯¼å…¥æˆåŠŸï¼Œå…±å¯¼å…¥ {success_count} æ¡é…ç½®")
                st.rerun()

            except Exception as e:
                st.error(f"å¯¼å…¥é…ç½®å¤±è´¥: {e}")

    def _cleanup_old_logs(self):
        """æ¸…ç†æ—§æ—¥å¿—"""
        try:
            retention_days = int(
                self.db_manager.get_system_config("log_retention") or "30")
            cutoff_date = datetime.now() - timedelta(days=retention_days)

            result = self.db_manager.execute_update(
                "DELETE FROM import_logs WHERE created_at < ?",
                (cutoff_date.strftime('%Y-%m-%d %H:%M:%S'),)
            )

            if result:
                st.success(f"å·²æ¸…ç† {retention_days} å¤©å‰çš„æ—¥å¿—")
            else:
                st.error("æ—¥å¿—æ¸…ç†å¤±è´¥")

        except Exception as e:
            st.error(f"æ¸…ç†æ—§æ—¥å¿—å¤±è´¥: {e}")

    def _cleanup_temp_data(self):
        """æ¸…ç†ä¸´æ—¶æ•°æ®"""
        st.markdown("#### ä¸´æ—¶æ•°æ®æ¸…ç†")

        # æ¸…ç†é€‰é¡¹
        st.markdown("##### æ¸…ç†é€‰é¡¹")
        col1, col2 = st.columns(2)
        with col1:
            clean_duplicates = st.checkbox(
                "æ¸…ç†é‡å¤æ•°æ®", value=True, help="æ¸…ç†å„è¡¨ä¸­çš„é‡å¤è®°å½•")
        with col2:
            clean_old_logs = st.checkbox(
                "æ¸…ç†æ—§æ—¥å¿—", value=True, help="æ¸…ç†è¶…è¿‡30å¤©çš„å¯¼å…¥æ—¥å¿—")

        col3, col4 = st.columns(2)
        with col3:
            clean_empty_records = st.checkbox(
                "æ¸…ç†ç©ºè®°å½•", value=True, help="æ¸…ç†å…³é”®å­—æ®µä¸ºç©ºçš„è®°å½•")
        with col4:
            clean_orphaned_records = st.checkbox(
                "æ¸…ç†å­¤ç«‹è®°å½•", value=True, help="æ¸…ç†æ²¡æœ‰å¯¹åº”æ˜ å°„çš„å°åŒºè®°å½•")

        # é«˜çº§é€‰é¡¹
        with st.expander("é«˜çº§æ¸…ç†é€‰é¡¹"):
            col5, col6 = st.columns(2)
            with col5:
                days_threshold = st.number_input(
                    "æ—¥å¿—ä¿ç•™å¤©æ•°", value=30, min_value=1, max_value=365, help="ä¿ç•™æœ€è¿‘Nå¤©çš„æ—¥å¿—")
            with col6:
                batch_size = st.number_input(
                    "æ‰¹å¤„ç†å¤§å°",
                    value=1000,
                    min_value=100,
                    max_value=10000,
                    help="æ¯æ¬¡æ¸…ç†çš„è®°å½•æ•°")

        if st.button("å¼€å§‹æ•°æ®æ¸…ç†", type="primary"):
            self._execute_data_cleanup(
                clean_duplicates,
                clean_old_logs,
                clean_empty_records,
                clean_orphaned_records,
                days_threshold,
                batch_size)

    def _execute_data_cleanup(
            self,
            clean_duplicates,
            clean_old_logs,
            clean_empty_records,
            clean_orphaned_records,
            days_threshold,
            batch_size):
        """æ‰§è¡Œæ•°æ®æ¸…ç†"""
        try:
            st.info("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…ç†...")
            cleanup_results = {}

            # 1. æ¸…ç†é‡å¤æ•°æ®
            if clean_duplicates:
                st.write("1ï¸âƒ£ æ¸…ç†é‡å¤æ•°æ®...")
                duplicates_result = self._clean_duplicate_data()
                cleanup_results['é‡å¤æ•°æ®'] = duplicates_result
                st.success(f"âœ… é‡å¤æ•°æ®æ¸…ç†å®Œæˆ: {duplicates_result}")

            # 2. æ¸…ç†æ—§æ—¥å¿—
            if clean_old_logs:
                st.write("2ï¸âƒ£ æ¸…ç†æ—§æ—¥å¿—...")
                logs_result = self._clean_old_logs(days_threshold)
                cleanup_results['æ—§æ—¥å¿—'] = logs_result
                st.success(f"âœ… æ—§æ—¥å¿—æ¸…ç†å®Œæˆ: {logs_result}")

            # 3. æ¸…ç†ç©ºè®°å½•
            if clean_empty_records:
                st.write("3ï¸âƒ£ æ¸…ç†ç©ºè®°å½•...")
                empty_result = self._clean_empty_records()
                cleanup_results['ç©ºè®°å½•'] = empty_result
                st.success(f"âœ… ç©ºè®°å½•æ¸…ç†å®Œæˆ: {empty_result}")

            # 4. æ¸…ç†å­¤ç«‹è®°å½•
            if clean_orphaned_records:
                st.write("4ï¸âƒ£ æ¸…ç†å­¤ç«‹è®°å½•...")
                orphaned_result = self._clean_orphaned_records()
                cleanup_results['å­¤ç«‹è®°å½•'] = orphaned_result
                st.success(f"âœ… å­¤ç«‹è®°å½•æ¸…ç†å®Œæˆ: {orphaned_result}")

            # æ˜¾ç¤ºæ¸…ç†ç»“æœ
            st.markdown("##### æ¸…ç†ç»“æœæ±‡æ€»")
            for item, result in cleanup_results.items():
                st.write(f"â€¢ {item}: {result}")

            st.success("ğŸ‰ æ•°æ®æ¸…ç†å®Œæˆï¼")

        except Exception as e:
            st.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")

    def _clean_duplicate_data(self):
        """æ¸…ç†é‡å¤æ•°æ®"""
        try:
            total_cleaned = 0

            # æ¸…ç†æ€§èƒ½æ•°æ®é‡å¤
            performance_duplicates = self.db_manager.execute_update("""
                DELETE FROM performance_data
                WHERE id NOT IN (
                    SELECT MIN(id) FROM performance_data
                    GROUP BY data_type, start_time, cgi
                )
            """)
            total_cleaned += performance_duplicates

            # æ¸…ç†å¹²æ‰°æ•°æ®é‡å¤
            interference_duplicates = self.db_manager.execute_update("""
                DELETE FROM interference_data
                WHERE id NOT IN (
                    SELECT MIN(id) FROM interference_data
                    GROUP BY start_time, cgi
                )
            """)
            total_cleaned += interference_duplicates

            return f"æ¸…ç†äº† {total_cleaned} æ¡é‡å¤è®°å½•"

        except Exception as e:
            return f"æ¸…ç†å¤±è´¥: {e}"

    def _clean_old_logs(self, days_threshold):
        """æ¸…ç†æ—§æ—¥å¿—"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_threshold)
            cutoff_str = cutoff_date.strftime('%Y-%m-%d %H:%M:%S')

            # æ¸…ç†å¯¼å…¥æ—¥å¿—
            logs_cleaned = self.db_manager.execute_update("""
                DELETE FROM import_logs
                WHERE created_at < ?
            """, (cutoff_str,))

            return f"æ¸…ç†äº† {logs_cleaned} æ¡æ—§æ—¥å¿—è®°å½•"

        except Exception as e:
            return f"æ¸…ç†å¤±è´¥: {e}"

    def _clean_empty_records(self):
        """æ¸…ç†ç©ºè®°å½•"""
        try:
            total_cleaned = 0

            # æ¸…ç†æ€§èƒ½æ•°æ®ä¸­çš„ç©ºè®°å½•
            empty_performance = self.db_manager.execute_update("""
                DELETE FROM performance_data
                WHERE cgi IS NULL OR cgi = '' OR start_time IS NULL OR start_time = ''
            """)
            total_cleaned += empty_performance

            # æ¸…ç†å¹²æ‰°æ•°æ®ä¸­çš„ç©ºè®°å½•
            empty_interference = self.db_manager.execute_update("""
                DELETE FROM interference_data
                WHERE cgi IS NULL OR cgi = '' OR start_time IS NULL OR start_time = ''
            """)
            total_cleaned += empty_interference

            return f"æ¸…ç†äº† {total_cleaned} æ¡ç©ºè®°å½•"

        except Exception as e:
            return f"æ¸…ç†å¤±è´¥: {e}"

    def _clean_orphaned_records(self):
        """æ¸…ç†å­¤ç«‹è®°å½•"""
        try:
            total_cleaned = 0

            # æ¸…ç†æ²¡æœ‰å¯¹åº”æ˜ å°„çš„æ€§èƒ½æ•°æ®
            orphaned_performance = self.db_manager.execute_update("""
                DELETE FROM performance_data
                WHERE cgi NOT IN (SELECT cgi FROM cell_mapping WHERE cgi IS NOT NULL)
            """)
            total_cleaned += orphaned_performance

            # æ¸…ç†æ²¡æœ‰å¯¹åº”æ˜ å°„çš„å¹²æ‰°æ•°æ®
            orphaned_interference = self.db_manager.execute_update("""
                DELETE FROM interference_data
                WHERE cgi NOT IN (SELECT cgi FROM cell_mapping WHERE cgi IS NOT NULL)
            """)
            total_cleaned += orphaned_interference

            return f"æ¸…ç†äº† {total_cleaned} æ¡å­¤ç«‹è®°å½•"

        except Exception as e:
            return f"æ¸…ç†å¤±è´¥: {e}"

    def _delete_data_by_date_range(self, data_type_label, start_date, end_date):
        """æŒ‰æ—¥æœŸèŒƒå›´åˆ é™¤å¹²æ‰°æ•°æ®å’Œæ€§èƒ½æ•°æ®ï¼ˆå®¹é‡ï¼‰"""
        try:
            if start_date > end_date:
                st.error("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
                return

            deleted_messages = []

            # 1. å¹²æ‰°æ•°æ®ï¼šä½¿ç”¨ date_str (YYYYMMDD)
            if data_type_label in ("å¹²æ‰°æ•°æ®", "å¹²æ‰°+æ€§èƒ½ï¼ˆå®¹é‡ï¼‰"):
                s_str = start_date.strftime('%Y%m%d')
                e_str = end_date.strftime('%Y%m%d')

                # æŸ¥è¯¢å°†è¢«åˆ é™¤çš„è®°å½•æ•°
                count_sql = """
                SELECT COUNT(*) as count FROM interference_data
                WHERE date_str BETWEEN ? AND ?
                """
                count_result = self.db_manager.execute_query(count_sql, (s_str, e_str))
                del_count = count_result[0]['count'] if count_result else 0

                if del_count > 0:
                    delete_sql = """
                    DELETE FROM interference_data
                    WHERE date_str BETWEEN ? AND ?
                    """
                    ok = self.db_manager.execute_update(delete_sql, (s_str, e_str))
                    if ok:
                        deleted_messages.append(f"å¹²æ‰°æ•°æ® {del_count} æ¡")
                    else:
                        st.error("åˆ é™¤å¹²æ‰°æ•°æ®å¤±è´¥")
                else:
                    deleted_messages.append("å¹²æ‰°æ•°æ® 0 æ¡ï¼ˆæ‰€é€‰æ—¥æœŸæ— æ•°æ®ï¼‰")

            # 2. æ€§èƒ½æ•°æ®ï¼ˆå®¹é‡ï¼‰ï¼šä½¿ç”¨ start_timeï¼ŒæŒ‰æ—¥æœŸæˆªå–
            if data_type_label in ("æ€§èƒ½æ•°æ®ï¼ˆå®¹é‡ï¼‰", "å¹²æ‰°+æ€§èƒ½ï¼ˆå®¹é‡ï¼‰"):
                s_dt = start_date.strftime('%Y-%m-%d 00:00:00')
                e_dt = end_date.strftime('%Y-%m-%d 23:59:59')

                count_sql = """
                SELECT COUNT(*) as count FROM performance_data
                WHERE data_type = 'capacity'
                  AND start_time BETWEEN ? AND ?
                """
                count_result = self.db_manager.execute_query(count_sql, (s_dt, e_dt))
                del_count = count_result[0]['count'] if count_result else 0

                if del_count > 0:
                    delete_sql = """
                    DELETE FROM performance_data
                    WHERE data_type = 'capacity'
                      AND start_time BETWEEN ? AND ?
                    """
                    ok = self.db_manager.execute_update(delete_sql, (s_dt, e_dt))
                    if ok:
                        deleted_messages.append(f"æ€§èƒ½æ•°æ®ï¼ˆå®¹é‡ï¼‰ {del_count} æ¡")
                    else:
                        st.error("åˆ é™¤æ€§èƒ½æ•°æ®ï¼ˆå®¹é‡ï¼‰å¤±è´¥")
                else:
                    deleted_messages.append("æ€§èƒ½æ•°æ®ï¼ˆå®¹é‡ï¼‰ 0 æ¡ï¼ˆæ‰€é€‰æ—¥æœŸæ— æ•°æ®ï¼‰")

            if deleted_messages:
                msg = "ï¼›".join(deleted_messages)
                st.success(f"åˆ é™¤å®Œæˆï¼š{msg}")
            else:
                st.info("æœªæ‰§è¡Œåˆ é™¤æ“ä½œ")

        except Exception as e:
            st.error(f"æŒ‰æ—¥æœŸåˆ é™¤æ•°æ®å¤±è´¥: {e}")

    def _cleanup_duplicate_data(self):
        """æ¸…ç†é‡å¤æ•°æ®"""
        try:
            # æ¸…ç†é‡å¤çš„å¹²æ‰°æ•°æ®
            interference_duplicates = self.db_manager.execute_update("""
                DELETE FROM interference_data
                WHERE id NOT IN (
                    SELECT MIN(id) FROM interference_data
                    GROUP BY date_str, cgi
                )
            """)

            # æ¸…ç†é‡å¤çš„å®¹é‡æ•°æ®
            capacity_duplicates = self.db_manager.execute_update("""
                DELETE FROM capacity_data
                WHERE id NOT IN (
                    SELECT MIN(id) FROM capacity_data
                    GROUP BY start_time, cgi
                )
            """)

            # æ¸…ç†é‡å¤çš„æµé‡æ•°æ®
            traffic_duplicates = self.db_manager.execute_update("""
                DELETE FROM performance_data
                WHERE data_type = 'traffic'
                AND id NOT IN (
                    SELECT MIN(id) FROM performance_data
                    WHERE data_type = 'traffic'
                    GROUP BY start_time, cgi
                )
            """)

            st.success("é‡å¤æ•°æ®æ¸…ç†å®Œæˆ")

        except Exception as e:
            st.error(f"æ¸…ç†é‡å¤æ•°æ®å¤±è´¥: {e}")

    def _generate_data_statistics(self):
        """ç”Ÿæˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Š"""
        try:
            stats = self.db_manager.get_database_stats()

            # åˆ›å»ºç»Ÿè®¡æŠ¥å‘Š
            report = {
                "ç”Ÿæˆæ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "æ•°æ®åº“å¤§å°": f"{stats.get('db_size_mb', 0):.1f} MB",
                "å„è¡¨è®°å½•æ•°": {
                    "å°åŒºæ˜ å°„è¡¨": stats.get('cell_mapping_count', 0),
                    "å·¥å‚è¡¨": stats.get('engineering_params_count', 0),
                    "å¹²æ‰°æ•°æ®è¡¨": stats.get('interference_data_count', 0),
                    "å®¹é‡æ•°æ®è¡¨": stats.get('capacity_data_count', 0),
                    "æµé‡æ•°æ®è¡¨": self.db_manager.execute_query("SELECT COUNT(*) as count FROM performance_data WHERE data_type = 'traffic'")[0]['count'],
                    "å·¥å…·è¡¨": stats.get('tools_count', 0),
                    "é…ç½®è¡¨": stats.get('system_config_count', 0),
                    "æ—¥å¿—è¡¨": stats.get('import_logs_count', 0)
                }
            }

            st.json(report)

            # ä¸‹è½½ç»Ÿè®¡æŠ¥å‘Š
            report_json = json.dumps(report, ensure_ascii=False, indent=2)
            st.download_button(
                "ä¸‹è½½ç»Ÿè®¡æŠ¥å‘Š",
                data=report_json.encode('utf-8'),
                file_name=f"data_statistics_{
                    datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json")

        except Exception as e:
            st.error(f"ç”Ÿæˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Šå¤±è´¥: {e}")

    def _display_logs(self, tool_filter, status_filter, limit):
        """æ˜¾ç¤ºæ—¥å¿—"""
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_conditions = []
            params = []

            if tool_filter != "å…¨éƒ¨":
                where_conditions.append("tool_name = ?")
                params.append(tool_filter)

            if status_filter != "å…¨éƒ¨":
                where_conditions.append("status = ?")
                params.append(status_filter)

            where_clause = " AND ".join(
                where_conditions) if where_conditions else "1=1"

            sql = """
            SELECT * FROM import_logs
            WHERE {where_clause}
            ORDER BY created_at DESC
            LIMIT ?
            """
            params.append(limit)

            logs = self.db_manager.execute_query(sql, tuple(params))

            if logs:
                logs_df = pd.DataFrame(logs)
                st.dataframe(
                    logs_df,
                    use_container_width=True,
                    hide_index=True)

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»è®°å½•æ•°", len(logs))
                with col2:
                    success_count = len(
                        [log for log in logs if log['status'] == 'success'])
                    st.metric("æˆåŠŸè®°å½•", success_count)
                with col3:
                    error_count = len(
                        [log for log in logs if log['status'] == 'error'])
                    st.metric("å¤±è´¥è®°å½•", error_count)
            else:
                st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ—¥å¿—è®°å½•")

        except Exception as e:
            st.error(f"æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {e}")

    def _export_logs(self):
        """å¯¼å‡ºæ—¥å¿—"""
        try:
            logs = self.db_manager.get_import_logs(limit=1000)
            logs_df = pd.DataFrame(logs)

            csv = logs_df.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                "ä¸‹è½½æ—¥å¿—æ–‡ä»¶",
                data=csv,
                file_name=f"import_logs_{
                    datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv")
        except Exception as e:
            st.error(f"å¯¼å‡ºæ—¥å¿—å¤±è´¥: {e}")

    def _cleanup_logs(self):
        """æ¸…ç†æ—¥å¿—"""
        try:
            # ä¿ç•™æœ€è¿‘30å¤©çš„æ—¥å¿—
            cutoff_date = datetime.now() - timedelta(days=30)

            result = self.db_manager.execute_update(
                "DELETE FROM import_logs WHERE created_at < ?",
                (cutoff_date.strftime('%Y-%m-%d %H:%M:%S'),)
            )

            if result:
                st.success("æ—¥å¿—æ¸…ç†å®Œæˆ")
            else:
                st.error("æ—¥å¿—æ¸…ç†å¤±è´¥")

        except Exception as e:
            st.error(f"æ¸…ç†æ—¥å¿—å¤±è´¥: {e}")

    def _optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        try:
            # æ‰§è¡ŒVACUUMæ“ä½œ
            self.db_manager.execute_update("VACUUM")
            st.success("æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")
        except Exception as e:
            st.error(f"æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")

    def _check_database_integrity(self):
        """æ£€æŸ¥æ•°æ®åº“å®Œæ•´æ€§"""
        try:
            # æ‰§è¡ŒPRAGMA integrity_check
            result = self.db_manager.execute_query("PRAGMA integrity_check")

            if result and result[0]['integrity_check'] == 'ok':
                st.success("æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
            else:
                st.error("æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥")

        except Exception as e:
            st.error(f"æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")

    def _rebuild_indexes(self):
        """é‡å»ºç´¢å¼•"""
        try:
            # åˆ é™¤æ‰€æœ‰ç´¢å¼•
            indexes = self.db_manager.execute_query(
                "SELECT name FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%'")

            for index in indexes:
                self.db_manager.execute_update(
                    f"DROP INDEX IF EXISTS {index['name']}")

            # é‡æ–°åˆ›å»ºç´¢å¼•
            self.db_manager._create_indexes(self.db_manager.get_connection())

            st.success("ç´¢å¼•é‡å»ºå®Œæˆ")

        except Exception as e:
            st.error(f"ç´¢å¼•é‡å»ºå¤±è´¥: {e}")

    def _get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            stats = self.db_manager.get_database_stats()

            return {
                "ç³»ç»Ÿç‰ˆæœ¬": "ä¼˜åŒ–ç™¾å®ç®±å·¥å…·é›† v1.0.0",
                "æ•°æ®åº“è·¯å¾„": self.db_manager.db_path,
                "æ•°æ®åº“å¤§å°": f"{stats.get('db_size_mb', 0):.1f} MB",
                "è®°å½•æ€»æ•°": sum([
                    stats.get('cell_mapping_count', 0),
                    stats.get('engineering_params_count', 0),
                    stats.get('interference_data_count', 0),
                    self.db_manager.execute_query("SELECT COUNT(*) as count FROM performance_data WHERE data_type = 'capacity'")[0]['count'],
                    stats.get('traffic_data_count', 0)
                ]),
                "å·¥å…·æ•°é‡": stats.get('tools_count', 0),
                "é…ç½®é¡¹æ•°é‡": stats.get('system_config_count', 0)
            }
        except Exception as e:
            return {"é”™è¯¯": str(e)}

    def _export_system_info(self, system_info):
        """å¯¼å‡ºç³»ç»Ÿä¿¡æ¯"""
        try:
            info_json = json.dumps(system_info, ensure_ascii=False, indent=2)
            st.download_button(
                "ä¸‹è½½ç³»ç»Ÿä¿¡æ¯",
                data=info_json.encode('utf-8'),
                file_name=f"system_info_{
                    datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json")
        except Exception as e:
            st.error(f"å¯¼å‡ºç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")

    def _reset_system(self):
        """é‡ç½®ç³»ç»Ÿ"""
        try:
            # æ¸…ç©ºæ‰€æœ‰æ•°æ®è¡¨
            tables = [
                'cell_mapping', 'engineering_params', 'interference_data',
                'capacity_data', 'traffic_data', 'import_logs'
            ]

            for table in tables:
                self.db_manager.execute_update(f"DELETE FROM {table}")

            st.success("ç³»ç»Ÿé‡ç½®å®Œæˆ")
            st.rerun()

        except Exception as e:
            st.error(f"ç³»ç»Ÿé‡ç½®å¤±è´¥: {e}")
    
    def _check_grid_name_mapping(self):
        """æ£€æŸ¥ç½‘æ ¼åç§°æ˜ å°„"""
        try:
            st.info("ğŸ” æ­£åœ¨æ£€æŸ¥ç½‘æ ¼åç§°æ˜ å°„...")
            
            # ç»Ÿè®¡ç½‘æ ¼åç§°çš„åˆ†å¸ƒæƒ…å†µ
            query = """
                SELECT 
                    grid_name,
                    COUNT(*) as count,
                    COUNT(DISTINCT grid_id) as unique_grid_ids,
                    COUNT(DISTINCT cgi) as unique_cgis
                FROM cell_mapping
                WHERE grid_name IS NOT NULL AND grid_name != ''
                GROUP BY grid_name
                ORDER BY count DESC
                LIMIT 50
            """
            
            results = self.db_manager.execute_query(query)
            
            if results:
                df = pd.DataFrame(results)
                st.success(f"âœ… æ£€æŸ¥å®Œæˆï¼Œå…±å‘ç° {len(df)} ç§ä¸åŒçš„ç½‘æ ¼åç§°")
                st.dataframe(df, use_container_width=True)
            else:
                st.warning("âš ï¸ æœªæ‰¾åˆ°ç½‘æ ¼åç§°æ•°æ®")
                
        except Exception as e:
            st.error(f"æ£€æŸ¥ç½‘æ ¼åç§°æ˜ å°„å¤±è´¥: {e}")
            self.logger.error(f"æ£€æŸ¥ç½‘æ ¼åç§°æ˜ å°„å¤±è´¥: {e}")
    
    def _check_grid_name_city_mapping(self):
        """æ£€æŸ¥ç½‘æ ¼åç§°é”™è¯¯æ˜ å°„ä¸ºåœ°å¸‚åç§°çš„è®°å½•"""
        try:
            st.info("ğŸ” æ­£åœ¨æ£€æŸ¥ç½‘æ ¼åç§°é”™è¯¯æ˜ å°„ï¼ˆåœ°å¸‚åç§°ï¼‰...")
            
            # å¸¸è§çš„åœ°å¸‚åç§°åˆ—è¡¨ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µæ‰©å±•ï¼‰
            city_names = [
                'é˜³æ±Ÿ', 'å¹¿å·', 'æ·±åœ³', 'ç æµ·', 'æ±•å¤´', 'ä½›å±±', 'éŸ¶å…³', 'æ¹›æ±Ÿ',
                'è‚‡åº†', 'æ±Ÿé—¨', 'èŒ‚å', 'æƒ å·', 'æ¢…å·', 'æ±•å°¾', 'æ²³æº', 'æ¸…è¿œ',
                'ä¸œè', 'ä¸­å±±', 'æ½®å·', 'æ­é˜³', 'äº‘æµ®', 'é˜³æ±Ÿå¸‚', 'å¹¿å·å¸‚', 'æ·±åœ³å¸‚'
            ]
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            city_conditions = "', '".join(city_names)
            query = f"""
                SELECT 
                    cgi,
                    celname,
                    grid_id,
                    grid_name,
                    COUNT(*) as count
                FROM cell_mapping
                WHERE grid_name IN ('{city_conditions}')
                GROUP BY cgi, grid_id, grid_name
                ORDER BY count DESC
            """
            
            results = self.db_manager.execute_query(query)
            
            if results:
                df = pd.DataFrame(results)
                
                # ç»Ÿè®¡ä¿¡æ¯
                total_records = len(df)
                unique_cgis = df['cgi'].nunique()
                unique_grid_ids = df['grid_id'].nunique()
                city_distribution = df['grid_name'].value_counts()
                
                st.warning(f"âš ï¸ **å‘ç° {total_records} æ¡ç½‘æ ¼åç§°é”™è¯¯æ˜ å°„ä¸ºåœ°å¸‚åç§°çš„è®°å½•**")
                st.write(f"  â€¢ æ¶‰åŠå°åŒºæ•°: {unique_cgis:,}")
                st.write(f"  â€¢ æ¶‰åŠç½‘æ ¼IDæ•°: {unique_grid_ids:,}")
                
                st.markdown("#### åœ°å¸‚åˆ†å¸ƒç»Ÿè®¡")
                st.dataframe(city_distribution.reset_index().rename(columns={'index': 'åœ°å¸‚åç§°', 'grid_name': 'è®°å½•æ•°'}), use_container_width=True)
                
                st.markdown("#### è¯¦ç»†è®°å½•ï¼ˆå‰100æ¡ï¼‰")
                st.dataframe(df.head(100), use_container_width=True)
                
                # æä¾›ä¿®å¤å»ºè®®
                st.markdown("#### ğŸ’¡ ä¿®å¤å»ºè®®")
                st.info("""
                è¿™äº›è®°å½•çš„ç½‘æ ¼åç§°è¢«é”™è¯¯åœ°æ˜ å°„ä¸ºåœ°å¸‚åç§°ã€‚ä¿®å¤æ–¹æ³•ï¼š
                1. é‡æ–°è¿è¡Œ"æ˜ å°„å°åŒºæ•°æ®å¯¼å…¥"åŠŸèƒ½ï¼ˆå·²ä¿®å¤æ˜ å°„é€»è¾‘ï¼‰
                2. ç³»ç»Ÿä¼šä¼˜å…ˆä»engineering_paramsè¡¨çš„"grid_name_no_buffer"æˆ–"grid_name_buffer_500m"å­—æ®µè·å–ç½‘æ ¼åç§°
                3. ä¸å†ä½¿ç”¨CSVæ–‡ä»¶çš„"åœ°å¸‚"åˆ—ä½œä¸ºç½‘æ ¼åç§°
                """)
                
                # å¯¼å‡ºåŠŸèƒ½
                if st.button("ğŸ“¥ å¯¼å‡ºé”™è¯¯æ˜ å°„è®°å½•", type="primary"):
                    csv = df.to_csv(index=False).encode('utf-8-sig')
                    st.download_button(
                        "ä¸‹è½½CSVæ–‡ä»¶",
                        data=csv,
                        file_name=f"ç½‘æ ¼åç§°é”™è¯¯æ˜ å°„æ£€æŸ¥_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
            else:
                st.success("âœ… æœªå‘ç°ç½‘æ ¼åç§°é”™è¯¯æ˜ å°„ä¸ºåœ°å¸‚åç§°çš„è®°å½•")
                
        except Exception as e:
            st.error(f"æ£€æŸ¥ç½‘æ ¼åç§°é”™è¯¯æ˜ å°„å¤±è´¥: {e}")
            self.logger.error(f"æ£€æŸ¥ç½‘æ ¼åç§°é”™è¯¯æ˜ å°„å¤±è´¥: {e}")
