#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµé‡ç›‘æ§åˆ†æå·¥å…· - æœ€ç»ˆç‰ˆæœ¬
"""

import streamlit as st
import pandas as pd
import io
from datetime import date, datetime, timedelta
import logging

class TrafficMonitor:
    """æµé‡ç›‘æ§åˆ†æå·¥å…·"""
    
    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)
    
    @staticmethod
    def determine_network_type(row):
        """
        åˆ¤æ–­å°åŒºåˆ¶å¼ï¼ˆ4G/5Gï¼‰
        
        è§„åˆ™ï¼š
        1. performance_data.pinduan ä¸­ï¼Œ4.9GHzã€2.6GHzã€700M ä¸º 5G å°åŒº
        2. cell_mapping.zhishi ä¸º '5G' ä¸º 5G å°åŒº
        3. å…¶ä¸­ä¸€ä¸ªè¡¨åˆ¤æ–­ä¸º 5G åˆ™ä¸º 5Gï¼Œå…¶ä»–åˆ™ä¸º 4G
        
        Args:
            row: DataFrame è¡Œæ•°æ®ï¼Œéœ€åŒ…å« 'pinduan' å’Œ 'zhishi' å­—æ®µ
            
        Returns:
            str: '5g' æˆ– '4g'
        """
        # 5G é¢‘æ®µåˆ—è¡¨ï¼ˆä»…åŒ…å«çœŸæ­£çš„5Gé¢‘æ®µï¼‰
        freq_5g = ['4.9GHz', '2.6GHz', '700M']
        
        # è·å–é¢‘æ®µå’Œåˆ¶å¼
        pinduan = row.get('pinduan', '')
        zhishi = row.get('zhishi', '')
        
        # åˆ¤æ–­é€»è¾‘ï¼šä»»ä¸€æ¡ä»¶æ»¡è¶³å³ä¸º 5G
        if pinduan in freq_5g or zhishi == '5G':
            return '5g'
        else:
            return '4g'

    def render(self):
        """æ¸²æŸ“å®¹æ™ºç­–ç•¥åˆ†æå¼•æ“ç•Œé¢"""
        st.title("ğŸ“ˆ å®¹æ™ºç­–ç•¥åˆ†æå¼•æ“")
        st.caption("ç½‘ç»œå®¹é‡ç­–ç•¥åˆ†æä¸æ€§èƒ½ä¼˜åŒ–å¹³å°")

        # åŠŸèƒ½å¯¼èˆª
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“Š é›¶ä½æµé‡åˆ†æ", 
            "ğŸ“ˆ æµé‡éª¤é™åˆ†æ", 
            "âš¡ é«˜è´Ÿè·å°åŒºæŸ¥è¯¢",
            "ğŸ” å°åŒºæŸ¥è¯¢",
            "ğŸ¯ æµé‡çªé™åˆ†æ"
        ])

        with tab1:
            self._render_zero_low_traffic_analysis()

        with tab2:
            self._render_traffic_drop_analysis()

        with tab3:
            self._render_high_load_analysis()

        with tab4:
            self._render_cell_query()

        with tab5:
            self._render_traffic_spike_analysis()

    def _render_zero_low_traffic_analysis(self):
        """æ¸²æŸ“é›¶ä½æµé‡åˆ†æé¡µé¢"""
        st.subheader("ğŸ“Š é›¶ä½æµé‡åˆ†æ")
        st.caption("åˆ†æé›¶æµé‡å’Œä½æµé‡å°åŒºï¼Œæ”¯æŒ4G/5Gä¸åŒé˜ˆå€¼")
        
        # è·å–æœ‰æ•°æ®çš„æ—¥æœŸåˆ—è¡¨
        available_dates = self._get_available_dates()
        
        # æ—¥æœŸèŒƒå›´é€‰æ‹©
        st.markdown("#### ğŸ“… æ—¶é—´èŒƒå›´")
        col_d1, col_d2 = st.columns(2)
        with col_d1:
            start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=date.today() - timedelta(days=7),
                key="zero_low_start_date"
            )
        with col_d2:
            end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ",
                value=date.today(),
                key="zero_low_end_date"
            )
        
        # æ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸæç¤º
        if available_dates:
            self._display_date_availability(available_dates, start_date, end_date)
        
        # é˜ˆå€¼è®¾ç½®
        st.markdown("#### é˜ˆå€¼è®¾ç½®")
        col1, col2 = st.columns(2)
        with col1:
            threshold_4g = st.number_input(
                "4Gä½æµé‡é˜ˆå€¼ (GB)",
                min_value=0.0,
                max_value=100.0,
                value=1.0,
                step=0.1,
                key="threshold_4g",
                help="4Gå°åŒºæµé‡ä½äºæ­¤é˜ˆå€¼å°†è¢«è¯†åˆ«ä¸ºä½æµé‡å°åŒº"
            )
        with col2:
            threshold_5g = st.number_input(
                "5Gä½æµé‡é˜ˆå€¼ (GB)",
                min_value=0.0,
                max_value=100.0,
                value=1.0,
                step=0.1,
                key="threshold_5g",
                help="5Gå°åŒºæµé‡ä½äºæ­¤é˜ˆå€¼å°†è¢«è¯†åˆ«ä¸ºä½æµé‡å°åŒº"
            )
        
        if st.button("å¼€å§‹åˆ†æ", key="analyze_zero_low_traffic"):
            # åˆ›å»ºè¿›åº¦æ¡å’Œæ—¥å¿—å®¹å™¨
            progress_bar = st.progress(0)
            status_text = st.empty()
            log_container = st.expander("ğŸ“‹ åˆ†ææ—¥å¿—", expanded=True)
            
            try:
                with log_container:
                    st.write("---")
                    st.write(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    st.write(f"ğŸ“… åˆ†ææ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
                    st.write(f"ğŸ“Š åˆ†æå¤©æ•°: {(end_date - start_date).days + 1} å¤©")
                    st.write(f"ğŸ¯ 4Gé˜ˆå€¼: {threshold_4g} GB")
                    st.write(f"ğŸ¯ 5Gé˜ˆå€¼: {threshold_5g} GB")
                    st.write("---")
                
                # æ­¥éª¤1: ç”Ÿæˆé›¶æµé‡åˆ†æ
                status_text.text("ğŸ” æ­£åœ¨æ‰§è¡Œé›¶æµé‡åˆ†æ...")
                progress_bar.progress(20)
                with log_container:
                    st.write("ğŸ” æ­¥éª¤ 1/4: å¼€å§‹é›¶æµé‡åˆ†æ...")
                
                zero_df = self._generate_zero_traffic_analysis(start_date, end_date)
                
                with log_container:
                    st.write(f"âœ… é›¶æµé‡åˆ†æå®Œæˆï¼Œå…±å‘ç° {len(zero_df)} ä¸ªé›¶æµé‡å°åŒº")
                    if not zero_df.empty and 'åˆ¶å¼' in zero_df.columns:
                        zero_4g = len(zero_df[zero_df['åˆ¶å¼'] == '4g'])
                        zero_5g = len(zero_df[zero_df['åˆ¶å¼'] == '5g'])
                        st.write(f"   - 4Gå°åŒº: {zero_4g} ä¸ª")
                        st.write(f"   - 5Gå°åŒº: {zero_5g} ä¸ª")
                
                progress_bar.progress(50)
                
                # æ­¥éª¤2: ç”Ÿæˆä½æµé‡åˆ†æ
                status_text.text("ğŸ” æ­£åœ¨æ‰§è¡Œä½æµé‡åˆ†æ...")
                with log_container:
                    st.write("ğŸ” æ­¥éª¤ 2/4: å¼€å§‹ä½æµé‡åˆ†æ...")
                
                low_df = self._generate_low_traffic_analysis(
                    start_date, end_date, threshold_4g, threshold_5g
                )
                
                with log_container:
                    st.write(f"âœ… ä½æµé‡åˆ†æå®Œæˆï¼Œå…±å‘ç° {len(low_df)} ä¸ªä½æµé‡å°åŒº")
                    if not low_df.empty and 'åˆ¶å¼' in low_df.columns:
                        low_4g = len(low_df[low_df['åˆ¶å¼'] == '4g'])
                        low_5g = len(low_df[low_df['åˆ¶å¼'] == '5g'])
                        st.write(f"   - 4Gå°åŒº: {low_4g} ä¸ª")
                        st.write(f"   - 5Gå°åŒº: {low_5g} ä¸ª")
                
                progress_bar.progress(80)
                
                # æ­¥éª¤3: ä¿å­˜æ•°æ®
                status_text.text("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†æç»“æœ...")
                with log_container:
                    st.write("ğŸ’¾ æ­¥éª¤ 3/4: ä¿å­˜åˆ†æç»“æœ...")
                
                st.session_state['zero_traffic_df'] = zero_df
                st.session_state['low_traffic_df'] = low_df
                st.session_state['zero_low_start_date_saved'] = start_date
                st.session_state['zero_low_end_date_saved'] = end_date
                st.session_state['zero_low_threshold_4g'] = threshold_4g
                st.session_state['zero_low_threshold_5g'] = threshold_5g
                
                progress_bar.progress(100)
                
                with log_container:
                    st.write("âœ… æ•°æ®ä¿å­˜å®Œæˆ")
                    st.write("---")
                    st.write(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    total_cells = len(zero_df) + len(low_df)
                    st.write(f"ğŸ“Š åˆ†æç»“æœæ±‡æ€»: å…±å‘ç° {total_cells} ä¸ªé—®é¢˜å°åŒº")
                
                # å®Œæˆ
                status_text.empty()
                progress_bar.empty()
                st.success("âœ… é›¶ä½æµé‡åˆ†æå®Œæˆï¼")
                
            except Exception as e:
                progress_bar.empty()
                status_text.empty()
                with log_container:
                    st.write("---")
                    st.write(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
                    st.write(f"â° å¤±è´¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                st.error(f"åˆ†æå¤±è´¥: {e}")
                self.logger.error(f"é›¶ä½æµé‡åˆ†æå¤±è´¥: {e}")
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        if 'zero_traffic_df' in st.session_state and not st.session_state['zero_traffic_df'].empty:
            zero_df = st.session_state['zero_traffic_df']
            low_df = st.session_state['low_traffic_df']
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("é›¶æµé‡å°åŒº", len(zero_df))
            with col2:
                st.metric("ä½æµé‡å°åŒº", len(low_df))
            with col3:
                zero_4g = len(zero_df[zero_df['åˆ¶å¼'] == '4g']) if 'åˆ¶å¼' in zero_df.columns and not zero_df.empty else 0
                low_4g = len(low_df[low_df['åˆ¶å¼'] == '4g']) if 'åˆ¶å¼' in low_df.columns and not low_df.empty else 0
                st.metric("4Gå°åŒº", zero_4g + low_4g)
            with col4:
                zero_5g = len(zero_df[zero_df['åˆ¶å¼'] == '5g']) if 'åˆ¶å¼' in zero_df.columns and not zero_df.empty else 0
                low_5g = len(low_df[low_df['åˆ¶å¼'] == '5g']) if 'åˆ¶å¼' in low_df.columns and not low_df.empty else 0
                st.metric("5Gå°åŒº", zero_5g + low_5g)
            
            # å¯¼å‡ºExcelæ–‡ä»¶
            if st.button("å¯¼å‡ºExcelæ–‡ä»¶", key="export_zero_low_traffic"):
                self._export_zero_low_traffic_excel(
                    zero_df, low_df,
                    st.session_state['zero_low_start_date_saved'],
                    st.session_state['zero_low_end_date_saved'],
                    st.session_state.get('zero_low_threshold_4g', 1.0),
                    st.session_state.get('zero_low_threshold_5g', 1.0)
                )

    def _render_traffic_drop_analysis(self):
        """æ¸²æŸ“æµé‡éª¤é™åˆ†æé¡µé¢"""
        st.subheader("ğŸ“ˆ æµé‡éª¤é™åˆ†æ")
        st.caption("å¯¹æ¯”ä¸¤ä¸ªæ—¶é—´æ®µçš„æµé‡å˜åŒ–ï¼Œè¯†åˆ«éª¤é™å°åŒº")
        
        # è·å–æœ‰æ•°æ®çš„æ—¥æœŸåˆ—è¡¨
        available_dates = self._get_available_dates()
        
        # å†å²æ—¶é—´æ®µï¼ˆç”¨äºå¯¹æ¯”çš„åŸºå‡†ï¼‰
        st.markdown("#### ğŸ“… å†å²æ—¶é—´æ®µï¼ˆå¯¹æ¯”åŸºå‡†ï¼‰")
        st.caption("ğŸ’¡ è¿™æ˜¯å†å²æ—¶é—´æ®µï¼Œç”¨äºä½œä¸ºæµé‡å¯¹æ¯”çš„åŸºå‡†")
        col1, col2 = st.columns(2)
        with col1:
            before_start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=date.today() - timedelta(days=10),
                key="before_start_date"
            )
        with col2:
            before_end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ",
                value=date.today() - timedelta(days=7),
                key="before_end_date"
            )
        
        # æ˜¾ç¤ºå†å²æ—¶é—´æ®µçš„æ•°æ®å¯ç”¨æ€§
        if available_dates:
            self._display_date_availability(available_dates, before_start_date, before_end_date)
        
        # å½“å‰æ—¶é—´æ®µï¼ˆéœ€è¦å¯¹æ¯”çš„æ—¶é—´æ®µï¼‰
        st.markdown("#### ğŸ“… å½“å‰æ—¶é—´æ®µï¼ˆéœ€è¦å¯¹æ¯”çš„æ—¶æ®µï¼‰")
        st.caption("ğŸ’¡ è¿™æ˜¯éœ€è¦å¯¹æ¯”çš„æ—¶é—´æ®µï¼Œå¦‚æœæµé‡è¾ƒå†å²æ—¶æ®µä¸‹é™æ˜æ˜¾ï¼Œå°†è¢«è¯†åˆ«ä¸ºéª¤é™")
        col1, col2 = st.columns(2)
        with col1:
            after_start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=date.today() - timedelta(days=3),
                key="after_start_date"
            )
        with col2:
            after_end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ",
                value=date.today(),
                key="after_end_date"
            )
        
        # æ˜¾ç¤ºå½“å‰æ—¶é—´æ®µçš„æ•°æ®å¯ç”¨æ€§
        if available_dates:
            self._display_date_availability(available_dates, after_start_date, after_end_date)
        
        # éª¤é™é˜ˆå€¼è®¾ç½®
        st.markdown("#### âš™ï¸ é˜ˆå€¼è®¾ç½®")
        drop_threshold = st.slider(
            "éª¤é™é˜ˆå€¼ (%)",
            min_value=10,
            max_value=90,
            value=50,
            step=10,
            key="traffic_drop_threshold",
            help="å¹³å‡æµé‡ä¸‹é™è¶…è¿‡æ­¤ç™¾åˆ†æ¯”å°†è¢«è¯†åˆ«ä¸ºéª¤é™"
        )
        
        if st.button("å¼€å§‹åˆ†æ", key="analyze_traffic_drop"):
            # åˆ›å»ºè¿›åº¦æ¡å’Œæ—¥å¿—å®¹å™¨
            progress_bar = st.progress(0)
            status_text = st.empty()
            log_container = st.expander("ğŸ“‹ åˆ†ææ—¥å¿—", expanded=True)
            
            try:
                with log_container:
                    st.write("---")
                    st.write(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    st.write(f"ğŸ“… å†å²æ—¶é—´æ®µï¼ˆåŸºå‡†ï¼‰: {before_start_date} è‡³ {before_end_date}")
                    st.write(f"ğŸ“… å½“å‰æ—¶é—´æ®µï¼ˆå¯¹æ¯”ï¼‰: {after_start_date} è‡³ {after_end_date}")
                    st.write(f"ğŸ¯ éª¤é™é˜ˆå€¼: {drop_threshold}%")
                    st.write("---")
                
                # æ­¥éª¤1: æ‰§è¡Œæµé‡éª¤é™åˆ†æ
                status_text.text("ğŸ” æ­£åœ¨æ‰§è¡Œæµé‡éª¤é™åˆ†æ...")
                progress_bar.progress(30)
                with log_container:
                    st.write("ğŸ” æ­¥éª¤ 1/3: å¼€å§‹æµé‡éª¤é™åˆ†æ...")
                
                df = self._generate_traffic_drop_analysis(
                    before_start_date, before_end_date,
                    after_start_date, after_end_date,
                    drop_threshold
                )
                
                with log_container:
                    st.write(f"âœ… æµé‡éª¤é™åˆ†æå®Œæˆï¼Œå…±å‘ç° {len(df)} ä¸ªéª¤é™å°åŒº")
                    if not df.empty:
                        # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
                        st.write("ğŸ“Š å‰5ä¸ªéª¤é™å°åŒº:")
                        display_cols = ['CGI', 'å°åŒºåç§°', 'å¯¹æ¯”å‰å¹³å‡æµé‡(GB)', 'å¯¹æ¯”åå¹³å‡æµé‡(GB)', 'æµé‡é™å¹…(%)']
                        available_cols = [col for col in display_cols if col in df.columns]
                        st.dataframe(df[available_cols].head(5))
                        
                        # æ£€æŸ¥ç‰¹å®šå°åŒº
                        target_cgi = '460-00-442681-65'
                        target_cell = df[df['CGI'] == target_cgi]
                        if not target_cell.empty:
                            st.write(f"âœ… æ‰¾åˆ°ç›®æ ‡å°åŒº {target_cgi}:")
                            st.write(target_cell[available_cols].to_string())
                        else:
                            st.write(f"âŒ æœªæ‰¾åˆ°ç›®æ ‡å°åŒº {target_cgi}")
                
                progress_bar.progress(80)
                
                # æ­¥éª¤2: ä¿å­˜æ•°æ®
                status_text.text("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†æç»“æœ...")
                with log_container:
                    st.write("ğŸ’¾ æ­¥éª¤ 2/3: ä¿å­˜åˆ†æç»“æœ...")
                
                st.session_state['traffic_drop_df'] = df
                st.session_state['traffic_drop_before_start'] = before_start_date
                st.session_state['traffic_drop_before_end'] = before_end_date
                st.session_state['traffic_drop_after_start'] = after_start_date
                st.session_state['traffic_drop_after_end'] = after_end_date
                
                progress_bar.progress(100)
                
                with log_container:
                    st.write("âœ… æ•°æ®ä¿å­˜å®Œæˆ")
                    st.write("---")
                    st.write(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                # å®Œæˆ
                status_text.empty()
                progress_bar.empty()
                st.success(f"âœ… æµé‡éª¤é™åˆ†æå®Œæˆï¼Œå…±å‘ç° {len(df)} ä¸ªéª¤é™å°åŒº")
                
            except Exception as e:
                progress_bar.empty()
                status_text.empty()
                with log_container:
                    st.write("---")
                    st.write(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
                    st.write(f"â° å¤±è´¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                st.error(f"åˆ†æå¤±è´¥: {e}")
                self.logger.error(f"æµé‡éª¤é™åˆ†æå¤±è´¥: {e}")
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        if 'traffic_drop_df' in st.session_state and not st.session_state['traffic_drop_df'].empty:
            df = st.session_state['traffic_drop_df']
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€»éª¤é™å°åŒºæ•°", len(df))
            with col2:
                avg_drop = df['æµé‡é™å¹…(%)'].mean() if 'æµé‡é™å¹…(%)' in df.columns else 0
                st.metric("å¹³å‡é™å¹…", f"{avg_drop:.1f}%")
            with col3:
                max_drop = df['æµé‡é™å¹…(%)'].max() if 'æµé‡é™å¹…(%)' in df.columns else 0
                st.metric("æœ€å¤§é™å¹…", f"{max_drop:.1f}%")
            with col4:
                drop_4g = len(df[df['åˆ¶å¼'] == '4g']) if 'åˆ¶å¼' in df.columns else 0
                drop_5g = len(df[df['åˆ¶å¼'] == '5g']) if 'åˆ¶å¼' in df.columns else 0
                st.metric("4G/5Gå°åŒº", f"{drop_4g}/{drop_5g}")
            
            # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
            st.dataframe(df, use_container_width=True)
            
            # å¯¼å‡ºExcelæ–‡ä»¶
            if st.button("å¯¼å‡ºExcelæ–‡ä»¶", key="export_traffic_drop"):
                self._export_traffic_drop_excel(
                    df,
                    st.session_state['traffic_drop_before_start'],
                    st.session_state['traffic_drop_before_end'],
                    st.session_state['traffic_drop_after_start'],
                    st.session_state['traffic_drop_after_end']
                )

    def _render_high_load_analysis(self):
        """æ¸²æŸ“é«˜è´Ÿè·å°åŒºæŸ¥è¯¢é¡µé¢"""
        st.subheader("âš¡ é«˜è´Ÿè·å°åŒºæŸ¥è¯¢")
        st.caption("æŸ¥è¯¢æŒ‡å®šæ—¶é—´å†…çš„é«˜è´Ÿè·å°åŒº")
        
        # æ—¶é—´èŒƒå›´è®¾ç½®
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=date.today() - timedelta(days=7),
                key="high_load_start_date"
            )
        with col2:
            end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ",
                value=date.today(),
                key="high_load_end_date"
            )
        
        # è¯´æ˜ä¿¡æ¯
        st.info("ğŸ’¡ é«˜è´Ÿè·å°åŒºæŸ¥è¯¢ï¼šæŸ¥è¯¢æŒ‡å®šæ—¶é—´å†… if_overcel='t' çš„å°åŒºæ¸…å•")
        
        if st.button("å¼€å§‹æŸ¥è¯¢", key="query_high_load"):
            try:
                summary_df, detail_df = self._generate_high_load_analysis(start_date, end_date)
                
                # ä¿å­˜åˆ°session stateï¼ˆå³ä½¿ä¸ºç©ºä¹Ÿä¿å­˜ï¼Œä»¥ä¾¿æ˜¾ç¤ºæç¤ºä¿¡æ¯ï¼‰
                st.session_state['high_load_summary'] = summary_df
                st.session_state['high_load_detail'] = detail_df
                # æ³¨æ„ï¼šä¸éœ€è¦æ‰‹åŠ¨è®¾ç½® high_load_start_date å’Œ high_load_end_date
                # å› ä¸º date_input widget å·²ç»è‡ªåŠ¨ç®¡ç†è¿™äº› session state å€¼
                
                if summary_df.empty:
                    st.warning("âš ï¸ æŸ¥è¯¢å®Œæˆï¼Œä½†åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°é«˜è´Ÿè·å°åŒºæ•°æ®")
                else:
                    st.success(f"âœ… é«˜è´Ÿè·å°åŒºæŸ¥è¯¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(summary_df)} ä¸ªé«˜è´Ÿè·å°åŒº")
                
            except Exception as e:
                st.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
                self.logger.error(f"é«˜è´Ÿè·å°åŒºæŸ¥è¯¢å¤±è´¥: {e}")
                # æ¸…é™¤ä¹‹å‰çš„ç»“æœ
                if 'high_load_summary' in st.session_state:
                    del st.session_state['high_load_summary']
                if 'high_load_detail' in st.session_state:
                    del st.session_state['high_load_detail']
        
        # æ˜¾ç¤ºæŸ¥è¯¢ç»“æœ
        if 'high_load_summary' in st.session_state:
            summary_df = st.session_state['high_load_summary']
            detail_df = st.session_state.get('high_load_detail', pd.DataFrame())
            
            if summary_df.empty:
                st.info("ğŸ“Š å½“å‰æ²¡æœ‰é«˜è´Ÿè·å°åŒºæ•°æ®ï¼Œè¯·è°ƒæ•´æŸ¥è¯¢æ—¶é—´èŒƒå›´æˆ–æ£€æŸ¥æ•°æ®")
            else:
                # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
                st.markdown("#### å°åŒºæ±‡æ€»æ¸…å•")
                st.dataframe(summary_df, use_container_width=True)
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("é«˜è´Ÿè·å°åŒºæ•°", len(summary_df))
                with col2:
                    st.metric("å¹³å‡é«˜è´Ÿè·æ¬¡æ•°", round(summary_df['é«˜è´Ÿè·æ¬¡æ•°'].mean(), 1) if 'é«˜è´Ÿè·æ¬¡æ•°' in summary_df.columns else 0)
                with col3:
                    st.metric("æœ€å¤§é«˜è´Ÿè·æ¬¡æ•°", summary_df['é«˜è´Ÿè·æ¬¡æ•°'].max() if 'é«˜è´Ÿè·æ¬¡æ•°' in summary_df.columns else 0)
                
                # æ˜¾ç¤ºè¯¦ç»†æ¸…å•
                st.markdown("#### å°åŒºè´Ÿè·è¯¦ç»†æ¸…å•")
                if not detail_df.empty:
                    st.dataframe(detail_df, use_container_width=True)
                else:
                    st.info("æš‚æ— è¯¦ç»†æ•°æ®")
                
                # å¯¼å‡ºExcelæ–‡ä»¶
                if st.button("å¯¼å‡ºExcelæ–‡ä»¶", key="export_high_load"):
                    self._export_high_load_excel(summary_df, detail_df, st.session_state.get('high_load_start_date', start_date), st.session_state.get('high_load_end_date', end_date))

    def _render_cell_query(self):
        """æ¸²æŸ“å°åŒºæŸ¥è¯¢é¡µé¢"""
        st.subheader("ğŸ” å°åŒºæŸ¥è¯¢")
        st.caption("æŸ¥è¯¢æ€§èƒ½è¡¨ä¸­çš„å°åŒºæ•°æ®ï¼ˆä¸å…³è”æ˜ å°„è¡¨ï¼‰")
        
        # æŸ¥è¯¢æ¡ä»¶
        col1, col2 = st.columns(2)
        with col1:
            query_type = st.selectbox(
                "æŸ¥è¯¢ç±»å‹",
                ["æŒ‰å°åŒºåç§°", "æŒ‰CGI"],
                key="cell_query_type"
            )
        with col2:
            query_value = st.text_input(
                "æŸ¥è¯¢å€¼",
                placeholder="è¯·è¾“å…¥æŸ¥è¯¢æ¡ä»¶",
                key="cell_query_value"
            )
        
        # æ—¥æœŸèŒƒå›´
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=date.today() - timedelta(days=7),
                key="cell_query_start_date"
            )
        with col2:
            end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ",
                value=date.today(),
                key="cell_query_end_date"
            )
        
        if st.button("å¼€å§‹æŸ¥è¯¢", key="query_cells"):
            try:
                df = self._generate_cell_query(query_type, query_value, start_date, end_date)
                
                # ä¿å­˜åˆ°session stateï¼ˆä½¿ç”¨ä¸åŒçš„keyåç§°é¿å…å†²çªï¼‰
                st.session_state['cell_query_df'] = df
                st.session_state['cell_query_type_saved'] = query_type
                st.session_state['cell_query_value_saved'] = query_value
                
                if not df.empty:
                    st.success(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(df)} ä¸ªå°åŒº")
                else:
                    st.warning("æœªæ‰¾åˆ°åŒ¹é…çš„å°åŒº")
                    
            except Exception as e:
                st.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
                self.logger.error(f"å°åŒºæŸ¥è¯¢å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºæŸ¥è¯¢ç»“æœ
        if 'cell_query_df' in st.session_state and not st.session_state['cell_query_df'].empty:
            df = st.session_state['cell_query_df']
            
            st.dataframe(df, use_container_width=True)
            
            # å¯¼å‡ºExcelæ–‡ä»¶
            if st.button("å¯¼å‡ºExcelæ–‡ä»¶", key="export_cell_query"):
                self._export_cell_query_excel(
                    df, 
                    st.session_state.get('cell_query_type_saved', 'æœªçŸ¥'), 
                    st.session_state.get('cell_query_value_saved', '')
                )

    def _generate_zero_traffic_analysis(self, start_date, end_date):
        """ç”Ÿæˆé›¶æµé‡åˆ†æï¼ˆæ”¯æŒæ—¶é—´èŒƒå›´ï¼ŒæŒ‰å°åŒºèšåˆï¼‰"""
        try:
            start_str = start_date.strftime('%Y-%m-%d 00:00:00')
            end_str = end_date.strftime('%Y-%m-%d 23:59:59')
            
            # ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢æ‰€æœ‰æ˜ å°„å°åŒºï¼ŒåŒ…æ‹¬æ²¡æœ‰æ€§èƒ½æ•°æ®çš„å°åŒº
            # ä½¿ç”¨ LEFT JOIN è·å–æ‰€æœ‰æ˜ å°„å°åŒºï¼Œå¹¶å…³è”å·¥å‚è¡¨è·å–ç‰©ç†ç«™ä¿¡æ¯
            query = '''
                SELECT DISTINCT
                    c.cgi, c.celname, c.grid_id, c.zhishi, c.pinduan,
                    c.grid_name, c.grid_pp, c.tt_mark, c.if_flag, 
                    c.if_cell, c.if_online, c.lon, c.lat, e.phy_name
                FROM cell_mapping c
                LEFT JOIN engineering_params e ON c.cgi = e.cgi
            '''
            
            all_cells_df = pd.DataFrame(self.db_manager.execute_query(query))
            
            if all_cells_df.empty:
                return pd.DataFrame()
            
            # ç¬¬äºŒæ­¥ï¼šæŸ¥è¯¢æ—¶é—´èŒƒå›´å†…æœ‰æ€§èƒ½æ•°æ®çš„å°åŒº
            query_with_data = '''
                SELECT
                    c.cgi, c.celname, c.grid_id, c.zhishi, c.pinduan,
                    c.grid_name, c.grid_pp, c.tt_mark, c.if_flag, 
                    c.if_cell, c.if_online, c.lon, c.lat, e.phy_name,
                    p.start_time, p.flwor_day
                FROM cell_mapping c
                LEFT JOIN engineering_params e ON c.cgi = e.cgi
                LEFT JOIN performance_data p ON c.cgi = p.cgi 
                    AND p.start_time BETWEEN ? AND ? 
                    AND p.data_type = 'capacity'
                WHERE p.cgi IS NOT NULL
            '''
            
            df = pd.DataFrame(self.db_manager.execute_query(
                query_with_data, [start_str, end_str]
            ))
            
            # ç¬¬ä¸‰æ­¥ï¼šæŒ‰å°åŒºèšåˆï¼Œè®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            def aggregate_date_flow(group):
                """èšåˆæ—¥æœŸå’Œæµé‡ä¿¡æ¯ä¸ºä¸€åˆ—ï¼Œæ ¼å¼ï¼š2025-10-17(1.23)ã€2025-10-18(2.34)"""
                # å…ˆæŒ‰æ—¥æœŸå»é‡ï¼Œç¡®ä¿æ¯ä¸ªæ—¥æœŸåªæœ‰ä¸€æ¡è®°å½•ï¼ˆå¦‚æœåŒä¸€æ—¥æœŸæœ‰å¤šæ¡ï¼Œå–æœ€åä¸€æ¡ï¼‰
                # æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆå»æ‰æ—¶åˆ†ç§’ï¼‰
                group_copy = group.copy()
                group_copy['date_only'] = pd.to_datetime(group_copy['start_time']).dt.date
                
                # å†æ¬¡éªŒè¯æ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿åªå¤„ç†åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                start_date_only = start_date
                end_date_only = end_date
                group_copy = group_copy[
                    (group_copy['date_only'] >= start_date_only) & 
                    (group_copy['date_only'] <= end_date_only)
                ]
                
                if group_copy.empty:
                    return ''
                
                # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œå–æ¯ç»„æœ€åä¸€æ¡è®°å½•ï¼ˆä¿æŒæœ€æ–°æ•°æ®ï¼‰
                deduplicated = group_copy.sort_values('start_time').drop_duplicates(subset=['date_only'], keep='last')
                # æŒ‰æ—¥æœŸæ’åº
                sorted_data = deduplicated.sort_values('start_time')
                # ç»„åˆæ—¥æœŸå’Œæµé‡
                date_flow_list = []
                for _, row in sorted_data.iterrows():
                    if pd.notna(row['start_time']) and pd.notna(row['flwor_day']):
                        date_flow_list.append(f"{row['start_time']}({row['flwor_day']:.2f})")
                return 'ã€'.join(date_flow_list) if date_flow_list else ''
            
            # å¤„ç†æœ‰æ•°æ®çš„å°åŒº
            agg_dict = {
                'celname': 'first',
                'grid_id': 'first',
                'zhishi': 'first',
                'pinduan': 'first',
                'grid_name': 'first',
                'grid_pp': 'first',
                'tt_mark': 'first',
                'if_flag': 'first',
                'if_cell': 'first',
                'if_online': 'first',
                'lon': 'first',
                'lat': 'first',
                'phy_name': 'first',
                'flwor_day': ['count', 'mean', 'max']
            }
            
            # å¤„ç†æœ‰æ•°æ®çš„å°åŒº
            if not df.empty:
                # å†æ¬¡è¿‡æ»¤æ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿åªç»Ÿè®¡æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                df['start_time_dt'] = pd.to_datetime(df['start_time'])
                start_date_dt = pd.to_datetime(start_str)
                end_date_dt = pd.to_datetime(end_str)
                df_filtered = df[
                    (df['start_time_dt'] >= start_date_dt) & 
                    (df['start_time_dt'] <= end_date_dt)
                ].copy()
                
                if df_filtered.empty:
                    grouped_df = pd.DataFrame()
                else:
                    # æ·»åŠ æ—¥æœŸåˆ—ç”¨äºè®¡ç®—å¤©æ•°
                    df_filtered['date_only'] = pd.to_datetime(df_filtered['start_time']).dt.date
                    
                    # ä¿®æ”¹èšåˆå­—å…¸ï¼Œæ­£ç¡®è®¡ç®—å¤©æ•°
                    agg_dict_corrected = agg_dict.copy()
                    agg_dict_corrected['date_only'] = 'nunique'  # è®¡ç®—å”¯ä¸€æ—¥æœŸæ•°
                    
                    grouped_df = df_filtered.groupby('cgi').agg(agg_dict_corrected).reset_index()
                    
                    # å±•å¹³å¤šçº§åˆ—å
                    grouped_df.columns = [
                        'cgi', 'celname', 'grid_id', 'zhishi', 'pinduan',
                        'grid_name', 'grid_pp', 'tt_mark', 'if_flag', 'if_cell',
                        'if_online', 'lon', 'lat', 'phy_name', 'data_days', 'avg_flow', 'max_flow', 'actual_days'
                    ]
                    
                    # ä½¿ç”¨å®é™…å¤©æ•°æ›¿æ¢åŸæ¥çš„è®°å½•æ•°
                    grouped_df['data_days'] = grouped_df['actual_days']
                    grouped_df = grouped_df.drop('actual_days', axis=1)
                    
                    # æ·»åŠ æ—¥æœŸæµé‡æ˜ç»†åˆ—ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®ï¼‰
                    date_flow_detail = df_filtered.groupby('cgi').apply(aggregate_date_flow).reset_index()
                    date_flow_detail.columns = ['cgi', 'date_flow_detail']
                    
                    # åˆå¹¶æ•°æ®
                    grouped_df = grouped_df.merge(date_flow_detail, on='cgi', how='left')
                
            else:
                grouped_df = pd.DataFrame()
            
            # ç¬¬å››æ­¥ï¼šå¤„ç†æ²¡æœ‰æ•°æ®çš„å°åŒº
            # ä»æ‰€æœ‰å°åŒºä¸­ç­›é€‰å‡ºæœ‰æ•°æ®çš„å°åŒº
            if not grouped_df.empty:
                cells_with_data = set(grouped_df['cgi'].unique())
            else:
                cells_with_data = set()
            
            # è·å–æ‰€æœ‰æ˜ å°„å°åŒºä¸­ï¼Œåœ¨æŸ¥è¯¢æ—¶é—´æ®µå†…æ²¡æœ‰æ•°æ®çš„å°åŒº
            all_cgis = set(all_cells_df['cgi'].unique())
            cells_without_data = all_cgis - cells_with_data
            
            # ä¸ºæ²¡æœ‰æ•°æ®çš„å°åŒºåˆ›å»ºè®°å½•
            no_data_list = []
            for cgi in cells_without_data:
                cell_info = all_cells_df[all_cells_df['cgi'] == cgi].iloc[0]
                no_data_list.append({
                    'cgi': cgi,
                    'celname': cell_info.get('celname'),
                    'grid_id': cell_info.get('grid_id'),
                    'zhishi': cell_info.get('zhishi'),
                    'pinduan': cell_info.get('pinduan'),
                    'grid_name': cell_info.get('grid_name'),
                    'grid_pp': cell_info.get('grid_pp'),
                    'tt_mark': cell_info.get('tt_mark'),
                    'if_flag': cell_info.get('if_flag'),
                    'if_cell': cell_info.get('if_cell'),
                    'if_online': cell_info.get('if_online'),
                    'lon': cell_info.get('lon'),
                    'lat': cell_info.get('lat'),
                    'phy_name': cell_info.get('phy_name'),
                    'data_days': 0,
                    'avg_flow': 0.0,
                    'max_flow': 0.0,
                    'date_flow_detail': ''
                })
            
            no_data_df = pd.DataFrame(no_data_list)
            
            # åˆå¹¶æœ‰æ•°æ®å’Œæ— æ•°æ®çš„å°åŒº
            if not grouped_df.empty and not no_data_df.empty:
                full_df = pd.concat([grouped_df, no_data_df], ignore_index=True)
            elif not grouped_df.empty:
                full_df = grouped_df
            elif not no_data_df.empty:
                full_df = no_data_df
            else:
                return pd.DataFrame()
            
            # é‡æ–°åˆ¤æ–­åˆ¶å¼ï¼ˆåŸºäº pinduan å’Œ zhishiï¼‰
            full_df['network_type'] = full_df.apply(self.determine_network_type, axis=1)
            
            # ç¬¬äº”æ­¥ï¼šç­›é€‰é›¶æµé‡å°åŒº
            # åŒ…æ‹¬ï¼š1) æœ‰æ•°æ®ä½†æœ€å¤§æµé‡ä¸º0çš„å°åŒº  2) å®Œå…¨æ²¡æœ‰æ•°æ®çš„å°åŒº
            full_df['is_zero_flow'] = (
                (full_df['data_days'] == 0) |  # æ²¡æœ‰æ•°æ®çš„å°åŒº
                ((full_df['data_days'] > 0) & (full_df['max_flow'] == 0))  # æœ‰æ•°æ®ä½†æµé‡ä¸º0çš„å°åŒº
            )
            
            zero_df = full_df[full_df['is_zero_flow']].copy()
            
            # åˆ é™¤è¾…åŠ©åˆ—
            zero_df = zero_df.drop(columns=['is_zero_flow', 'max_flow'])
            
            # æ·»åŠ é—®é¢˜ç±»å‹æ ‡ç­¾
            # åŒºåˆ†ï¼šå®Œå…¨æ²¡æœ‰æ•°æ®çš„æ ‡è®°ä¸º"é›¶æµé‡å°åŒº"ï¼Œæœ‰æ•°æ®ä½†æµé‡ä¸º0çš„æ ‡è®°ä¸º"é›¶æµé‡"
            zero_df['é—®é¢˜ç±»å‹'] = zero_df.apply(
                lambda row: 'é›¶æµé‡å°åŒº' if row['data_days'] == 0 else 'é›¶æµé‡',
                axis=1
            )
            
            # é‡å‘½ååˆ—åä¸ºä¸­æ–‡
            chinese_columns = {
                'cgi': 'CGI',
                'celname': 'å°åŒºåç§°',
                'grid_id': 'ç½‘æ ¼ID',
                'zhishi': 'åˆ¶å¼(åŸå§‹)',
                'pinduan': 'é¢‘æ®µ',
                'network_type': 'åˆ¶å¼',
                'grid_name': 'ç½‘æ ¼åç§°',
                'grid_pp': 'ç½‘æ ¼æ ‡ç­¾',
                'tt_mark': 'å¤‡æ³¨',
                'if_flag': 'æ˜¯å¦ç¼“å†²åŒº',
                'if_cell': 'æ˜¯å¦æ˜ å°„å°åŒº',
                'if_online': 'æ˜¯å¦åœ¨ç½‘ç®¡',
                'lon': 'ç»åº¦',
                'lat': 'çº¬åº¦',
                'phy_name': 'ç‰©ç†ç«™',
                'data_days': 'æ•°æ®å¤©æ•°',
                'avg_flow': 'æ—¥å¹³å‡æµé‡(GB)',
                'date_flow_detail': 'æ—¥æœŸæµé‡æ˜ç»†'
            }
            zero_df = zero_df.rename(columns=chinese_columns)
            
            # åˆ é™¤åŸå§‹åˆ¶å¼åˆ—
            if 'åˆ¶å¼(åŸå§‹)' in zero_df.columns:
                zero_df = zero_df.drop(columns=['åˆ¶å¼(åŸå§‹)'])
            
            return zero_df
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆé›¶æµé‡åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()

    def _generate_low_traffic_analysis(self, start_date, end_date, 
                                       threshold_4g, threshold_5g):
        """ç”Ÿæˆä½æµé‡åˆ†æï¼ˆæ”¯æŒæ—¶é—´èŒƒå›´ï¼ŒæŒ‰å°åŒºèšåˆï¼‰"""
        try:
            start_str = start_date.strftime('%Y-%m-%d 00:00:00')
            end_str = end_date.strftime('%Y-%m-%d 23:59:59')
            
            # ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢æ—¶é—´èŒƒå›´å†…æœ‰æ€§èƒ½æ•°æ®çš„å°åŒºçš„æµé‡æ•°æ®
            # æ³¨æ„ï¼šä½¿ç”¨ INNER JOIN åªæŸ¥è¯¢æœ‰æ•°æ®çš„å°åŒºï¼Œé¿å… NULL å€¼å½±å“èšåˆç»“æœ
            query = '''
                SELECT
                    c.cgi, c.celname, c.grid_id, c.zhishi, c.pinduan,
                    c.grid_name, c.grid_pp, c.tt_mark, c.if_flag, 
                    c.if_cell, c.if_online, c.lon, c.lat, e.phy_name,
                    p.start_time, p.flwor_day
                FROM cell_mapping c
                INNER JOIN performance_data p ON c.cgi = p.cgi 
                    AND p.start_time BETWEEN ? AND ? 
                    AND p.data_type = 'capacity'
                LEFT JOIN engineering_params e ON c.cgi = e.cgi
            '''
            
            df = pd.DataFrame(self.db_manager.execute_query(
                query, [start_str, end_str]
            ))
            if df.empty:
                return pd.DataFrame()
            
            # ç¬¬äºŒæ­¥ï¼šæŒ‰å°åŒºèšåˆï¼Œè®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            # å†æ¬¡è¿‡æ»¤æ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿åªç»Ÿè®¡æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
            df['start_time_dt'] = pd.to_datetime(df['start_time'])
            start_date_dt = pd.to_datetime(start_str)
            end_date_dt = pd.to_datetime(end_str)
            df_filtered = df[
                (df['start_time_dt'] >= start_date_dt) & 
                (df['start_time_dt'] <= end_date_dt)
            ].copy()
            
            if df_filtered.empty:
                return pd.DataFrame()
            
            def aggregate_date_flow(group):
                """èšåˆæ—¥æœŸå’Œæµé‡ä¿¡æ¯ä¸ºä¸€åˆ—ï¼Œæ ¼å¼ï¼š2025-10-17(1.23)ã€2025-10-18(2.34)"""
                # å…ˆæŒ‰æ—¥æœŸå»é‡ï¼Œç¡®ä¿æ¯ä¸ªæ—¥æœŸåªæœ‰ä¸€æ¡è®°å½•ï¼ˆå¦‚æœåŒä¸€æ—¥æœŸæœ‰å¤šæ¡ï¼Œå–æœ€åä¸€æ¡ï¼‰
                # æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆå»æ‰æ—¶åˆ†ç§’ï¼‰
                group_copy = group.copy()
                group_copy['date_only'] = pd.to_datetime(group_copy['start_time']).dt.date
                
                # å†æ¬¡éªŒè¯æ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿åªå¤„ç†åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                start_date_only = start_date
                end_date_only = end_date
                group_copy = group_copy[
                    (group_copy['date_only'] >= start_date_only) & 
                    (group_copy['date_only'] <= end_date_only)
                ]
                
                if group_copy.empty:
                    return ''
                
                # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œå–æ¯ç»„æœ€åä¸€æ¡è®°å½•ï¼ˆä¿æŒæœ€æ–°æ•°æ®ï¼‰
                deduplicated = group_copy.sort_values('start_time').drop_duplicates(subset=['date_only'], keep='last')
                # æŒ‰æ—¥æœŸæ’åº
                sorted_data = deduplicated.sort_values('start_time')
                # ç»„åˆæ—¥æœŸå’Œæµé‡
                date_flow_list = []
                for _, row in sorted_data.iterrows():
                    if pd.notna(row['start_time']) and pd.notna(row['flwor_day']):
                        date_flow_list.append(f"{row['start_time']}({row['flwor_day']:.2f})")
                return 'ã€'.join(date_flow_list) if date_flow_list else ''
            
            agg_dict = {
                'celname': 'first',
                'grid_id': 'first',
                'zhishi': 'first',
                'pinduan': 'first',
                'grid_name': 'first',
                'grid_pp': 'first',
                'tt_mark': 'first',
                'if_flag': 'first',
                'if_cell': 'first',
                'if_online': 'first',
                'lon': 'first',
                'lat': 'first',
                'phy_name': 'first',
                'flwor_day': ['count', 'mean', 'max']
            }
            
            # æ·»åŠ æ—¥æœŸåˆ—ç”¨äºè®¡ç®—å¤©æ•°
            df_filtered['date_only'] = pd.to_datetime(df_filtered['start_time']).dt.date
            
            # ä¿®æ”¹èšåˆå­—å…¸ï¼Œæ­£ç¡®è®¡ç®—å¤©æ•°
            agg_dict_corrected = agg_dict.copy()
            agg_dict_corrected['date_only'] = 'nunique'  # è®¡ç®—å”¯ä¸€æ—¥æœŸæ•°
            
            # å…ˆè¿›è¡ŒåŸºæœ¬èšåˆï¼ˆä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®ï¼‰
            grouped_df = df_filtered.groupby('cgi').agg(agg_dict_corrected).reset_index()
            
            # å±•å¹³å¤šçº§åˆ—å
            grouped_df.columns = [
                    'cgi', 'celname', 'grid_id', 'zhishi', 'pinduan',
                    'grid_name', 'grid_pp', 'tt_mark', 'if_flag', 'if_cell',
                'if_online', 'lon', 'lat', 'phy_name', 'data_days', 'avg_flow', 'max_flow', 'actual_days'
            ]
            
            # ä½¿ç”¨å®é™…å¤©æ•°æ›¿æ¢åŸæ¥çš„è®°å½•æ•°
            grouped_df['data_days'] = grouped_df['actual_days']
            grouped_df = grouped_df.drop('actual_days', axis=1)
            
            # æ·»åŠ æ—¥æœŸæµé‡æ˜ç»†åˆ—ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®ï¼‰
            date_flow_detail = df_filtered.groupby('cgi').apply(aggregate_date_flow).reset_index()
            date_flow_detail.columns = ['cgi', 'date_flow_detail']
            
            # åˆå¹¶æ•°æ®
            grouped_df = grouped_df.merge(date_flow_detail, on='cgi', how='left')
            
            # é‡æ–°åˆ¤æ–­åˆ¶å¼ï¼ˆåŸºäº pinduan å’Œ zhishiï¼‰
            grouped_df['network_type'] = grouped_df.apply(self.determine_network_type, axis=1)
            
            # ç¬¬ä¸‰æ­¥ï¼šç­›é€‰ä½æµé‡å°åŒº
            # é€»è¾‘ï¼šæŸ¥è¯¢æ—¶é—´æ®µå†…å¹³å‡æµé‡ < é˜ˆå€¼ çš„å°åŒº
            # æ¡ä»¶ï¼šmax_flow > 0ï¼ˆæ’é™¤å…¨é›¶æµé‡ï¼‰ä¸” avg_flow < é˜ˆå€¼
            low_4g = (grouped_df['network_type'] == '4g') & \
                     (grouped_df['max_flow'] > 0) & \
                     (grouped_df['avg_flow'] < threshold_4g)
            low_5g = (grouped_df['network_type'] == '5g') & \
                     (grouped_df['max_flow'] > 0) & \
                     (grouped_df['avg_flow'] < threshold_5g)
            
            grouped_df = grouped_df[low_4g | low_5g].copy()
            
            if grouped_df.empty:
                return pd.DataFrame()
            
            # åˆ é™¤max_flowåˆ—ï¼ˆä»…ç”¨äºç­›é€‰ï¼‰
            grouped_df = grouped_df.drop(columns=['max_flow'])
            
            # æ·»åŠ é—®é¢˜ç±»å‹æ ‡ç­¾
            grouped_df['é—®é¢˜ç±»å‹'] = 'ä½æµé‡'
            
            # é‡å‘½ååˆ—åä¸ºä¸­æ–‡
            chinese_columns = {
                'cgi': 'CGI',
                'celname': 'å°åŒºåç§°',
                'grid_id': 'ç½‘æ ¼ID',
                'zhishi': 'åˆ¶å¼(åŸå§‹)',
                'pinduan': 'é¢‘æ®µ',
                'network_type': 'åˆ¶å¼',
                'grid_name': 'ç½‘æ ¼åç§°',
                'grid_pp': 'ç½‘æ ¼æ ‡ç­¾',
                'tt_mark': 'å¤‡æ³¨',
                'if_flag': 'æ˜¯å¦ç¼“å†²åŒº',
                'if_cell': 'æ˜¯å¦æ˜ å°„å°åŒº',
                'if_online': 'æ˜¯å¦åœ¨ç½‘ç®¡',
                'lon': 'ç»åº¦',
                'lat': 'çº¬åº¦',
                'phy_name': 'ç‰©ç†ç«™',
                'data_days': 'æ•°æ®å¤©æ•°',
                'avg_flow': 'æ—¥å¹³å‡æµé‡(GB)',
                'date_flow_detail': 'æ—¥æœŸæµé‡æ˜ç»†'
            }
            grouped_df = grouped_df.rename(columns=chinese_columns)
            
            # åˆ é™¤åŸå§‹åˆ¶å¼åˆ—
            if 'åˆ¶å¼(åŸå§‹)' in grouped_df.columns:
                grouped_df = grouped_df.drop(columns=['åˆ¶å¼(åŸå§‹)'])
            
            return grouped_df
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆä½æµé‡åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()

    def _generate_traffic_drop_analysis(self, before_start_date, before_end_date, 
                                        after_start_date, after_end_date, drop_threshold):
        """ç”Ÿæˆæµé‡éª¤é™åˆ†æï¼ˆæ—¶é—´æ®µå¯¹æ¯”ï¼‰"""
        try:
            before_start_str = before_start_date.strftime('%Y-%m-%d 00:00:00')
            before_end_str = before_end_date.strftime('%Y-%m-%d 23:59:59')
            after_start_str = after_start_date.strftime('%Y-%m-%d 00:00:00')
            after_end_str = after_end_date.strftime('%Y-%m-%d 23:59:59')
            
            # ä½¿ç”¨ LEFT JOIN æŸ¥è¯¢ï¼ŒåŒ…å«å¯¹æ¯”å‰æœ‰æ•°æ®ä½†å¯¹æ¯”åå¯èƒ½æ²¡æœ‰æ•°æ®çš„å°åŒº
            compare_query = '''
                SELECT
                    before_data.cgi,
                    before_data.avg_flow_before,
                    after_data.avg_flow_after,
                    CASE 
                        WHEN after_data.avg_flow_after IS NULL THEN 1 
                        ELSE 0 
                    END as is_after_no_data
                FROM (
                    SELECT
                        cgi,
                        AVG(flwor_day) as avg_flow_before
                    FROM performance_data
                    WHERE start_time BETWEEN ? AND ?
                        AND data_type = 'capacity'
                    GROUP BY cgi
                ) before_data
                LEFT JOIN (
                    SELECT
                        cgi,
                        AVG(flwor_day) as avg_flow_after
                    FROM performance_data
                    WHERE start_time BETWEEN ? AND ?
                        AND data_type = 'capacity'
                    GROUP BY cgi
                ) after_data ON before_data.cgi = after_data.cgi
            '''
            
            compare_df = pd.DataFrame(self.db_manager.execute_query(
                compare_query, [before_start_str, before_end_str, after_start_str, after_end_str]
            ))
            
            if compare_df.empty:
                st.warning("âš ï¸ å¯¹æ¯”å‰æ—¶é—´æ®µå†…æ²¡æœ‰æ•°æ®")
                return pd.DataFrame()
            
            # å¤„ç†å¯¹æ¯”åæ²¡æœ‰æ•°æ®çš„æƒ…å†µ
            compare_df['avg_flow_after'] = compare_df['avg_flow_after'].fillna(0)
            
            # è®¡ç®—æµé‡é™å¹…
            compare_df['flow_drop_ratio'] = (
                (compare_df['avg_flow_before'] - compare_df['avg_flow_after']) / 
                compare_df['avg_flow_before'] * 100
            )
            
            # è®¡ç®—æµé‡ä¸‹é™ï¼ˆGBï¼‰
            compare_df['flow_drop_gb'] = compare_df['avg_flow_before'] - compare_df['avg_flow_after']
            
            # ç­›é€‰éª¤é™å°åŒºï¼ˆé™å¹… >= é˜ˆå€¼ï¼‰
            threshold_ratio = drop_threshold
            drop_df = compare_df[compare_df['flow_drop_ratio'] >= threshold_ratio].copy()
            
            if drop_df.empty:
                st.info("âœ… æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„æµé‡éª¤é™å°åŒº")
                return pd.DataFrame()
            
            # å…³è”å°åŒºæ˜ å°„ä¿¡æ¯å’Œå·¥ç¨‹å‚æ•°ä¿¡æ¯
            mapping_query = '''
                SELECT 
                    c.cgi, c.celname, c.grid_id, c.zhishi, c.grid_name, c.grid_pp,
                    c.tt_mark, c.if_flag, c.if_cell, c.if_online, c.lon, c.lat,
                    e.phy_name, e.pinduan, e.antenna_name
                FROM cell_mapping c
                LEFT JOIN engineering_params e ON c.cgi = e.cgi
                WHERE c.cgi IN ({})
            '''.format(','.join(['?'] * len(drop_df)))
            
            mapping_df = pd.DataFrame(self.db_manager.execute_query(
                mapping_query, drop_df['cgi'].tolist()
            ))
            
            # åˆå¹¶æ˜ å°„ä¿¡æ¯
            result_df = drop_df.merge(mapping_df, on='cgi', how='left')
            
            # åº”ç”¨åˆ¶å¼åˆ¤æ–­
            result_df['network_type'] = result_df.apply(self.determine_network_type, axis=1)
            
            # å¤„ç†å¯¹æ¯”åæ²¡æœ‰æ•°æ®çš„æƒ…å†µï¼Œæ˜¾ç¤ºç‰¹æ®Šæ ‡è®°
            def format_flow_value(row):
                if row['is_after_no_data'] == 1:
                    return "0ï¼ˆæ— æ•°æ®ï¼‰"
                else:
                    return f"{row['avg_flow_after']:.2f}"
            
            result_df['å¯¹æ¯”åå¹³å‡æµé‡(GB)'] = result_df.apply(format_flow_value, axis=1)
            
            # é‡å‘½ååˆ—åä¸ºä¸­æ–‡
            chinese_columns = {
                'cgi': 'CGI',
                'celname': 'å°åŒºåç§°',
                'grid_id': 'ç½‘æ ¼ID',
                'zhishi': 'åˆ¶å¼(åŸå§‹)',
                'network_type': 'åˆ¶å¼',
                'pinduan': 'é¢‘æ®µ',
                'antenna_name': 'å¤©çº¿åå­—',
                'grid_name': 'ç½‘æ ¼åç§°',
                'grid_pp': 'ç½‘æ ¼æ ‡ç­¾',
                'tt_mark': 'å¤‡æ³¨',
                'if_flag': 'æ˜¯å¦ç¼“å†²åŒº',
                'if_cell': 'æ˜¯å¦æ˜ å°„å°åŒº',
                'if_online': 'æ˜¯å¦åœ¨ç½‘ç®¡',
                'lon': 'ç»åº¦',
                'lat': 'çº¬åº¦',
                'phy_name': 'ç‰©ç†ç«™',
                'avg_flow_before': 'å¯¹æ¯”å‰å¹³å‡æµé‡(GB)',
                'flow_drop_ratio': 'æµé‡é™å¹…(%)',
                'flow_drop_gb': 'æµé‡ä¸‹é™(GB)'
            }
            result_df = result_df.rename(columns=chinese_columns)
            
            # åˆ é™¤åŸå§‹åˆ¶å¼åˆ—
            if 'åˆ¶å¼(åŸå§‹)' in result_df.columns:
                result_df = result_df.drop(columns=['åˆ¶å¼(åŸå§‹)'])
            
            # æŒ‰é™å¹…æ’åº
            result_df = result_df.sort_values('æµé‡é™å¹…(%)', ascending=False)
            
            return result_df
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæµé‡éª¤é™åˆ†æå¤±è´¥: {e}")
            st.error(f"ç”Ÿæˆæµé‡éª¤é™åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()

    def _generate_high_load_analysis(self, start_date, end_date):
        """ç”Ÿæˆé«˜è´Ÿè·å°åŒºåˆ†æ"""
        try:
            start_str = start_date.strftime('%Y-%m-%d 00:00:00')
            end_str = end_date.strftime('%Y-%m-%d 23:59:59')
            
            # æŸ¥è¯¢é«˜è´Ÿè·å°åŒºæ±‡æ€»ï¼ˆä½¿ç”¨LEFT JOINï¼Œå³ä½¿ä¸åœ¨æ˜ å°„è¡¨ä¸­ä¹Ÿèƒ½æ˜¾ç¤ºï¼‰
            summary_query = '''
                SELECT
                    COALESCE(c.cgi, p.cgi) as cgi,
                    COALESCE(c.celname, p.celname) as celname,
                    c.grid_id,
                    COALESCE(c.zhishi, '') as zhishi,
                    COALESCE(c.pinduan, p.pinduan, '') as pinduan,
                    c.grid_name, c.grid_pp, c.tt_mark, c.if_flag, c.if_cell, c.if_online,
                    c.lon, c.lat,
                    COUNT(*) as é«˜è´Ÿè·æ¬¡æ•°,
                    GROUP_CONCAT(p.start_time) as é«˜è´Ÿè·æ—¥æœŸ
                FROM performance_data p
                LEFT JOIN cell_mapping c ON c.cgi = p.cgi
                WHERE p.if_overcel = 't'
                  AND p.start_time BETWEEN ? AND ?
                  AND p.data_type = 'capacity'
                GROUP BY COALESCE(c.cgi, p.cgi),
                         COALESCE(c.celname, p.celname),
                         c.grid_id, COALESCE(c.zhishi, ''),
                         COALESCE(c.pinduan, p.pinduan, ''),
                         c.grid_name, c.grid_pp, c.tt_mark, c.if_flag, c.if_cell, c.if_online,
                         c.lon, c.lat
                ORDER BY é«˜è´Ÿè·æ¬¡æ•° DESC
            '''
            
            summary_df = pd.DataFrame(self.db_manager.execute_query(summary_query, [start_str, end_str]))
            
            # æŸ¥è¯¢é«˜è´Ÿè·å°åŒºè¯¦ç»†æ•°æ®ï¼ˆä½¿ç”¨LEFT JOINï¼Œå³ä½¿ä¸åœ¨æ˜ å°„è¡¨ä¸­ä¹Ÿèƒ½æ˜¾ç¤ºï¼‰
            detail_query = '''
                SELECT
                    COALESCE(c.cgi, p.cgi) as cgi,
                    COALESCE(c.celname, p.celname) as celname,
                    c.grid_id,
                    COALESCE(c.zhishi, '') as zhishi,
                    COALESCE(c.pinduan, p.pinduan, '') as pinduan,
                    c.grid_name, c.grid_pp, c.tt_mark, c.if_flag, c.if_cell, c.if_online,
                    c.lon, c.lat, p.start_time, p.flwor_day, p.if_overcel,
                    p.ul_prb_mang, p.dl_prb_mang, p.pdcch_mang, p.rrc_average, p.rrc_max
                FROM performance_data p
                LEFT JOIN cell_mapping c ON c.cgi = p.cgi
                WHERE p.if_overcel = 't'
                  AND p.start_time BETWEEN ? AND ?
                  AND p.data_type = 'capacity'
                ORDER BY COALESCE(c.cgi, p.cgi), p.start_time
            '''
            
            detail_df = pd.DataFrame(self.db_manager.execute_query(detail_query, [start_str, end_str]))
            
            # é‡å‘½ååˆ—åä¸ºä¸­æ–‡
            chinese_columns = {
                'cgi': 'CGI',
                'celname': 'å°åŒºåç§°',
                'grid_id': 'ç½‘æ ¼ID',
                'zhishi': 'åˆ¶å¼',
                'pinduan': 'é¢‘æ®µ',
                'grid_name': 'ç½‘æ ¼å',
                'grid_pp': 'ç½‘æ ¼æ ‡ç­¾',
                'tt_mark': 'å¤‡æ³¨',
                'if_flag': 'æ˜¯å¦ç¼“å†²åŒº',
                'if_cell': 'æ˜¯å¦æ˜ å°„å°åŒº',
                'if_online': 'æ˜¯å¦åœ¨ç½‘ç®¡',
                'lon': 'ç»åº¦',
                'lat': 'çº¬åº¦',
                'start_time': 'æ—¥æœŸ',
                'flwor_day': 'æ—¥æµé‡',
                'if_overcel': 'æ˜¯å¦é«˜è´Ÿè·',
                'ul_prb_mang': 'ä¸Šè¡ŒPRBåˆ©ç”¨ç‡',
                'dl_prb_mang': 'ä¸‹è¡ŒPRBåˆ©ç”¨ç‡',
                'pdcch_mang': 'PDCCHåˆ©ç”¨ç‡',
                'rrc_average': 'RRCå¹³å‡è¿æ¥æ•°',
                'rrc_max': 'RRCæœ€å¤§è¿æ¥æ•°'
            }
            
            if not summary_df.empty:
                summary_df = summary_df.rename(columns=chinese_columns)
            if not detail_df.empty:
                detail_df = detail_df.rename(columns=chinese_columns)
            
            return summary_df, detail_df
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆé«˜è´Ÿè·å°åŒºåˆ†æå¤±è´¥: {e}")
            return pd.DataFrame(), pd.DataFrame()

    def _generate_cell_query(self, query_type, query_value, start_date, end_date):
        """ç”Ÿæˆå°åŒºæŸ¥è¯¢ï¼ˆä»…ä»æ€§èƒ½è¡¨æŸ¥è¯¢ï¼Œä¸å…³è”æ˜ å°„è¡¨ï¼‰"""
        try:
            start_str = start_date.strftime('%Y-%m-%d 00:00:00')
            end_str = end_date.strftime('%Y-%m-%d 23:59:59')
            
            # æ ¹æ®æŸ¥è¯¢ç±»å‹æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼ˆä»…æ”¯æŒæŒ‰CGIå’ŒæŒ‰å°åŒºåç§°ï¼‰
            if query_type == "æŒ‰å°åŒºåç§°":
                where_condition = "p.celname LIKE ?"
                query_param = f"%{query_value}%"
            elif query_type == "æŒ‰CGI":
                where_condition = "p.cgi LIKE ?"
                query_param = f"%{query_value}%"
            else:
                return pd.DataFrame()
            
            # ç›´æ¥ä»æ€§èƒ½è¡¨æŸ¥è¯¢ï¼Œä¸å…³è”æ˜ å°„è¡¨
            query = f'''
                SELECT
                    p.cgi, p.celname, p.pinduan, p.phy_name, p.cco_area_name,
                    p.start_time, p.flwor_day, p.if_overcel,
                    p.ul_prb_mang, p.dl_prb_mang, p.pdcch_mang, 
                    p.rrc_average, p.rrc_max, p.flwor_ul_mang, p.flwor_dl_mang, p.prb_max
                FROM performance_data p
                WHERE p.start_time BETWEEN ? AND ? 
                    AND p.data_type = 'capacity'
                    AND {where_condition}
                ORDER BY p.cgi, p.start_time
            '''
            
            df = pd.DataFrame(self.db_manager.execute_query(query, [start_str, end_str, query_param]))
            
            if df.empty:
                return pd.DataFrame()
            
            # é‡å‘½ååˆ—åä¸ºä¸­æ–‡
            chinese_columns = {
                'cgi': 'CGI',
                'celname': 'å°åŒºåç§°',
                'pinduan': 'é¢‘æ®µ',
                'phy_name': 'ç‰©ç†ç«™',
                'cco_area_name': 'CCOåŒºåŸŸåç§°',
                'start_time': 'æ—¥æœŸ',
                'flwor_day': 'æ—¥æµé‡(GB)',
                'if_overcel': 'æ˜¯å¦é«˜è´Ÿè·',
                'ul_prb_mang': 'ä¸Šè¡ŒPRBåˆ©ç”¨ç‡',
                'dl_prb_mang': 'ä¸‹è¡ŒPRBåˆ©ç”¨ç‡',
                'pdcch_mang': 'PDCCHåˆ©ç”¨ç‡',
                'rrc_average': 'RRCå¹³å‡è¿æ¥æ•°',
                'rrc_max': 'RRCæœ€å¤§è¿æ¥æ•°',
                'flwor_ul_mang': 'ä¸Šè¡Œæµé‡(GB)',
                'flwor_dl_mang': 'ä¸‹è¡Œæµé‡(GB)',
                'prb_max': 'PRBæœ€å¤§å€¼'
            }
            df = df.rename(columns=chinese_columns)
            
            return df
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå°åŒºæŸ¥è¯¢å¤±è´¥: {e}")
            return pd.DataFrame()

    def _export_zero_low_traffic_excel(self, zero_df, low_df, start_date, end_date, threshold_4g, threshold_5g):
        """å¯¼å‡ºé›¶ä½æµé‡åˆ†æExcelæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"é›¶ä½æµé‡åˆ†ææŠ¥å‘Š_{start_date}è‡³{end_date}_{timestamp}.xlsx"
            
            # å¤åˆ¶æ•°æ®æ¡†ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            zero_export = zero_df.copy() if not zero_df.empty else pd.DataFrame()
            low_export = low_df.copy() if not low_df.empty else pd.DataFrame()
            
            # æ›´æ–°é›¶æµé‡å°åŒºçš„é—®é¢˜ç±»å‹
            if not zero_export.empty and 'é—®é¢˜ç±»å‹' in zero_export.columns:
                zero_export['é—®é¢˜ç±»å‹'] = 'é›¶æµé‡ï¼ˆæµé‡ä¸º0ï¼‰'
            
            # æ›´æ–°ä½æµé‡å°åŒºçš„é—®é¢˜ç±»å‹ï¼ˆæ ¹æ®åˆ¶å¼æ˜¾ç¤ºä¸åŒé˜ˆå€¼ï¼‰
            if not low_export.empty and 'é—®é¢˜ç±»å‹' in low_export.columns:
                if 'åˆ¶å¼' in low_export.columns:
                    # æ ¹æ®åˆ¶å¼è®¾ç½®ä¸åŒçš„é—®é¢˜ç±»å‹æè¿°
                    low_export['é—®é¢˜ç±»å‹'] = low_export['åˆ¶å¼'].apply(
                        lambda x: f'ä½æµé‡ï¼ˆæµé‡ä½äº{threshold_5g}GBï¼‰' if x == '5g' 
                        else f'ä½æµé‡ï¼ˆæµé‡ä½äº{threshold_4g}GBï¼‰'
                    )
                else:
                    # å¦‚æœæ²¡æœ‰åˆ¶å¼åˆ—ï¼Œä½¿ç”¨4Gé˜ˆå€¼ä½œä¸ºé»˜è®¤
                    low_export['é—®é¢˜ç±»å‹'] = f'ä½æµé‡ï¼ˆæµé‡ä½äº{threshold_4g}GBï¼‰'
            
            # åˆ›å»ºExcelæ–‡ä»¶
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                if not zero_export.empty:
                    zero_export.to_excel(writer, sheet_name='é›¶æµé‡å°åŒº', index=False)
                if not low_export.empty:
                    low_export.to_excel(writer, sheet_name='ä½æµé‡å°åŒº', index=False)
            
            # æä¾›ä¸‹è½½
            st.download_button(
                label="ä¸‹è½½Excelæ–‡ä»¶",
                data=output.getvalue(),
                file_name=filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        except Exception as e:
            st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            self.logger.error(f"å¯¼å‡ºé›¶ä½æµé‡åˆ†æExcelå¤±è´¥: {e}")

    def _export_traffic_drop_excel(self, df, before_start, before_end, after_start, after_end):
        """å¯¼å‡ºæµé‡éª¤é™åˆ†æExcelæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"æµé‡éª¤é™åˆ†ææŠ¥å‘Š_å¯¹æ¯”å‰{before_start}è‡³{before_end}_å¯¹æ¯”å{after_start}è‡³{after_end}_{timestamp}.xlsx"
            
            # å®šä¹‰åˆ—é¡ºåº
            column_order = [
                'CGI', 'å°åŒºåç§°', 'åˆ¶å¼', 'é¢‘æ®µ', 'å¤©çº¿åå­—', 'ç‰©ç†ç«™', 'ç½‘æ ¼ID', 'ç½‘æ ¼åç§°', 'ç½‘æ ¼æ ‡ç­¾', 'å¤‡æ³¨',
                'æ˜¯å¦ç¼“å†²åŒº', 'æ˜¯å¦æ˜ å°„å°åŒº', 'æ˜¯å¦åœ¨ç½‘ç®¡', 'å¯¹æ¯”å‰å¹³å‡æµé‡(GB)', 'å¯¹æ¯”åå¹³å‡æµé‡(GB)',
                'avg_flow_after', 'is_after_no_data', 'æµé‡é™å¹…(%)', 'æµé‡ä¸‹é™(GB)', 'ç»åº¦', 'çº¬åº¦'
            ]
            
            # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œåªä¿ç•™å­˜åœ¨çš„åˆ—
            existing_columns = [col for col in column_order if col in df.columns]
            df_reordered = df[existing_columns]
            
            # åˆ›å»ºExcelæ–‡ä»¶
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df_reordered.to_excel(writer, sheet_name="æµé‡éª¤é™åˆ†æ", index=False)
            
            # æä¾›ä¸‹è½½
            st.download_button(
                label="ä¸‹è½½Excelæ–‡ä»¶",
                data=output.getvalue(),
                file_name=filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        except Exception as e:
            st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            self.logger.error(f"å¯¼å‡ºæµé‡éª¤é™åˆ†æExcelå¤±è´¥: {e}")

    def _export_high_load_excel(self, summary_df, detail_df, start_date, end_date):
        """å¯¼å‡ºé«˜è´Ÿè·å°åŒºåˆ†æExcelæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"é«˜è´Ÿè·å°åŒºåˆ†ææŠ¥å‘Š_{start_date}_{end_date}_{timestamp}.xlsx"
            
            # åˆ›å»ºExcelæ–‡ä»¶
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                if not summary_df.empty:
                    summary_df.to_excel(writer, sheet_name='å°åŒºæ±‡æ€»æ¸…å•', index=False)
                if not detail_df.empty:
                    detail_df.to_excel(writer, sheet_name='å°åŒºè´Ÿè·è¯¦ç»†æ¸…å•', index=False)
            
            # æä¾›ä¸‹è½½
            st.download_button(
                label="ä¸‹è½½Excelæ–‡ä»¶",
                data=output.getvalue(),
                file_name=filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        except Exception as e:
            st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            self.logger.error(f"å¯¼å‡ºé«˜è´Ÿè·å°åŒºåˆ†æExcelå¤±è´¥: {e}")

    def _export_cell_query_excel(self, df, query_type, query_value):
        """å¯¼å‡ºå°åŒºæŸ¥è¯¢Excelæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"å°åŒºæŸ¥è¯¢ç»“æœ_{query_type}_{query_value}_{timestamp}.xlsx"
            
            # åˆ›å»ºExcelæ–‡ä»¶
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name="å°åŒºæŸ¥è¯¢ç»“æœ", index=False)
            
            # æä¾›ä¸‹è½½
            st.download_button(
                label="ä¸‹è½½Excelæ–‡ä»¶",
                data=output.getvalue(),
                file_name=filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        except Exception as e:
            st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            self.logger.error(f"å¯¼å‡ºå°åŒºæŸ¥è¯¢Excelå¤±è´¥: {e}")
    
    def _get_available_dates(self):
        """è·å–æ•°æ®åº“ä¸­æœ‰æ•°æ®çš„æ—¥æœŸåˆ—è¡¨"""
        try:
            query = '''
                SELECT DISTINCT DATE(start_time) as date_str
                FROM performance_data
                WHERE data_type = 'capacity'
                ORDER BY date_str DESC
                LIMIT 90
            '''
            result = self.db_manager.execute_query(query)
            if result:
                # å°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸º date å¯¹è±¡
                dates = []
                for row in result:
                    date_str = row['date_str']
                    if date_str:
                        try:
                            # è§£ææ—¥æœŸå­—ç¬¦ä¸²
                            date_obj = datetime.strptime(date_str, '%Y-%m-%d').date()
                            dates.append(date_obj)
                        except:
                            pass
                return dates
            return []
        except Exception as e:
            self.logger.error(f"è·å–å¯ç”¨æ—¥æœŸå¤±è´¥: {e}")
            return []
    
    def _display_date_availability(self, available_dates, start_date, end_date):
        """æ˜¾ç¤ºæ—¥æœŸå¯ç”¨æ€§æç¤º"""
        try:
            if not available_dates:
                return
            
            # è·å–æœ€æ—©å’Œæœ€æ™šçš„æ•°æ®æ—¥æœŸ
            min_date = min(available_dates)
            max_date = max(available_dates)
            
            # æ£€æŸ¥é€‰æ‹©çš„æ—¥æœŸèŒƒå›´å†…æœ‰å¤šå°‘å¤©æœ‰æ•°æ®
            selected_dates = []
            current = start_date
            while current <= end_date:
                if current in available_dates:
                    selected_dates.append(current)
                current += timedelta(days=1)
            
            # æ˜¾ç¤ºæç¤ºä¿¡æ¯
            col1, col2 = st.columns([3, 1])
            
            with col1:
                if selected_dates:
                    st.success(f"âœ… é€‰æ‹©èŒƒå›´å†…æœ‰ {len(selected_dates)} å¤©æœ‰æ•°æ®")
                else:
                    st.warning(f"âš ï¸ é€‰æ‹©èŒƒå›´å†…æ²¡æœ‰æ•°æ®")
            
            with col2:
                with st.expander("ğŸ“… æŸ¥çœ‹è¯¦æƒ…"):
                    st.write(f"**æ•°æ®æ—¥æœŸèŒƒå›´ï¼š**")
                    st.write(f"æœ€æ—©ï¼š{min_date}")
                    st.write(f"æœ€æ™šï¼š{max_date}")
                    st.write(f"**å…± {len(available_dates)} å¤©**")
                    
                    if len(selected_dates) > 0:
                        st.write(f"\n**å·²é€‰èŒƒå›´å†…çš„æ•°æ®æ—¥æœŸï¼š**")
                        # æ˜¾ç¤ºæœ€å¤šå‰10ä¸ªæ—¥æœŸ
                        display_dates = sorted(selected_dates, reverse=True)[:10]
                        for d in display_dates:
                            st.write(f"ğŸŸ¢ {d}")
                        if len(selected_dates) > 10:
                            st.write(f"... è¿˜æœ‰ {len(selected_dates) - 10} å¤©")
                    
        except Exception as e:
            self.logger.error(f"æ˜¾ç¤ºæ—¥æœŸå¯ç”¨æ€§å¤±è´¥: {e}")

    def _find_traffic_drop_date(self, daily_data, drop_threshold=50, window_size=7):
        """ä½¿ç”¨æ»‘åŠ¨çª—å£ç®—æ³•æŸ¥æ‰¾æµé‡çªé™æ—¥æœŸ
        
        ç®—æ³•æ€è·¯ï¼š
        1. ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—æ¯ä¸ªçª—å£çš„å¹³å‡æµé‡
        2. çª—å£å¤§å°ï¼šå¯è‡ªå®šä¹‰ï¼ˆé»˜è®¤7å¤©ï¼‰
        3. é˜ˆå€¼ï¼šå¯è‡ªå®šä¹‰ï¼ˆé»˜è®¤50%ï¼‰
        4. æ£€æŸ¥å‰çª—å£å’Œåçª—å£çš„å¹³å‡æµé‡å˜åŒ–
        
        Args:
            daily_data: DataFrameï¼ŒåŒ…å«dateå’Œflwor_dayåˆ—
            drop_threshold: ä¸‹é™é˜€å€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œé»˜è®¤50%
            window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤7å¤©
            
        Returns:
            datetime.date: çªé™æ—¥æœŸï¼Œå¦‚æœæœªæ‰¾åˆ°çªé™åˆ™è¿”å›None
        """
        try:
            # è‡³å°‘éœ€è¦3ä¸ªçª—å£çš„æ•°æ®ï¼ˆå‰çª—å£ + çªé™æ—¥ + åç»­çª—å£ï¼‰
            min_days = window_size * 2 + 1
            if len(daily_data) < min_days:
                return None
            
            # è®¡ç®—é˜ˆå€¼æ¯”ä¾‹
            threshold_ratio = drop_threshold / 100.0
            
            # éå†æ•°æ®ï¼ŒæŸ¥æ‰¾çªé™ç‚¹
            for i in range(window_size, len(daily_data)):
                current_date = daily_data.iloc[i]['date']
                current_flow = daily_data.iloc[i]['flwor_day']
                
                # å‰çª—å£ï¼ˆçªé™å‰çš„Nå¤©ï¼Œä¸åŒ…æ‹¬å½“å‰æ—¥ï¼‰
                before_window = daily_data.iloc[i-window_size:i]
                if len(before_window) < window_size:
                    continue
                
                avg_before = before_window['flwor_day'].mean()
                
                # åçª—å£ï¼ˆçªé™åçš„Nå¤©ï¼ŒåŒ…æ‹¬å½“å‰æ—¥ï¼‰
                after_window = daily_data.iloc[i:i+window_size]
                if len(after_window) < window_size:
                    continue
                
                avg_after = after_window['flwor_day'].mean()
                
                # åŠ¨æ€é˜ˆå€¼åˆ¤æ–­ï¼š
                # 1. å‰çª—å£å¹³å‡æµé‡ > 1GB
                # 2. å½“å‰æ—¥æµé‡ < å‰çª—å£å¹³å‡ * (1 - threshold_ratio)ï¼ˆä¸‹é™è¶…è¿‡é˜ˆå€¼ï¼‰
                # 3. åçª—å£å¹³å‡æµé‡ < å‰çª—å£å¹³å‡ * (1 - threshold_ratio * 1.5)ï¼ˆç¡®è®¤æŒç»­ä½æµé‡ï¼‰
                drop_ratio = 1 - threshold_ratio
                confirm_ratio = 1 - threshold_ratio * 1.5
                
                if (avg_before > 1.0 and 
                    current_flow < avg_before * drop_ratio and 
                    avg_after < avg_before * confirm_ratio):
                    
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç¬¦åˆæ¡ä»¶çš„æ—¥æœŸä½œä¸ºçªé™æ—¥æœŸ
                    return current_date
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ˜ç¡®çš„çªé™ç‚¹ï¼Œå°è¯•æ›´å®½æ¾çš„æ¡ä»¶
            # ä»…æ£€æŸ¥å½“å‰æ—¥æµé‡å¯¹æ¯”å‰çª—å£å¹³å‡æ˜¯å¦ä¸‹é™è¶…è¿‡é˜ˆå€¼
            for i in range(window_size, len(daily_data)):
                current_date = daily_data.iloc[i]['date']
                current_flow = daily_data.iloc[i]['flwor_day']
                
                before_window = daily_data.iloc[i-window_size:i]
                if len(before_window) < window_size:
                    continue
                
                avg_before = before_window['flwor_day'].mean()
                drop_ratio = 1 - threshold_ratio
                
                # æ›´å®½æ¾çš„æ¡ä»¶ï¼šå‰çª—å£å¹³å‡ > 1GB ä¸”å½“å‰æ—¥æµé‡ < å‰çª—å£å¹³å‡ * drop_ratio
                if avg_before > 1.0 and current_flow < avg_before * drop_ratio:
                    return current_date
            
            return None
            
        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾æµé‡çªé™æ—¥æœŸå¤±è´¥: {e}")
            return None

    def _render_traffic_spike_analysis(self):
        """æ¸²æŸ“æµé‡çªé™åˆ†æé¡µé¢"""
        st.subheader("ğŸ¯ æµé‡çªé™åˆ†æ")
        st.caption("åˆ†ææŒ‡å®šCGIåœ¨æ—¶é—´æ®µå†…çš„æµé‡çªé™æƒ…å†µ")
        
        # è¾“å…¥é…ç½®
        st.markdown("#### ğŸ“‹ è¾“å…¥é…ç½®")
        
        # åˆ†æç±»å‹é€‰æ‹©
        analysis_type = st.radio(
            "é€‰æ‹©åˆ†æç±»å‹",
            ["æŒ‡å®šCGIåˆ†æ", "å…¨ç½‘å°åŒºåˆ†æ", "æ‰‡åŒºçº§åˆ†æ"],
            horizontal=True,
            key="traffic_spike_analysis_type"
        )
        
        cgi_input = ""
        if analysis_type == "æŒ‡å®šCGIåˆ†æ":
            # CGIè¾“å…¥
            st.markdown("**CGIåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰**")
            cgi_input = st.text_area(
                "è¾“å…¥CGIï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
                height=150,
                help="å¯è¾“å…¥ä¸€ä¸ªæˆ–å¤šä¸ªCGIï¼Œæ¯è¡Œä¸€ä¸ª",
                placeholder="460-00-12635644-1\n460-00-12635523-16\n460-00-12635495-3"
            )
        elif analysis_type == "å…¨ç½‘å°åŒºåˆ†æ":
            st.info("ğŸŒ å…¨ç½‘å°åŒºåˆ†æï¼šå°†åˆ†ææ‰€æœ‰å·¥å‚å°åŒºï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        else:
            st.info("ğŸ¢ æ‰‡åŒºçº§åˆ†æï¼šåˆ†ææ‰‡åŒºæ•´ä½“ä¸‹é™vså•ä¸ªå°åŒºä¸‹é™ï¼Œåˆ¤æ–­æ˜¯å¦å…±å¤©çº¿")
        
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input(
                "å¼€å§‹æ—¥æœŸ",
                value=date.today() - timedelta(days=30),
                key="spike_start_date"
            )
        with col2:
            end_date = st.date_input(
                "ç»“æŸæ—¥æœŸ",
                value=date.today(),
                key="spike_end_date"
            )
        
        st.markdown("#### âš™ï¸ åˆ†æå‚æ•°é…ç½®")
        col3, col4 = st.columns(2)
        with col3:
            drop_threshold = st.slider(
                "ä¸‹é™é˜€å€¼ (%)",
                min_value=10,
                max_value=90,
                value=50,
                step=5,
                help="æµé‡ç›¸æ¯”å‰ä¸€ä¸ªçª—å£ä¸‹é™çš„æ¯”ä¾‹ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºæ˜¯çªé™"
            )
        with col4:
            window_size = st.slider(
                "æ»‘åŠ¨çª—å£ (å¤©)",
                min_value=3,
                max_value=14,
                value=7,
                step=1,
                help="æ»‘åŠ¨çª—å£å¤§å°ï¼Œç”¨äºè®¡ç®—å¹³å‡æµé‡å’Œåˆ¤æ–­çªé™"
            )
        
        st.info(f"ğŸ“Š å½“å‰é…ç½®ï¼šä¸‹é™é˜€å€¼ {drop_threshold}%ï¼Œæ»‘åŠ¨çª—å£ {window_size} å¤©")
        
        # åˆ†ææŒ‰é’®
        if st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
            if analysis_type == "æŒ‡å®šCGIåˆ†æ":
                if not cgi_input.strip():
                    st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªCGI")
                    return
                
                # è§£æCGIåˆ—è¡¨
                cgi_list = [cgi.strip() for cgi in cgi_input.strip().split('\n') if cgi.strip()]
                
                if not cgi_list:
                    st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„CGI")
                    return
                
                st.info(f"ğŸ“Š åˆ†æ {len(cgi_list)} ä¸ªCGIçš„æµé‡çªé™æƒ…å†µ...")
                result_df = self._generate_traffic_spike_analysis(cgi_list, start_date, end_date, drop_threshold, window_size)
                
            elif analysis_type == "å…¨ç½‘å°åŒºåˆ†æ":
                # å…¨ç½‘å°åŒºåˆ†æï¼šè·å–æ‰€æœ‰æ˜ å°„å°åŒº
                cgi_list = None
                st.info(f"ğŸŒ å…¨ç½‘å°åŒºåˆ†æä¸­...")
                result_df = self._generate_traffic_spike_analysis_with_progress(cgi_list, start_date, end_date, drop_threshold, window_size)
                
            else:
                # æ‰‡åŒºçº§åˆ†æ
                st.info(f"ğŸ¢ æ‰‡åŒºçº§åˆ†æä¸­...")
                result_df = self._generate_sector_level_analysis(start_date, end_date, drop_threshold, window_size)
            
            st.info(f"âš™ï¸ åˆ†æå‚æ•°ï¼šä¸‹é™é˜€å€¼ {drop_threshold}%ï¼Œæ»‘åŠ¨çª—å£ {window_size} å¤©")
            
            if not result_df.empty:
                if analysis_type == "æ‰‡åŒºçº§åˆ†æ":
                    st.success(f"âœ… æ‰‡åŒºçº§åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(result_df)} ä¸ªæ‰‡åŒº")
                    
                    # æ˜¾ç¤ºæ‰‡åŒºçº§åˆ†æç»“æœ
                    st.markdown("#### ğŸ“Š æ‰‡åŒºçº§åˆ†æç»“æœ")
                    st.dataframe(result_df, use_container_width=True, hide_index=True)
                    
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    st.markdown("#### ğŸ“ˆ åˆ†æç»Ÿè®¡")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        sector_total = len(result_df)
                        st.metric("åˆ†ææ‰‡åŒºæ•°", sector_total)
                    with col2:
                        sector_drop = len(result_df[result_df['ä¸‹é™ç±»å‹'] == 'æ‰‡åŒºæ•´ä½“ä¸‹é™'])
                        st.metric("æ‰‡åŒºæ•´ä½“ä¸‹é™", sector_drop)
                    with col3:
                        cell_drop = len(result_df[result_df['ä¸‹é™ç±»å‹'] == 'å•ä¸ª/éƒ¨åˆ†å°åŒºä¸‹é™'])
                        st.metric("å•ä¸ªå°åŒºä¸‹é™", cell_drop)
                    
                    # å¯¼å‡ºåŠŸèƒ½
                    st.markdown("#### ğŸ“¥ å¯¼å‡ºåŠŸèƒ½")
                    self._export_sector_analysis_excel(result_df, start_date, end_date)
                else:
                    st.success(f"âœ… åˆ†æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(result_df)} ä¸ªæµé‡çªé™å°åŒº")
                    
                    # æ˜¾ç¤ºç»“æœ
                    st.markdown("#### ğŸ“Š åˆ†æç»“æœ")
                    st.dataframe(result_df, use_container_width=True, hide_index=True)
                    
                    # å¯¼å‡ºExcel
                    self._export_traffic_spike_excel(result_df, start_date, end_date)
            else:
                st.info("â„¹ï¸ æœªå‘ç°æµé‡çªé™æƒ…å†µ")
    
    def _generate_traffic_spike_analysis_with_progress(self, cgi_list, start_date, end_date, drop_threshold=50, window_size=7):
        """ç”Ÿæˆæµé‡çªé™åˆ†æï¼ˆå¸¦è¿›åº¦æ¡ï¼‰"""
        try:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # æ ‡è®°æ˜¯å¦ä¸ºå…¨ç½‘åˆ†æ
            is_network_wide = (cgi_list is None)
            
            # å¦‚æœcgi_listä¸ºNoneï¼Œè·å–æ‰€æœ‰æœ‰æ•°æ®å°åŒºçš„æ•°é‡ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
            if is_network_wide:
                status_text.text("ğŸ” æ­¥éª¤ 1/5: ç»Ÿè®¡å…¨ç½‘æœ‰æ€§èƒ½æ•°æ®çš„å°åŒº...")
                progress_bar.progress(5)
                
                start_str = start_date.strftime('%Y-%m-%d 00:00:00')
                end_str = end_date.strftime('%Y-%m-%d 23:59:59')
                
                # ç»Ÿè®¡æœ‰æ•°æ®çš„å°åŒºæ•°é‡
                count_query = "SELECT COUNT(DISTINCT cgi) as count FROM performance_data WHERE start_time BETWEEN ? AND ? AND data_type = 'capacity'"
                count_result = self.db_manager.execute_query(count_query, [start_str, end_str])
                total_cgis = count_result[0]['count'] if count_result else 0
                
                if total_cgis == 0:
                    progress_bar.empty()
                    status_text.empty()
                    return pd.DataFrame()
            else:
                total_cgis = len(cgi_list)
            
            status_text.text(f"ğŸ“Š æ­¥éª¤ 2/5: æŸ¥è¯¢ {total_cgis} ä¸ªCGIçš„æµé‡æ•°æ®...")
            progress_bar.progress(20)
            
            # æŸ¥è¯¢æµé‡æ•°æ®
            start_str = start_date.strftime('%Y-%m-%d 00:00:00')
            end_str = end_date.strftime('%Y-%m-%d 23:59:59')
            
            if is_network_wide:
                # å…¨ç½‘åˆ†æï¼šç›´æ¥æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰æ•°æ®
                query = '''
                    SELECT
                        cgi,
                        start_time,
                        flwor_day
                    FROM performance_data
                    WHERE start_time BETWEEN ? AND ?
                        AND data_type = 'capacity'
                    ORDER BY cgi, start_time
                '''
                params = [start_str, end_str]
            else:
                # æŒ‡å®šCGIåˆ†æï¼šä½¿ç”¨INæŸ¥è¯¢
                query = '''
                    SELECT
                        cgi,
                        start_time,
                        flwor_day
                    FROM performance_data
                    WHERE cgi IN ({})
                        AND start_time BETWEEN ? AND ?
                        AND data_type = 'capacity'
                    ORDER BY cgi, start_time
                '''.format(','.join(['?'] * len(cgi_list)))
                params = cgi_list + [start_str, end_str]
            
            data = self.db_manager.execute_query(query, params)
            
            if not data:
                progress_bar.empty()
                status_text.empty()
                return pd.DataFrame()
            
            status_text.text(f"ğŸ” æ­¥éª¤ 3/5: åˆ†æ {total_cgis} ä¸ªCGIçš„æµé‡çªé™æƒ…å†µ...")
            progress_bar.progress(40)
            
            df = pd.DataFrame(data)
            df['start_time'] = pd.to_datetime(df['start_time'])
            df['date'] = df['start_time'].dt.date
            
            # æŒ‰CGIåˆ†ç»„åˆ†æï¼ˆä¼˜åŒ–ï¼šä¸€æ¬¡æ€§æŒ‰CGIåˆ†ç»„ï¼Œå‡å°‘é‡å¤ç­›é€‰ï¼‰
            status_text.text(f"ğŸ” æ­¥éª¤ 3/5: æŒ‰CGIåˆ†ç»„æ•°æ®...")
            progress_bar.progress(45)
            
            # ä¸€æ¬¡æ€§æŒ‰CGIåˆ†ç»„
            grouped = df.groupby('cgi')
            
            # æŒ‰CGIåˆ†ç»„åˆ†æ
            results = []
            total_cgis_analyzed = len(grouped)
            
            min_required_days = window_size * 2 + 1
            
            # ä¸ºæ¯ä¸ªCGIæ·»åŠ å¢é‡è¿›åº¦æ˜¾ç¤º
            for idx, (cgi, cgi_data) in enumerate(grouped):
                if (idx + 1) % 100 == 0:
                    progress = 50 + int((idx + 1) / total_cgis_analyzed * 30)
                    status_text.text(f"ğŸ” æ­¥éª¤ 3/5: å·²åˆ†æ {idx + 1}/{total_cgis_analyzed} ä¸ªCGI...")
                    progress_bar.progress(progress)
                
                # è‡³å°‘éœ€è¦è¶³å¤Ÿçš„window_sizeæ•°æ®æ‰èƒ½åˆ†æ
                if len(cgi_data) < min_required_days:
                    continue
                
                # æŒ‰æ—¥æœŸèšåˆ
                daily_data = cgi_data.groupby('date')['flwor_day'].mean().reset_index()
                daily_data = daily_data.sort_values('date')
                
                # ä½¿ç”¨æ»‘åŠ¨çª—å£ç®—æ³•æŸ¥æ‰¾æµé‡çªé™
                drop_date = self._find_traffic_drop_date(daily_data, drop_threshold, window_size)
                
                if drop_date is None:
                    continue
                
                # è®¡ç®—çªé™å‰7æ—¥æµé‡ï¼ˆä¸åŒ…æ‹¬çªé™æ—¥ï¼‰
                before_7days = daily_data[daily_data['date'] < drop_date].tail(7)
                avg_before = before_7days['flwor_day'].mean() if len(before_7days) >= 7 else 0
                
                # è®¡ç®—çªé™å7æ—¥æµé‡ï¼ˆåŒ…æ‹¬çªé™æ—¥ï¼‰
                after_7days = daily_data[daily_data['date'] >= drop_date].head(7)
                avg_after = after_7days['flwor_day'].mean() if len(after_7days) >= 7 else 0
                
                # ä¸‹é™æ¯”ä¾‹
                drop_ratio = ((avg_before - avg_after) / avg_before * 100) if avg_before > 0 else 0
                
                # ç›®å‰æœ€æ–°7æ—¥æµé‡ï¼ˆæœ€è¿‘7å¤©ï¼‰
                latest_7days = daily_data.tail(7)
                latest_7day = latest_7days['flwor_day'].mean() if len(latest_7days) >= 7 else 0
                
                # åˆ¤æ–­æ˜¯å¦æ¢å¤
                recovery_threshold = avg_before * 0.5
                if latest_7day >= recovery_threshold:
                    conclusion = "å·²æ¢å¤"
                else:
                    conclusion = "æœªæ¢å¤"
                
                results.append({
                    'cgi': cgi,
                    'drop_date': drop_date,
                    'avg_before': avg_before,
                    'avg_after': avg_after,
                    'drop_ratio': drop_ratio,
                    'latest_7day': latest_7day,
                    'conclusion': conclusion
                })
            
            status_text.text("âœ… æ­¥éª¤ 4/5: å…³è”å°åŒºä¿¡æ¯...")
            progress_bar.progress(85)
            
            if not results:
                progress_bar.empty()
                status_text.empty()
                return pd.DataFrame()
            
            result_df = pd.DataFrame(results)
            result_df['drop_date_str'] = result_df['drop_date'].apply(lambda x: x.strftime('%mæœˆ%dæ—¥'))
            
            # å…³è”å°åŒºä¿¡æ¯ï¼ˆä»å·¥å‚è¡¨è·å–ï¼‰
            mapping_query = '''
                SELECT 
                    cgi, celname, zhishi, phy_name, antenna_name,
                    grid_id_no_buffer, grid_name_no_buffer, grid_label_no_buffer,
                    grid_id_buffer_500m, grid_name_buffer_500m, grid_label_buffer_500m
                FROM engineering_params
                WHERE cgi IN ({})
            '''.format(','.join(['?'] * len(result_df)))
            
            mapping_df = pd.DataFrame(self.db_manager.execute_query(
                mapping_query, result_df['cgi'].tolist()
            ))
            
            # åˆå¹¶ä¿¡æ¯
            result_df = result_df.merge(mapping_df, on='cgi', how='left')
            
            # æ„å»ºæœ€ç»ˆç»“æœï¼ˆåªä¿ç•™æŒ‡å®šåˆ—ï¼Œåˆå¹¶ç½‘æ ¼å­—æ®µï¼‰
            final_results = []
            for _, row in result_df.iterrows():
                # åˆå¹¶ç½‘æ ¼IDï¼šå°†ä¸ç¼“å†²å’Œç¼“å†²500ç±³çš„ç½‘æ ¼IDåˆå¹¶
                grid_id_no_buffer = str(row.get('grid_id_no_buffer', '')).strip() if row.get('grid_id_no_buffer') else ''
                grid_id_buffer_500m = str(row.get('grid_id_buffer_500m', '')).strip() if row.get('grid_id_buffer_500m') else ''
                grid_id_list = []
                if grid_id_no_buffer:
                    grid_id_list.append(grid_id_no_buffer)
                if grid_id_buffer_500m:
                    grid_id_list.append(grid_id_buffer_500m)
                grid_id = ','.join(grid_id_list) if grid_id_list else ''
                
                # åˆå¹¶ç½‘æ ¼åï¼šå°†ä¸ç¼“å†²å’Œç¼“å†²500ç±³çš„ç½‘æ ¼ååˆå¹¶
                grid_name_no_buffer = str(row.get('grid_name_no_buffer', '')).strip() if row.get('grid_name_no_buffer') else ''
                grid_name_buffer_500m = str(row.get('grid_name_buffer_500m', '')).strip() if row.get('grid_name_buffer_500m') else ''
                grid_name_list = []
                if grid_name_no_buffer:
                    grid_name_list.append(grid_name_no_buffer)
                if grid_name_buffer_500m:
                    grid_name_list.append(grid_name_buffer_500m)
                grid_name = ','.join(grid_name_list) if grid_name_list else ''
                
                # åˆå¹¶ç½‘æ ¼æ ‡ç­¾ï¼šå°†ä¸ç¼“å†²å’Œç¼“å†²500ç±³çš„ç½‘æ ¼æ ‡ç­¾åˆå¹¶
                grid_label_no_buffer = str(row.get('grid_label_no_buffer', '')).strip() if row.get('grid_label_no_buffer') else ''
                grid_label_buffer_500m = str(row.get('grid_label_buffer_500m', '')).strip() if row.get('grid_label_buffer_500m') else ''
                grid_label_list = []
                if grid_label_no_buffer:
                    grid_label_list.append(grid_label_no_buffer)
                if grid_label_buffer_500m:
                    grid_label_list.append(grid_label_buffer_500m)
                grid_label = ','.join(grid_label_list) if grid_label_list else ''
                
                final_results.append({
                    'CGI': row['cgi'],
                    'å°åŒºåç§°': row.get('celname', ''),
                    'åˆ¶å¼': row.get('zhishi', ''),
                    'ç‰©ç†ç«™': row.get('phy_name', ''),
                    'å¤©çº¿': row.get('antenna_name', ''),
                    'ç½‘æ ¼ID': grid_id,
                    'ç½‘æ ¼å': grid_name,
                    'ç½‘æ ¼æ ‡ç­¾': grid_label,
                    'çªé™å‰æµé‡æ—¥å¹³å‡(GB)': round(row['avg_before'], 2),
                    'çªé™åæµé‡æ—¥å¹³å‡(GB)': round(row['avg_after'], 2),
                    'æµé‡ä¸‹é™æ—¥æœŸ': row['drop_date_str'],
                    'ä¸‹é™æ¯”ä¾‹': round(row['drop_ratio'], 2),
                    'ç›®å‰æœ€æ–°7æ—¥æ—¥æµé‡(GB)': round(row['latest_7day'], 2),
                    'ç»“è®º': row.get('conclusion', '')
                })
            
            status_text.text(f"âœ… æ­¥éª¤ 5/5: åˆ†æå®Œæˆï¼Œå…±å‘ç° {len(final_results)} ä¸ªçªé™å°åŒº")
            progress_bar.progress(100)
            
            # æ¸…ç©ºè¿›åº¦æ¡
            progress_bar.empty()
            status_text.empty()
            
            return pd.DataFrame(final_results)
            
        except Exception as e:
            if 'progress_bar' in locals():
                progress_bar.empty()
            if 'status_text' in locals():
                status_text.empty()
            st.error(f"åˆ†æå¤±è´¥: {e}")
            self.logger.error(f"ç”Ÿæˆæµé‡çªé™åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _generate_traffic_spike_analysis(self, cgi_list, start_date, end_date, drop_threshold=50, window_size=7):
        """ç”Ÿæˆæµé‡çªé™åˆ†æ"""
        try:
            start_str = start_date.strftime('%Y-%m-%d 00:00:00')
            end_str = end_date.strftime('%Y-%m-%d 23:59:59')
            
            # æŸ¥è¯¢æµé‡æ•°æ®
            if cgi_list is None:
                # å…¨ç½‘å°åŒºåˆ†æï¼šç›´æ¥æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰æ•°æ®
                query = '''
                    SELECT
                        cgi,
                        start_time,
                        flwor_day
                    FROM performance_data
                    WHERE start_time BETWEEN ? AND ?
                        AND data_type = 'capacity'
                    ORDER BY cgi, start_time
                '''
                params = [start_str, end_str]
                
                # å¦‚æœéœ€è¦è·å–cgi_listä¾›åç»­ä½¿ç”¨ï¼ˆè™½ç„¶ä¸‹é¢é€»è¾‘æ”¹äº†ï¼Œä½†ä¸ºäº†å…¼å®¹ï¼‰
                # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ˜¾å¼è·å–æ‰€æœ‰CGIåˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥éå†æŸ¥è¯¢ç»“æœ
            else:
                query = '''
                    SELECT
                        cgi,
                        start_time,
                        flwor_day
                    FROM performance_data
                    WHERE cgi IN ({})
                        AND start_time BETWEEN ? AND ?
                        AND data_type = 'capacity'
                    ORDER BY cgi, start_time
                '''.format(','.join(['?'] * len(cgi_list)))
                params = cgi_list + [start_str, end_str]
            
            data = self.db_manager.execute_query(query, params)
            
            if not data:
                return pd.DataFrame()
            
            df = pd.DataFrame(data)
            df['start_time'] = pd.to_datetime(df['start_time'])
            df['date'] = df['start_time'].dt.date
            
            # æŒ‰CGIåˆ†ç»„åˆ†æ
            results = []
            
            # ä½¿ç”¨groupbyä¸€æ¬¡æ€§åˆ†ç»„ï¼Œæ¯”éå†cgi_listæ›´é«˜æ•ˆä¸”æ”¯æŒå…¨ç½‘åˆ†æ
            grouped = df.groupby('cgi')
            
            for cgi, cgi_data in grouped:
                # è‡³å°‘éœ€è¦è¶³å¤Ÿçš„window_sizeæ•°æ®æ‰èƒ½åˆ†æ
                min_required_days = window_size * 2 + 1
                if len(cgi_data) < min_required_days:
                    continue
                
                # æŒ‰æ—¥æœŸèšåˆ
                daily_data = cgi_data.groupby('date')['flwor_day'].mean().reset_index()
                daily_data = daily_data.sort_values('date')
                
                # ä½¿ç”¨æ»‘åŠ¨çª—å£ç®—æ³•æŸ¥æ‰¾æµé‡çªé™
                drop_date = self._find_traffic_drop_date(daily_data, drop_threshold, window_size)
                
                if drop_date is None:
                    continue
                
                # è®¡ç®—çªé™å‰7æ—¥æµé‡ï¼ˆä¸åŒ…æ‹¬çªé™æ—¥ï¼‰
                before_7days = daily_data[daily_data['date'] < drop_date].tail(7)
                avg_before = before_7days['flwor_day'].mean() if len(before_7days) >= 7 else 0
                
                # è®¡ç®—çªé™å7æ—¥æµé‡ï¼ˆåŒ…æ‹¬çªé™æ—¥ï¼‰
                after_7days = daily_data[daily_data['date'] >= drop_date].head(7)
                avg_after = after_7days['flwor_day'].mean() if len(after_7days) >= 7 else 0
                
                # ä¸‹é™æ¯”ä¾‹
                drop_ratio = ((avg_before - avg_after) / avg_before * 100) if avg_before > 0 else 0
                
                # ç›®å‰æœ€æ–°7æ—¥æµé‡ï¼ˆæœ€è¿‘7å¤©ï¼‰
                latest_7days = daily_data.tail(7)
                latest_7day = latest_7days['flwor_day'].mean() if len(latest_7days) >= 7 else 0
                
                # åˆ¤æ–­æ˜¯å¦æ¢å¤
                # å¦‚æœæœ€æ–°7æ—¥æµé‡ >= çªé™å‰æµé‡ * 0.5ï¼Œè®¤ä¸ºå·²æ¢å¤
                recovery_threshold = avg_before * 0.5
                if latest_7day >= recovery_threshold:
                    conclusion = "å·²æ¢å¤"
                else:
                    conclusion = "æœªæ¢å¤"
                
                results.append({
                    'cgi': cgi,
                    'drop_date': drop_date,
                    'avg_before': avg_before,
                    'avg_after': avg_after,
                    'drop_ratio': drop_ratio,
                    'latest_7day': latest_7day,
                    'conclusion': conclusion
                })
            
            if not results:
                return pd.DataFrame()
            
            result_df = pd.DataFrame(results)
            result_df['drop_date_str'] = result_df['drop_date'].apply(lambda x: x.strftime('%mæœˆ%dæ—¥'))
            
            # å…³è”å°åŒºä¿¡æ¯ï¼ˆä»å·¥å‚è¡¨è·å–ï¼‰
            mapping_query = '''
                SELECT 
                    cgi, celname, zhishi, phy_name, antenna_name,
                    grid_id_no_buffer, grid_name_no_buffer, grid_label_no_buffer,
                    grid_id_buffer_500m, grid_name_buffer_500m, grid_label_buffer_500m
                FROM engineering_params
                WHERE cgi IN ({})
            '''.format(','.join(['?'] * len(result_df)))
            
            mapping_df = pd.DataFrame(self.db_manager.execute_query(
                mapping_query, result_df['cgi'].tolist()
            ))
            
            # åˆå¹¶ä¿¡æ¯
            result_df = result_df.merge(mapping_df, on='cgi', how='left')
            
            # æ„å»ºæœ€ç»ˆç»“æœï¼ˆåªä¿ç•™æŒ‡å®šåˆ—ï¼Œåˆå¹¶ç½‘æ ¼å­—æ®µï¼‰
            final_results = []
            for _, row in result_df.iterrows():
                # åˆå¹¶ç½‘æ ¼IDï¼šå°†ä¸ç¼“å†²å’Œç¼“å†²500ç±³çš„ç½‘æ ¼IDåˆå¹¶
                grid_id_no_buffer = str(row.get('grid_id_no_buffer', '')).strip() if row.get('grid_id_no_buffer') else ''
                grid_id_buffer_500m = str(row.get('grid_id_buffer_500m', '')).strip() if row.get('grid_id_buffer_500m') else ''
                grid_id_list = []
                if grid_id_no_buffer:
                    grid_id_list.append(grid_id_no_buffer)
                if grid_id_buffer_500m:
                    grid_id_list.append(grid_id_buffer_500m)
                grid_id = ','.join(grid_id_list) if grid_id_list else ''
                
                # åˆå¹¶ç½‘æ ¼åï¼šå°†ä¸ç¼“å†²å’Œç¼“å†²500ç±³çš„ç½‘æ ¼ååˆå¹¶
                grid_name_no_buffer = str(row.get('grid_name_no_buffer', '')).strip() if row.get('grid_name_no_buffer') else ''
                grid_name_buffer_500m = str(row.get('grid_name_buffer_500m', '')).strip() if row.get('grid_name_buffer_500m') else ''
                grid_name_list = []
                if grid_name_no_buffer:
                    grid_name_list.append(grid_name_no_buffer)
                if grid_name_buffer_500m:
                    grid_name_list.append(grid_name_buffer_500m)
                grid_name = ','.join(grid_name_list) if grid_name_list else ''
                
                # åˆå¹¶ç½‘æ ¼æ ‡ç­¾ï¼šå°†ä¸ç¼“å†²å’Œç¼“å†²500ç±³çš„ç½‘æ ¼æ ‡ç­¾åˆå¹¶
                grid_label_no_buffer = str(row.get('grid_label_no_buffer', '')).strip() if row.get('grid_label_no_buffer') else ''
                grid_label_buffer_500m = str(row.get('grid_label_buffer_500m', '')).strip() if row.get('grid_label_buffer_500m') else ''
                grid_label_list = []
                if grid_label_no_buffer:
                    grid_label_list.append(grid_label_no_buffer)
                if grid_label_buffer_500m:
                    grid_label_list.append(grid_label_buffer_500m)
                grid_label = ','.join(grid_label_list) if grid_label_list else ''
                
                final_results.append({
                    'CGI': row['cgi'],
                    'å°åŒºåç§°': row.get('celname', ''),
                    'åˆ¶å¼': row.get('zhishi', ''),
                    'ç‰©ç†ç«™': row.get('phy_name', ''),
                    'å¤©çº¿': row.get('antenna_name', ''),
                    'ç½‘æ ¼ID': grid_id,
                    'ç½‘æ ¼å': grid_name,
                    'ç½‘æ ¼æ ‡ç­¾': grid_label,
                    'çªé™å‰æµé‡æ—¥å¹³å‡(GB)': round(row['avg_before'], 2),
                    'çªé™åæµé‡æ—¥å¹³å‡(GB)': round(row['avg_after'], 2),
                    'æµé‡ä¸‹é™æ—¥æœŸ': row['drop_date_str'],
                    'ä¸‹é™æ¯”ä¾‹': round(row['drop_ratio'], 2),
                    'ç›®å‰æœ€æ–°7æ—¥æ—¥æµé‡(GB)': round(row['latest_7day'], 2),
                    'ç»“è®º': row.get('conclusion', '')
                })
            
            return pd.DataFrame(final_results)
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæµé‡çªé™åˆ†æå¤±è´¥: {e}")
            st.error(f"ç”Ÿæˆæµé‡çªé™åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _export_traffic_spike_excel(self, df, start_date, end_date):
        """å¯¼å‡ºæµé‡çªé™åˆ†æExcelæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"æµé‡çªé™åˆ†ææŠ¥å‘Š_{start_date}è‡³{end_date}_{timestamp}.xlsx"
            
            # åˆ›å»ºExcelæ–‡ä»¶
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name="æµé‡çªé™åˆ†æ", index=False)
            
            # æä¾›ä¸‹è½½
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½Excelæ–‡ä»¶",
                data=output.getvalue(),
                file_name=filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        except Exception as e:
            st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            self.logger.error(f"å¯¼å‡ºæµé‡çªé™åˆ†æExcelå¤±è´¥: {e}")
    
    def _export_sector_analysis_excel(self, df, start_date, end_date):
        """å¯¼å‡ºæ‰‡åŒºçº§åˆ†æExcelæ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"æ‰‡åŒºçº§æµé‡çªé™åˆ†æ_{start_date}è‡³{end_date}_{timestamp}.xlsx"
            
            # åˆ›å»ºExcelæ–‡ä»¶
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name="æ‰‡åŒºçº§åˆ†æ", index=False)
            
            # æä¾›ä¸‹è½½
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½æ‰‡åŒºçº§åˆ†æExcelæ–‡ä»¶",
                data=output.getvalue(),
                file_name=filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        except Exception as e:
            st.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            self.logger.error(f"å¯¼å‡ºæ‰‡åŒºçº§åˆ†æExcelå¤±è´¥: {e}")
    
    def _generate_sector_level_analysis(self, start_date, end_date, drop_threshold=50, window_size=7):
        """ç”Ÿæˆæ‰‡åŒºçº§æµé‡çªé™åˆ†æ"""
        try:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # æ­¥éª¤1ï¼šè·å–æ‰€æœ‰å·¥å‚å°åŒº
            status_text.text("ğŸ” æ­¥éª¤ 1/6: è·å–æ‰€æœ‰å·¥å‚å°åŒº...")
            progress_bar.progress(10)
            
            mapping_query = "SELECT DISTINCT cgi FROM engineering_params WHERE cgi IS NOT NULL"
            mapping_result = self.db_manager.execute_query(mapping_query)
            all_cgis = [row['cgi'] for row in mapping_result]
            
            if not all_cgis:
                progress_bar.empty()
                status_text.empty()
                return pd.DataFrame()
            
            # æ­¥éª¤2ï¼šæŸ¥è¯¢æµé‡æ•°æ®
            status_text.text(f"ğŸ“Š æ­¥éª¤ 2/6: æŸ¥è¯¢ {len(all_cgis)} ä¸ªCGIçš„æµé‡æ•°æ®...")
            progress_bar.progress(20)
            
            start_str = start_date.strftime('%Y-%m-%d 00:00:00')
            end_str = end_date.strftime('%Y-%m-%d 23:59:59')
            
            query = '''
                SELECT
                    cgi,
                    start_time,
                    flwor_day,
                    cco_area_name
                FROM performance_data
                WHERE cgi IN ({})
                    AND start_time BETWEEN ? AND ?
                    AND data_type = 'capacity'
                ORDER BY cgi, start_time
            '''.format(','.join(['?'] * len(all_cgis)))
            
            params = all_cgis + [start_str, end_str]
            data = self.db_manager.execute_query(query, params)
            
            if not data:
                progress_bar.empty()
                status_text.empty()
                return pd.DataFrame()
            
            # æ­¥éª¤3ï¼šè¯†åˆ«æµé‡çªé™å°åŒº
            status_text.text(f"ğŸ” æ­¥éª¤ 3/6: è¯†åˆ«æµé‡çªé™å°åŒº...")
            progress_bar.progress(40)
            
            df = pd.DataFrame(data)
            df['start_time'] = pd.to_datetime(df['start_time'])
            df['date'] = df['start_time'].dt.date
            
            # æŒ‰CGIåˆ†ç»„åˆ†æ
            drop_cells = []
            grouped = df.groupby('cgi')
            
            for cgi, cgi_data in grouped:
                min_required_days = window_size * 2 + 1
                if len(cgi_data) < min_required_days:
                    continue
                
                # æŒ‰æ—¥æœŸèšåˆ
                daily_data = cgi_data.groupby('date')['flwor_day'].mean().reset_index()
                daily_data = daily_data.sort_values('date')
                
                # ä½¿ç”¨æ»‘åŠ¨çª—å£ç®—æ³•æŸ¥æ‰¾æµé‡çªé™
                drop_date = self._find_traffic_drop_date(daily_data, drop_threshold, window_size)
                
                if drop_date is not None:
                    # è·å–è¯¥CGIçš„cco_area_nameï¼ˆå–ç¬¬ä¸€ä¸ªéç©ºå€¼ï¼‰
                    cco_area_name = cgi_data['cco_area_name'].dropna().iloc[0] if not cgi_data['cco_area_name'].dropna().empty else ''
                    drop_cells.append({
                        'cgi': cgi,
                        'drop_date': drop_date,
                        'cco_area_name': cco_area_name
                    })
            
            if not drop_cells:
                progress_bar.empty()
                status_text.empty()
                st.warning("âš ï¸ æœªå‘ç°æµé‡çªé™å°åŒº")
                return pd.DataFrame()
            
            # æ­¥éª¤4ï¼šè·å–å·¥å‚ä¿¡æ¯
            status_text.text(f"ğŸ¢ æ­¥éª¤ 4/6: è·å– {len(drop_cells)} ä¸ªçªé™å°åŒºçš„å·¥å‚ä¿¡æ¯...")
            progress_bar.progress(60)
            
            drop_cgis = [cell['cgi'] for cell in drop_cells]
            
            # æŸ¥è¯¢å·¥å‚ä¿¡æ¯ï¼Œä½¿ç”¨ç‰©ç†ç«™åä½œä¸ºæ‰‡åŒºæ ‡è¯†
            engineering_query = '''
                SELECT 
                    cgi,
                    phy_name,
                    antenna_name,
                    celname,
                    zhishi,
                    pinduan
                FROM engineering_params
                WHERE cgi IN ({})
            '''.format(','.join(['?'] * len(drop_cgis)))
            
            engineering_data = self.db_manager.execute_query(engineering_query, drop_cgis)
            
            if not engineering_data:
                progress_bar.empty()
                status_text.empty()
                st.warning("âš ï¸ æœªæ‰¾åˆ°å·¥å‚ä¿¡æ¯")
                return pd.DataFrame()
            
            # æ­¥éª¤5ï¼šæ‰‡åŒºçº§åˆ†æ
            status_text.text("ğŸ” æ­¥éª¤ 5/6: è¿›è¡Œæ‰‡åŒºçº§åˆ†æ...")
            progress_bar.progress(80)
            
            # åˆ›å»ºå·¥å‚DataFrame
            eng_df = pd.DataFrame(engineering_data)
            
            # åˆå¹¶çªé™å°åŒºå’Œå·¥å‚ä¿¡æ¯
            drop_df = pd.DataFrame(drop_cells)
            merged_df = drop_df.merge(eng_df, on='cgi', how='left')
            
            # æŒ‰ç‰©ç†ç«™ï¼ˆæ‰‡åŒºï¼‰åˆ†ç»„åˆ†æ
            sector_analysis = []
            
            for phy_name, sector_cells in merged_df.groupby('phy_name'):
                if pd.isna(phy_name) or phy_name == '':
                    continue
                
                # è·å–è¯¥æ‰‡åŒºä¸‹æ‰€æœ‰å°åŒºï¼ˆåŒ…æ‹¬éçªé™å°åŒºï¼‰
                all_sector_cells_query = '''
                    SELECT cgi, celname, antenna_name, zhishi, pinduan
                    FROM engineering_params
                    WHERE phy_name = ?
                '''
                all_sector_cells = self.db_manager.execute_query(all_sector_cells_query, (phy_name,))
                all_sector_df = pd.DataFrame(all_sector_cells)
                
                # ç»Ÿè®¡æ‰‡åŒºä¿¡æ¯
                total_cells_in_sector = len(all_sector_df)
                drop_cells_in_sector = len(sector_cells)
                drop_ratio = (drop_cells_in_sector / total_cells_in_sector * 100) if total_cells_in_sector > 0 else 0
                
                # åˆ¤æ–­æ‰‡åŒºçº§ä¸‹é™ç±»å‹
                if drop_ratio == 100:
                    drop_type = "æ‰‡åŒºæ•´ä½“ä¸‹é™"
                elif drop_ratio > 0:
                    drop_type = "å•ä¸ª/éƒ¨åˆ†å°åŒºä¸‹é™"
                else:
                    drop_type = "æ— çªé™"
                
                # åˆ†æå¤©çº¿æƒ…å†µ
                drop_antennas = sector_cells['antenna_name'].dropna().unique()
                all_antennas = all_sector_df['antenna_name'].dropna().unique()
                
                # åˆ¤æ–­æ˜¯å¦å…±å¤©çº¿
                if len(drop_antennas) == 1 and len(all_antennas) == 1:
                    antenna_status = "å…±å¤©çº¿"
                elif len(drop_antennas) == 1:
                    antenna_status = "çªé™å°åŒºå…±å¤©çº¿"
                else:
                    antenna_status = "ä¸å…±å¤©çº¿"
                
                # è·å–æ‰‡åŒºåå­—ï¼ˆcco_area_nameï¼‰
                sector_names = sector_cells['cco_area_name'].dropna().unique()
                sector_name = ', '.join(sector_names) if len(sector_names) > 0 else ''
                
                # è·å–çªé™æ—¥æœŸ
                drop_dates = sector_cells['drop_date'].dropna().unique()
                drop_date_str = ', '.join([str(d) for d in drop_dates]) if len(drop_dates) > 0 else ''
                
                # è®¡ç®—æ‰‡åŒºçº§æµé‡ç»Ÿè®¡
                sector_traffic_before = 0
                sector_traffic_after = 0
                sector_traffic_drop = 0
                sector_drop_ratio = 0
                
                # è·å–è¯¥æ‰‡åŒºæ‰€æœ‰çªé™å°åŒºçš„æµé‡æ•°æ®
                sector_drop_cgis = sector_cells['cgi'].tolist()
                if sector_drop_cgis:
                    # æŸ¥è¯¢è¿™äº›å°åŒºçš„æµé‡æ•°æ®
                    sector_traffic_query = '''
                        SELECT cgi, start_time, flwor_day
                        FROM performance_data
                        WHERE cgi IN ({})
                            AND start_time BETWEEN ? AND ?
                            AND data_type = 'capacity'
                        ORDER BY cgi, start_time
                    '''.format(','.join(['?'] * len(sector_drop_cgis)))
                    
                    sector_traffic_data = self.db_manager.execute_query(
                        sector_traffic_query, 
                        sector_drop_cgis + [start_str, end_str]
                    )
                    
                    if sector_traffic_data:
                        sector_traffic_df = pd.DataFrame(sector_traffic_data)
                        sector_traffic_df['start_time'] = pd.to_datetime(sector_traffic_df['start_time'])
                        sector_traffic_df['date'] = sector_traffic_df['start_time'].dt.date
                        
                        # æŒ‰CGIåˆ†ç»„è®¡ç®—æ¯ä¸ªå°åŒºçš„çªé™å‰åæµé‡
                        total_before = 0
                        total_after = 0
                        valid_cells = 0
                        
                        for cgi in sector_drop_cgis:
                            cgi_data = sector_traffic_df[sector_traffic_df['cgi'] == cgi]
                            if len(cgi_data) < window_size * 2 + 1:
                                continue
                                
                            daily_data = cgi_data.groupby('date')['flwor_day'].mean().reset_index()
                            daily_data = daily_data.sort_values('date')
                            
                            # æ‰¾åˆ°çªé™æ—¥æœŸ
                            drop_date = self._find_traffic_drop_date(daily_data, drop_threshold, window_size)
                            if drop_date is None:
                                continue
                            
                            # è®¡ç®—çªé™å‰åæµé‡
                            before_data = daily_data[daily_data['date'] < drop_date].tail(window_size)
                            after_data = daily_data[daily_data['date'] >= drop_date].head(window_size)
                            
                            if len(before_data) >= window_size and len(after_data) >= window_size:
                                avg_before = before_data['flwor_day'].mean()
                                avg_after = after_data['flwor_day'].mean()
                                
                                total_before += avg_before
                                total_after += avg_after
                                valid_cells += 1
                        
                        if valid_cells > 0:
                            sector_traffic_before = round(total_before, 2)
                            sector_traffic_after = round(total_after, 2)
                            sector_traffic_drop = round(total_before - total_after, 2)
                            sector_drop_ratio = round((total_before - total_after) / total_before * 100, 2) if total_before > 0 else 0
                
                # æ”¶é›†æ‰‡åŒºåˆ†æç»“æœ
                sector_analysis.append({
                    'ç‰©ç†ç«™': phy_name,
                    'æ‰‡åŒºåå­—': sector_name,
                    'æ‰‡åŒºæ€»å°åŒºæ•°': total_cells_in_sector,
                    'çªé™å°åŒºæ•°': drop_cells_in_sector,
                    'ä¸‹é™ç±»å‹': drop_type,
                    'å¤©çº¿çŠ¶æ€': antenna_status,
                    'åˆ¶å¼': ', '.join(sector_cells['zhishi'].dropna().unique()),
                    'é¢‘æ®µ': ', '.join(sector_cells['pinduan'].dropna().unique()),
                    'çªé™å‰æµé‡(GB)': sector_traffic_before,
                    'çªé™åæµé‡(GB)': sector_traffic_after,
                    'çªé™æµé‡(GB)': sector_traffic_drop,
                    'çªé™æ¯”ä¾‹(%)': sector_drop_ratio,
                    'çªé™æ—¥æœŸ': drop_date_str,
                    'çªé™å°åŒºåˆ—è¡¨': ', '.join(sector_cells['cgi'].tolist())
                })
            
            # æ­¥éª¤6ï¼šç”Ÿæˆæœ€ç»ˆç»“æœ
            status_text.text("âœ… æ­¥éª¤ 6/6: ç”Ÿæˆåˆ†æç»“æœ...")
            progress_bar.progress(100)
            
            result_df = pd.DataFrame(sector_analysis)
            
            # æ¸…ç©ºè¿›åº¦æ¡
            progress_bar.empty()
            status_text.empty()
            
            return result_df
            
        except Exception as e:
            if 'progress_bar' in locals():
                progress_bar.empty()
            if 'status_text' in locals():
                status_text.empty()
            st.error(f"æ‰‡åŒºçº§åˆ†æå¤±è´¥: {e}")
            self.logger.error(f"æ‰‡åŒºçº§åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()
